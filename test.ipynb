{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from enum import Enum, IntEnum\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_scrap = \"\"\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.set_window_size(1400,1000)\n",
    "driver.get(\"https://www.pepper.pl/nowe\")\n",
    "time.sleep(0.7)\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, \"html5lib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">967,99zł</span>] date: ['3 min'], name: Zestaw Garmin Edge 530 Bundle z czujnikiem tętna, prędkość i kadencji.\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">79,17zł</span>] date: ['7 min'], name: Rick Astley - Hold Me in Your Arms (2023 Remaster) - Winyl LP\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">30,89zł</span>] date: ['10 min'], name: Watomierz miernik zużycia energii 16A\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">35,99zł</span>] date: ['10 min'], name: Wałek do ciasta piernik folkroll\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">259zł</span>] date: ['12 min'], name: Chłodzenie Cooler Master MasterLiquid ML240L V2 RGB 2x120mm (kolor biały w tej samej cenie) @ x-kom\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">1zł</span>] date: ['Stacjonarnie'], name: Mydło w kostce Luksja 90g mix | NETTO\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">6,29zł</span>] date: ['14 min', 'Stacjonarnie'], name: Pepsi puszka 6x330ml za 6,29zl. Lidl\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">7,98zł</span>] date: ['Stacjonarnie'], name: Piwo Brooklyn Brewery Pilsner 4-pak +1 gratis @Netto\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">23,99zł</span>] date: ['15 min'], name: Wkładki do butów scholl 42.5-45\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">9,99zł</span>] date: ['Stacjonarnie', '18 min'], name: Filet z kurczaka\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">800,99zł</span>] date: ['18 min'], name: Słuchawki SONY WH-1000XM4 przy płatności blik\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">10,99zł</span>] date: ['Stacjonarnie'], name: Filet z piersi kurczaka kg @Netto\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">3 187zł</span>] date: ['27 min'], name: Orbitrek Kettler Optima 400\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">10,33zł</span>] date: ['27 min'], name: Łańcuch choinkowy 6 szt. po 2m. trzy kolory do wyboru\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">632zł</span>] date: ['28 min'], name: Projektor Wanbo T2 Max\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">45zł</span>] date: ['33 min'], name: Klawiatura DELL KB216\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">3 899zł</span>] date: ['35 min'], name: Laptop Acer Swift Go 14 SFG14-71-54M4\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">37,25zł</span>] date: ['35 min'], name: Wiedźmin 3 : Dziki Gon - Edycja Kompletna | + 3,75zł Nagrody Epic | Możliwe 24,95zł\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">1 075zł</span>] date: ['39 min'], name: Płyta indukcyjna Whirlpool WS Q4860 NE 59cm\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">189,99zł</span>] date: ['41 min'], name: Plecak puccini Ryan Air/Wizzair\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">29,99zł</span>] date: ['41 min'], name: Szelki - Bytom\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">149zł</span>] date: ['52 min'], name: Giacomo Conti Garnitur CESARE GAGS000207\n",
      "discount price: [] date: ['53 min'], name: -15% na podłogi przy zakupach za min. 100 zł @ Komfort\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">1 849zł</span>] date: ['55 min'], name: Zmywarka Beko BDIN38643C bPro500 59,8cm\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">90zł</span>] date: ['56 min'], name: Woda perfumowana Lattafa Asad 100ml\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">524,20zł</span>] date: ['57 min'], name: Wyjazd na 2 mecze fc barcelona ( Villarreal, osasuna) 28.01- 31.01 od 524,20 zlotych( możliwe 426.2 z WDC)\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">699zł</span>] date: ['1 g, 1 min'], name: Słuchawki nauszne SteelSeries Arctis Nova Pro X (na kablu) @ Media Expert\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">0,45zł</span>] date: ['1 g, 9 min', 'Stacjonarnie'], name: Maska Medyczna Opharm 50 sztuk pudełko niebieska / Maska Medyczna FFP2 KN95 5szt\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">3 999zł</span>] date: ['Zaktualizowano 1 g, 18 min temu'], name: Telewizor LG 75UR91003LA 75\" 4K Smart TV HDMI 2.1 DVB-T2\n",
      "discount price: [<span class=\"thread-price text--b cept-tp size--all-l size--fromW3-xl\">1,99zł</span>] date: ['1 g, 22 min', 'Stacjonarnie'], name: Chipsy i chrupki Cheetos i Lays 190g Carrefour Best Mall Sadyba WWA\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "articles = soup.find_all('article')\n",
    "for article in articles:\n",
    "    name = article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "    date_tag = article.find_all('div', {\"class\":\"size--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "    discount_price = article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "    raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "    items_to_remove = list()\n",
    "    filtered_list = list()\n",
    "    for string in raw_string_list:\n",
    "        if \"/\" in string:\n",
    "            items_to_remove.append(string)\n",
    "        if \":\" in string:\n",
    "            items_to_remove.append(string)\n",
    "        if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\"]:\n",
    "            items_to_remove.append(string)\n",
    "        if string.startswith(\"Wysyłka\"):\n",
    "            items_to_remove.append(string)\n",
    "    counts = Counter(items_to_remove)\n",
    "    for string in raw_string_list:\n",
    "        if counts[string]:\n",
    "            counts[string] -= 1\n",
    "        else:\n",
    "            filtered_list.append(string)\n",
    "\n",
    "\n",
    "    print(f\"discount price: {discount_price} date: {filtered_list}, name: {name}\")\n",
    "    #print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-03\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "from typing import List, Union\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.set_window_size(1400,1000)\n",
    "driver.get(\"https://www.pepper.pl/promocje/akcja-swiec-blaskiem-z-odblaskiem-darmowe-odblaski-w-warszawie-761817\")\n",
    "time.sleep(0.7)\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, \"html5lib\")\n",
    "\n",
    "\n",
    "date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "filtered_list = date_string.split()\n",
    "day_string = filtered_list[0]\n",
    "month_string = filtered_list[1]\n",
    "year_string = filtered_list[2].strip(',')\n",
    "\n",
    "if len(day_string[0]) == 2:\n",
    "    day = day_string\n",
    "else:\n",
    "    day = day_string.zfill(2)\n",
    "\n",
    "month = Months.__members__[month_string].value\n",
    "year = year_string\n",
    "prepared_data = '-'.join([year, month, day])\n",
    "\n",
    "print(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[707151, 'Smartfon Samsung Galaxy S22 8/128GB Zielony', 2439.0, 'NA', 'NA', '2023-08-09', 'NA - Login Required.']\n",
      "[704584, 'Smartfon Samsung Galaxy S22 (S901) 8/256GB 6,1\" Dynamic AMOLED 2X 2340x1080 3700mAh Dual SIM 5G Green', 2799.0, 'NA', 'NA', '2023-08-02', 'NA - Login Required.']\n",
      "[702868, 'Galaxy S21 Case / obudowa / etui', 17.1, 'NA', 'NA', '2023-07-28', 'NA - Login Required.']\n",
      "[702788, '2 szt. Wodoszczelne etui na telefon komórkowy TOPK nowy model 2023 - TYLKO PRIME', 39.99, 'NA', 'NA', '2023-07-28', 'NA - Login Required.']\n",
      "[700810, 'Białe Etui Samsung Silicone Cover With Strap do Galaxy S22 Ultra', 19.99, -86.0, 139.99, '2023-07-21', 'NA - Login Required.']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "from datetime import datetime, timedelta, date, timezone\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "import signal\n",
    "import traceback\n",
    "from source.pepper_app.constans import OLD_DATES_DATA_PATTERN_1, OLD_DATES_DATA_PATTERN_2\n",
    "\n",
    "from source.pepper_app.environment_config import CustomEnvironment\n",
    "from source.pepper_app.constans import DATA_HEADER, STATS_HEADER, REQUEST_HEADER, RESPONSE_HEADER\n",
    "\n",
    "from collections import Counter\n",
    "from requests.exceptions import ConnectionError, HTTPError, MissingSchema, ReadTimeout\n",
    "from source.pepper_app.constans import (CSV_COLUMNS,\n",
    "                                STATS_HEADER)\n",
    "\n",
    "class ScrapPage:\n",
    "\n",
    "\n",
    "    def __init__(self, category_type: str, articles_to_retrieve: int, to_csv: bool=False,\n",
    "                to_database: bool=False, to_statistics: bool=True, searched_article: str='NA', \n",
    "                scrap_continuously: bool=False, scrap_choosen_data: bool=True) -> None:\n",
    "        self.category_type = category_type\n",
    "        self.articles_to_retrieve = articles_to_retrieve\n",
    "        self.to_database = to_database\n",
    "        self.to_csv = to_csv\n",
    "        self.to_statistics = to_statistics\n",
    "        self.start_page = 3\n",
    "        self.searched_article = searched_article\n",
    "        self.scrap_continuously = scrap_continuously\n",
    "        self.scrap_choosen_data = scrap_choosen_data\n",
    "\n",
    "    def scrap_page(self, url_to_scrap: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "        \"\"\"Setting up selenium webdriver, scraping page with bs4.\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            #options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options)\n",
    "                #driver = webdriver.Remote(command_executor='http://selenium-hub:4444/wd/hub', options=options)\n",
    "            driver.set_window_size(1400,1000)\n",
    "            driver.get(url_to_scrap)\n",
    "            time.sleep(0.7)\n",
    "            page = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page, \"html5lib\")\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "    def select_url(self) -> str:\n",
    "        \"\"\"Selection of the website address depending on the type of scrapping.\"\"\"\n",
    "        if self.scrap_continuously == True:\n",
    "            url_to_scrap = \"\".join([CustomEnvironment.get_url(), \"nowe\"])\n",
    "            return url_to_scrap\n",
    "        elif self.category_type == \"nowe\" and self.scrap_continuously == False:\n",
    "            url_to_scrap = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?page=\", str(self.start_page)])\n",
    "            return url_to_scrap\n",
    "        elif self.category_type == \"search\" and self.scrap_continuously == False:\n",
    "            searched_article = str(self.searched_article.replace(\" \",\"%20\"))\n",
    "            url_to_scrap = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?q=\",\n",
    "                                    searched_article, \"&page=\", str(self.start_page)])\n",
    "            return url_to_scrap\n",
    "        else:\n",
    "            raise Exception(f\"The variables were defined incorrectly.\")\n",
    "\n",
    "\n",
    "    def infinite_scroll_handling(self) -> List[str]:\n",
    "        \"\"\"Handling scraping through subsequent pages.\"\"\"\n",
    "        try:\n",
    "            flag = True\n",
    "            retrived_articles = list()\n",
    "            while flag:\n",
    "                url_to_scrap = self.select_url()\n",
    "                soup = self.scrap_page(url_to_scrap)\n",
    "                flag_nowe = CheckConditions(soup).check_if_last_page_nowe()\n",
    "                flag_search = CheckConditions(soup).check_if_last_page_search()\n",
    "                if flag_nowe == False or flag_search == False:\n",
    "                    flag = False\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                flag = CheckConditions(soup).check_if_no_items_found()\n",
    "                if flag == False:\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                if flag == True:\n",
    "                    articles = soup.find_all('article')\n",
    "                    retrived_articles += articles\n",
    "                else:\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                if len(retrived_articles) >= int(self.articles_to_retrieve):\n",
    "                    flag = False\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                self.start_page += 1\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Infinite scroll failed:\\\n",
    "                            {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "\n",
    "    def get_items_details_depending_on_the_function(self):\n",
    "        \"\"\"Completing the list of articles and extracting data details depending on the type of scrapping.\"\"\"\n",
    "        if self.scrap_continuously == True and self.scrap_choosen_data == False:\n",
    "            while True:\n",
    "                retrived_articles = self.scrap_continuously_by_refreshing_page()\n",
    "                self.get_items_details(retrived_articles)\n",
    "        elif self.scrap_continuously == False and self.scrap_choosen_data == True:\n",
    "            retrived_articles = self.infinite_scroll_handling()\n",
    "            all_items = self.get_items_details(retrived_articles)\n",
    "            return all_items\n",
    "        else:\n",
    "            raise Exception(f\"Matching get_items_details depending on the selected \\\n",
    "                            functionality failed. \\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "    def get_items_details(self, retrived_articles) -> list():\n",
    "        \"\"\"Getting item detailes.\"\"\"\n",
    "        start_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        all_items = list()\n",
    "        try:\n",
    "            for article in retrived_articles:\n",
    "                item = list()\n",
    "                item.append(GetItemId(article).get_data())\n",
    "                item.append(GetItemName(article).get_data())\n",
    "                item.append(GetItemDiscountPrice(article).get_data())\n",
    "                item.append(GetItemPercentageDiscount(article).get_data())\n",
    "                item.append(GetItemRegularPrice(article).get_data())\n",
    "                item.append(GetItemAddedDate(article).get_data())\n",
    "                item.append(GetItemUrl(article).get_data())\n",
    "                if item not in all_items:\n",
    "                    all_items.append(item)\n",
    "                if '' in item:\n",
    "                    logging.warning(\"Data retrieving failed. None values detected\")\n",
    "                    break\n",
    "                if self.to_csv:\n",
    "                    self.save_data_to_csv(item)\n",
    "                #if self.to_database:\n",
    "                #    LoadItemDetailsToDatabase(item).load_to_db()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item details failed :\\\n",
    "                        {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "        end_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = end_time - start_time\n",
    "\n",
    "        if self.to_statistics:\n",
    "            try:\n",
    "                stats_info = self.get_scraping_stats_info(action_execution_datetime)\n",
    "                #LoadScrapingStatisticsToDatabase(stats_info).load_to_db()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Populating ScrapingStatistics table failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "        return all_items\n",
    "\n",
    "    def save_data_to_csv(self, item) -> None:\n",
    "        \"\"\"Saving data to csv file.\"\"\"\n",
    "        try:\n",
    "            header = False\n",
    "            if not os.path.exists('scraped.csv'):\n",
    "                header = True\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "            else:\n",
    "                header = False\n",
    "                df_e = pd.read_csv('scraped.csv')\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                if df['item_id'][0] not in df_e['item_id'].tolist():\n",
    "                    df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Saving data to csv failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "    def get_scraping_stats_info(self, action_execution_datetime: datetime) -> List[Union[str, int, bool, float]]:\n",
    "        \"\"\"Getting scraping stats info.\"\"\"\n",
    "        stats_info = list()\n",
    "\n",
    "        category_type = self.category_type\n",
    "        retrieved_articles_quantity = self.articles_to_retrieve\n",
    "        time_of_the_action = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = action_execution_datetime\n",
    "        searched_article = self.searched_article\n",
    "        to_csv = self.to_csv\n",
    "        to_database  = self.to_database\n",
    "        scrap_continuously = self.scrap_continuously\n",
    "        scrap_choosen_data = self.scrap_choosen_data\n",
    "\n",
    "        stats=[category_type, retrieved_articles_quantity,\n",
    "            time_of_the_action, action_execution_datetime, searched_article,\n",
    "            to_csv, to_database, scrap_continuously, scrap_choosen_data]\n",
    "\n",
    "        for field in stats:\n",
    "            stats_info.append(field)\n",
    "\n",
    "        return stats_info\n",
    "\n",
    "\n",
    "    def scrap_continuously_by_refreshing_page(self) -> List[str]:\n",
    "        \"\"\"Scraping data function for continuously scraping feature.\"\"\"\n",
    "        retrived_articles = list()\n",
    "\n",
    "        soup = self.scrap_page()\n",
    "        time.sleep(20)\n",
    "        articles = soup.find_all('article')\n",
    "        retrived_articles += articles\n",
    "\n",
    "        return retrived_articles\n",
    "\n",
    "\n",
    "class CheckConditions:\n",
    "\n",
    "\n",
    "    def __init__(self, soup: BeautifulSoup) -> None:\n",
    "        self.soup = soup\n",
    "\n",
    "\n",
    "    def check_if_last_page_nowe(self) -> bool:\n",
    "        \"\"\"Checking 'nowe' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h1', {\"class\":\"size--all-xl size--fromW3-xxl text--b space--b-2\"})[0].get_text()\n",
    "            if searched_ending_string.startswith(\"Ups\"):\n",
    "                logging.warning(\"No more pages to scrap.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    def check_if_last_page_search(self) -> bool:\n",
    "        \"\"\"Checking 'search' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-l\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"box--all-i size--all-s vAlign--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t Okazje()\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number > 0:\n",
    "                logging.warning(\"No more pages to scrap.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    def check_if_no_items_found(self) -> bool:\n",
    "        \"\"\"Checking if searched item was found.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-l\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"box--all-i size--all-s vAlign--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t Okazje()\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number == 0:\n",
    "                logging.warning(\"The searched item was not found.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "\n",
    "class GetItemName:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            name = self.article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "            return name\n",
    "        except IndexError as e:\n",
    "            name = self.article.find_all(attrs={'class': \"thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "            return name\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item name failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemId:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> int:\n",
    "        try:\n",
    "            item_id = self.article.get(\"id\")\n",
    "            item_id = item_id.strip('thread_')\n",
    "            item_id = int(item_id)\n",
    "            return item_id\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item id failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemDiscountPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "            if len(discount_price) == 0:\n",
    "                discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl text--color-greyShade\"})\n",
    "\n",
    "            if len(discount_price) > 0:\n",
    "                discount_price = discount_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ','')\n",
    "                if discount_price == \"ZADARMO\":\n",
    "                    discount_price = float(0)\n",
    "                else:\n",
    "                    discount_price = float(discount_price)                \n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                discount_price = \"NA\"\n",
    "            return discount_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item discount price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "\n",
    "class GetItemRegularPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            regular_price = self.article.find_all(attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"})\n",
    "            if len(regular_price) > 0:\n",
    "                regular_price = float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ',''))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                regular_price = \"NA\"\n",
    "            return regular_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item regular price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "\n",
    "class GetItemPercentageDiscount:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            percentage_discount = self.article.find_all(attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            if len(percentage_discount) > 0:\n",
    "                percentage_discount = float(percentage_discount[0].get_text().strip('%'))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                percentage_discount = \"NA\"\n",
    "            return percentage_discount\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item percentage discount failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemUrl:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            item_url = self.article.find_all(attrs={'class', 'cept-tt thread-link linkPlain thread-title--list js-thread-title'})[0]['href']\n",
    "            print(self.article)\n",
    "            return item_url\n",
    "        except IndexError as e:\n",
    "            item_url = 'NA - Login Required.'\n",
    "            return item_url\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item url failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemAddedDate:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_raw_data(self) -> List[str]:\n",
    "\n",
    "        try:\n",
    "            date_tag = self.article.find_all('div', {\"class\":\"size--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "            raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "            return raw_string_list\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item added date failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "\n",
    "        try:\n",
    "            filtered_list = self.clean_list()\n",
    "            filtered_list = self.check_missing_date()\n",
    "            date_string_likely = filtered_list[0]\n",
    "            if date_string_likely == \"NA\":\n",
    "                url_with_item = GetItemUrl.get_data(self)\n",
    "                soup = self.scrap_page(url_with_item)\n",
    "                prepared_data = self.fill_missing_date(soup)\n",
    "                return prepared_data\n",
    "            else:\n",
    "                stripped_date_string_likely = self.strip_date_string(date_string_likely)\n",
    "                prepared_data = self.date_format_conversion(stripped_date_string_likely)\n",
    "            return prepared_data\n",
    "\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name: {e}\")\n",
    "\n",
    "    def strip_date_string(self, date_string_likely: str) -> str:\n",
    "\n",
    "        try:\n",
    "            if date_string_likely.startswith(\"Zaktualizowano \") and date_string_likely.endswith(\" temu\"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                stripped_date_string_likely = stripped_date_string_likely.rstrip(\" temu\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.endswith(\"Lokalnie\"):\n",
    "                stripped_date_string_likely = date_string_likely.rstrip(\"Lokalnie\")\n",
    "                return stripped_date_string_likely\n",
    "            else:\n",
    "                stripped_date_string_likely = date_string_likely\n",
    "                return stripped_date_string_likely\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Stripping date string failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def date_format_conversion(self, stripped_date_string_likely: str) -> str:\n",
    "\n",
    "        try:\n",
    "            if stripped_date_string_likely.endswith(('min', 'g', 's')):\n",
    "                prepared_data = date.today().strftime(\"%Y-%m-%d\")\n",
    "                return prepared_data\n",
    "            elif stripped_date_string_likely.startswith(tuple(Months.keys())) and len(stripped_date_string_likely) < 8:\n",
    "                if len(stripped_date_string_likely[4:]) == 3:\n",
    "                    day = stripped_date_string_likely[4:6]\n",
    "                else:\n",
    "                    day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = str(date.today().year)\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_1, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:6]\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[8:13]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_2, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[7:12]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Data format conversion tailed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "    def clean_list(self) -> List[str]:\n",
    "\n",
    "        raw_string_list = self.get_raw_data()\n",
    "        items_to_remove = list()\n",
    "        filtered_list = list()\n",
    "\n",
    "        try:\n",
    "            for string in raw_string_list:\n",
    "                if \"/\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if \":\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\", \"Stacjonarnie\"]:\n",
    "                    items_to_remove.append(string)\n",
    "                if string.startswith(\"Wysyłka\"):\n",
    "                    items_to_remove.append(string)\n",
    "\n",
    "            counts = Counter(items_to_remove)\n",
    "\n",
    "            for string in raw_string_list:\n",
    "                if counts[string]:\n",
    "                    counts[string] -= 1\n",
    "                else:\n",
    "                    filtered_list.append(string)\n",
    "\n",
    "            return filtered_list\n",
    "\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "\n",
    "    def check_missing_date(self) -> List[str]:\n",
    "\n",
    "        filtered_list = self.clean_list()\n",
    "\n",
    "        try:\n",
    "            if len(filtered_list) == 0:\n",
    "                filtered_list.append(\"NA\")\n",
    "                return filtered_list\n",
    "            else:\n",
    "                return filtered_list\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "\n",
    "    def fill_missing_date(self, soup) -> str:\n",
    "\n",
    "        try:\n",
    "            date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "            filtered_list = date_string.split()\n",
    "            day_string = filtered_list[0]\n",
    "            month_string = filtered_list[1]\n",
    "            year_string = filtered_list[2].strip(',')\n",
    "\n",
    "            if len(day_string[0]) == 2:\n",
    "                day = day_string\n",
    "            else:\n",
    "                day = day_string.zfill(2)\n",
    "\n",
    "            month = Months.__members__[month_string].value\n",
    "            year = year_string\n",
    "            prepared_data = '-'.join([year, month, day])\n",
    "            return prepared_data\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "\n",
    "    def scrap_page(self, url_with_item: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options)\n",
    "                #driver = webdriver.Remote(command_executor='http://selenium-hub:4444/wd/hub', options=options)\n",
    "            driver.get(url_with_item)\n",
    "            time.sleep(0.7)\n",
    "            page_with_item = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page_with_item, 'html5lib')\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "category_type = \"search\"\n",
    "articles_to_retrieve = 5\n",
    "searched_article = \"S22\"\n",
    "\n",
    "output = ScrapPage(category_type, articles_to_retrieve, searched_article=searched_article)\n",
    "all_items = output.get_items_details_depending_on_the_function()\n",
    "\n",
    "\n",
    "for i in all_items:\n",
    "    print(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
