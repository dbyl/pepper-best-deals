{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from enum import Enum, IntEnum\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_scrap = \"\"\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.set_window_size(1400,1000)\n",
    "driver.get(\"https://www.pepper.pl/nowe?page=16\")\n",
    "time.sleep(0.7)\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, \"html5lib\")\n",
    "\n",
    "with open(\"soup.html\", \"w\") as file:\n",
    "    file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "path_to_file = Path(\"source/pepper_app/tests/fixtures/to_test_get_info/soup.html\")\n",
    "with open(path_to_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = file.read()\n",
    "soup = BeautifulSoup(soup, \"html5lib\")\n",
    "articles = soup.find_all('article')\n",
    "retrived_articles = articles[:]\n",
    "\n",
    "print(len(retrived_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread_803607\n",
      "Szampon dla mężczyzn MASVERI Men (przeciwłupieżowy / przeciw wypadaniu włosów), brak SLS parabenów, trzy warianty zapachowe\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">19,99zł</span>]\n",
      "[<span class=\"mute--text text--lineThrough space--ml-1 size--all-l size--fromW3-xl\">28,99zł</span>]\n",
      "[<span class=\"text--color-charcoal space--ml-1 size--all-l size--fromW3-xl\">-31%</span>]\n",
      "[<div class=\"flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"><span class=\"metaRibbon size--all-s lbox--v-1 boxAlign-ai--all-c overflow--wrap-off space--l-3 text--color-greyShade\" short-view=\"true\"><svg class=\"space--mr-1 icon icon--hourglass text--color-greyShade\" height=\"22\" width=\"18\"><use xlink:href=\"/assets/img/ico_38c79.svg#hourglass\"></use></svg><span><span class=\"text--b\">18/03/2024</span></span></span><span class=\"metaRibbon size--all-s lbox--v-1 boxAlign-ai--all-c overflow--wrap-off space--l-3 text--color-greyShade\" short-view=\"true\"><svg class=\"space--mr-1 icon icon--clock text--color-greyShade\" height=\"22\" width=\"22\"><use xlink:href=\"/assets/img/ico_38c79.svg#clock\"></use></svg><span>mar 1.</span></span><!----></div>]\n",
      "['18/03/2024', 'mar 1.']\n"
     ]
    }
   ],
   "source": [
    "articles = soup.find_all('article')\n",
    "\n",
    "article = articles[1]\n",
    "article_id =  article.get(\"id\")\n",
    "name = article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "discount_price = article.find_all(\"span\", {'class': \"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\"})\n",
    "regular_price = article.find_all(\"span\", {'class': \"mute--text text--lineThrough space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "percentage_discount = article.find_all(\"span\", {'class': \"text--color-charcoal space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "date_tag = article.find_all('div', {\"class\":\"flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "\n",
    "print(article_id)\n",
    "print(name)\n",
    "print(discount_price)\n",
    "print(regular_price)\n",
    "print(percentage_discount)\n",
    "print(date_tag)\n",
    "print(raw_string_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for article in articles:\n",
    "    article_id =  article.get(\"id\")\n",
    "    name = article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "    discount_price = article.find_all(\"span\", {'class': \"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\"})\n",
    "    regular_price = article.find_all(\"span\", {'class': \"mute--text text--lineThrough space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "    percentage_discount = article.find_all(\"span\", {'class': \"text--color-charcoal space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "    date_tag = article.find_all('div', {\"class\":\"flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "    raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "\n",
    "\n",
    "    #print(f\"{raw_string_list}\")\n",
    "    #print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m date_tag \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      6\u001b[0m discount_price \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind_all(attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread-price text--b cept-tp size--all-l size--fromW3-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m----> 7\u001b[0m raw_string_list \u001b[38;5;241m=\u001b[39m \u001b[43mdate_tag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m items_to_remove \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m      9\u001b[0m filtered_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "File \u001b[0;32m~/pepper-best-deals/env/lib/python3.10/site-packages/bs4/element.py:2428\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   2427\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   2429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResultSet object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key\n\u001b[1;32m   2430\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "articles = soup.find_all('article')\n",
    "for article in articles:\n",
    "    name = article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "    date_tag = article.find_all('div', {\"class\":\"size--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "    discount_price = article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "    raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "    items_to_remove = list()\n",
    "    filtered_list = list()\n",
    "    for string in raw_string_list:\n",
    "        if \"/\" in string:\n",
    "            items_to_remove.append(string)\n",
    "        if \":\" in string:\n",
    "            items_to_remove.append(string)\n",
    "        if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\"]:\n",
    "            items_to_remove.append(string)\n",
    "        if string.startswith(\"Wysyłka\"):\n",
    "            items_to_remove.append(string)\n",
    "    counts = Counter(items_to_remove)\n",
    "    for string in raw_string_list:\n",
    "        if counts[string]:\n",
    "            counts[string] -= 1\n",
    "        else:\n",
    "            filtered_list.append(string)\n",
    "\n",
    "\n",
    "    print(f\"discount price: {discount_price} date: {filtered_list}, name: {name}\")\n",
    "    #print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-03\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "from typing import List, Union\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "\"\"\"options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.set_window_size(1400,1000)\n",
    "driver.get(\"https://www.pepper.pl/nowe\")\n",
    "time.sleep(0.7)\n",
    "page = driver.page_source\"\"\"\n",
    "path_to_file = Path(\"source/pepper_app/tests/fixtures/to_test_get_info/soup_with_none.html\")\n",
    "    with open(path_to_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = file.read()\n",
    "soup = BeautifulSoup(page, \"html5lib\")\n",
    "\n",
    "\n",
    "date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "filtered_list = date_string.split()\n",
    "day_string = filtered_list[0]\n",
    "month_string = filtered_list[1]\n",
    "year_string = filtered_list[2].strip(',')\n",
    "\n",
    "if len(day_string[0]) == 2:\n",
    "    day = day_string\n",
    "else:\n",
    "    day = day_string.zfill(2)\n",
    "\n",
    "month = Months.__members__[month_string].value\n",
    "year = year_string\n",
    "prepared_data = '-'.join([year, month, day])\n",
    "\n",
    "print(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "[704487, 'Zegarek sportowy Garmin Instinct 2 Solar', 1149.0, -17.0, 1386.0, '2024-03-03', 'https://www.pepper.pl/promocje/garmin-instinct-2-solar-704487']\n",
      "[704486, 'Rękawice Mechanix Wear Original Carbon Black', 69.99, -50.0, 139.99, '2024-03-03', 'https://www.pepper.pl/promocje/rekawice-mechanix-wear-original-carbon-black-704486']\n",
      "[704485, 'Świeczki zapachowe Auchan Gdańsk kołobrzeska', 3.0, -61.0, 7.69, '2024-03-03', 'https://www.pepper.pl/promocje/swieczki-zapachowe-auchan-gdansk-kolobrzeska-704485']\n",
      "[704484, 'Crash Team Rumble - Edycja Standardowa za 78,69 zł / Edycja Deluxe za 101,21 zł z Tureckiego Xbox Store @ Xbox One / Xbox Series X / S', 78.69, -23.0, 101.99, '2024-03-03', 'https://www.pepper.pl/promocje/crash-team-rumble-edycja-standardowa-za-7869-zl-edycja-deluxe-za-10121-zl-z-tureckiego-xbox-store-at-xbox-one-xbox-series-x-s-704484']\n",
      "[704482, 'Katowice-Wenecja za 164PLN 4-9.10 (z bagażem 10kg 334/os)', 164.0, 'NA', 'NA', '2024-03-03', 'https://www.pepper.pl/promocje/katowice-wenecja-za-164pln-4-910-z-bagazem-10kg-334os-704482']\n",
      "[704123, 'Smartfon POCO X5 5G 8/256GB Global (Snapdragon 695, 6.67\" 2400 x 1080 Amoled), $269, wysyłka z Chin @ Gshopper', 1080.0, -17.0, 1295.0, '2024-03-03', 'https://www.pepper.pl/promocje/smartfon-poco-x5-5g-8256gb-snapdragon-695-667-2400-x-1080-amoled-269-at-gshopper-704123']\n",
      "[704478, 'Wózek ogrodowy składany, ładowność do 70 kg', 199.0, -13.0, 229.0, '2024-03-03', 'https://www.pepper.pl/promocje/wozek-ogrodowy-skladany-ladownosc-do-70-kg-704478']\n",
      "[704477, '+2 Monety przez cały sierpnień w środy przy zamówieniu z dostawą Allegro One Box', 'NA', 'NA', 'NA', '2024-03-03', 'https://www.pepper.pl/promocje/2-monety-w-srody-w-sierpniu-przy-zamowieniu-do-allegro-one-box-704477']\n",
      "[704473, 'Warszawa-Malmo za 78PLN 7-10.10', 78.0, 'NA', 'NA', '2024-03-03', 'https://www.pepper.pl/promocje/warszawa-malmo-za-78pln-7-1010-704473']\n",
      "[704475, 'Wrzesień: 2 noce w Panorama Ski Bike Spa & Restaurant (Sudety) z wyżywieniem HB dla 2 osób lub rodziny 2+2 za 538 zł/pokój @ Triverna', 538.0, -42.0, 924.0, '2024-03-03', 'https://www.pepper.pl/promocje/wrzesien-2-noce-w-panorama-ski-bike-spa-restaurant-sudety-z-wyzywieniem-hb-dla-2-osob-lub-rodziny-22-za-538-zlpokoj-at-triverna-704475']\n",
      "[704468, 'Hulajnoga OKAI ES20', 2000.0, -17.0, 2399.0, '2024-03-03', 'https://www.pepper.pl/promocje/hulajnoga-okai-es20-704468']\n",
      "[704474, 'I HEART REVOLUTION HEARTS WYPIEKANY ROZŚWIETLACZ GODDESS OF FAITH', 15.99, 'NA', 'NA', '2024-03-03', 'https://www.pepper.pl/promocje/i-heart-revolution-hearts-wypiekany-rozswietlacz-goddess-of-faith-w-super-cenie-704474']\n",
      "[704466, '20% rabatu na zestawy w Pandora!', 'NA', 'NA', 'NA', '2024-03-03', 'https://www.pepper.pl/promocje/20-rabatu-na-zestawy-w-pandora-704466']\n",
      "[704472, 'Pilarko-zagłębiarka tarczowa przewodowa BLANK&HAGEN TTD-PZ1200 165 mm 1200W', 299.0, -49.0, 589.0, '2024-03-03', 'https://www.pepper.pl/promocje/pilarko-zaglebiarka-tarczowa-przewodowa-blankhagen-ttd-pz1200-165-mm-1200w-704472']\n",
      "[704471, 'Zasilacz OCPC Gaming Energia 2 GD850M 850W 80 Plus Gold', 449.0, -18.0, 549.0, '2024-03-03', 'https://www.pepper.pl/promocje/zasilacz-ocpc-gaming-energia-2-gd850m-850w-80-plus-gold-704471']\n",
      "[704470, 'Pancerne etui do IPhone 13 PRO', 479.6, -60.0, 1199.0, '2024-03-03', 'https://www.pepper.pl/promocje/pancerne-etui-do-iphone-13-pro-704470']\n",
      "[704469, 'Męska bluza z bawełny Adidas R.Y.V. Basic za 129zł @ Lounge by Zalando', 129.0, -47.0, 242.0, '2024-03-03', 'https://www.pepper.pl/promocje/meska-bluza-z-bawelny-adidas-ryv-basic-za-129zl-at-lounge-by-zalando-704469']\n",
      "[704467, 'Tunezja Djerba czarter Rainbow walizka podręczna i bagaż rejestrowany w cenie', 499.0, 'NA', 'NA', '2024-03-03', 'https://www.pepper.pl/promocje/tunezja-djerba-czarter-rainbow-walizka-podreczna-i-bagaz-rejestrowany-w-cenie-704467']\n",
      "[704465, 'Stół ogrodowy Farm II 220x100 drewniany', 697.0, -72.0, 2499.0, '2024-03-03', 'https://www.pepper.pl/promocje/stol-ogrodowy-farm-ii-220x100-drewniany-704465']\n",
      "[704459, 'Nawigacja rowerowa - Wahoo Elmnt Roam V2 Bundle', 1892.0, -21.0, 2399.0, '2024-03-03', 'https://www.pepper.pl/promocje/nawigacja-rowerowa-wahoo-elmnt-roam-v2-bundle-704459']\n",
      "[704464, 'The Elder Scrolls IV: Oblivion Game of the Year Edition (PC) za 12,29 zł z Norweskiego Store / Turecki Store 1,17 zł', 12.29, 'NA', 'NA', '2024-03-03', 'https://www.pepper.pl/promocje/the-elder-scrolls-iv-oblivion-game-of-the-year-edition-pc-za-1229-zl-z-norweskiego-store-704464']\n",
      "[704462, 'Wyprzedaż w Diverse! Dodatkowe 50% rabatu na drugą tańszą rzecz!', 'NA', 'NA', 'NA', '2024-03-03', 'https://www.pepper.pl/kupony/wyprzedaz-w-diverse-dodatkowe-50-rabatu-na-druga-tansza-rzecz-704462']\n",
      "[704461, 'Stanik sportowy Hunkemöller THE PRO 2.0 ZINNIA z lekkim wsparciem - dużo rozmiarów', 85.0, -29.0, 119.0, '2024-03-03', 'https://www.pepper.pl/promocje/stanik-sportowy-hunkemoller-the-pro-20-zinnia-z-lekkim-wsparciem-duzo-rozmiarow-704461']\n",
      "[704460, 'Burger King, cheeseburger lub plant-based cheeseburger za 5zł', 5.0, -28.0, 6.95, '2024-03-03', 'https://www.pepper.pl/promocje/burger-king-cheeseburger-lub-plant-based-cheeseburger-za-5zl-704460']\n",
      "[704458, 'Jean Paul Gaultier Le Male Elixir 125ml | Flaconi', 376.29, -17.0, 454.99, '2024-03-03', 'https://www.pepper.pl/promocje/jean-paul-gaultier-le-male-elixir-125ml-flaconi-704458']\n",
      "[704130, 'Kod rabatowy 15% na płytki, ceramikę, armaturę łazienkową (na nieprzecenione) @ Blu', 'NA', 'NA', 'NA', '2024-03-03', 'https://www.pepper.pl/kupony/kod-rabatowy-15-na-plytki-ceramike-armature-lazienkowa-na-nieprzecenione-at-blu-704130']\n",
      "[704457, 'Notino -15% lub -18% kod rabatowy dla mężczyzn', 'NA', 'NA', 'NA', '2024-03-03', 'https://www.pepper.pl/kupony/notino-15-lub-18-kod-rabatowy-dla-mezczyzn-704457']\n",
      "[704456, 'Depeche Mode \"Songs of Faith and Devotion\" Winyl 180gr Amazon It', 94.67, 'NA', 'NA', '2024-03-03', 'https://www.pepper.pl/promocje/depeche-mode-songs-of-faith-and-devotion-winyl-180gr-amazon-it-704456']\n",
      "[704455, 'Kolagen premium SFD', 34.99, -22.0, 44.99, '2024-03-03', 'https://www.pepper.pl/promocje/kolagen-premium-sfd-704455']\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from enum import Enum, IntEnum\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from typing import List, Union\n",
    "from datetime import datetime, timedelta, date, timezone\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "import signal\n",
    "import traceback\n",
    "from source.pepper_app.constans import OLD_DATES_DATA_PATTERN_1, OLD_DATES_DATA_PATTERN_2\n",
    "\n",
    "from source.pepper_app.environment_config import CustomEnvironment\n",
    "from source.pepper_app.constans import DATA_HEADER, STATS_HEADER, REQUEST_HEADER, RESPONSE_HEADER\n",
    "\n",
    "from collections import Counter\n",
    "from requests.exceptions import ConnectionError, HTTPError, MissingSchema, ReadTimeout\n",
    "from source.pepper_app.constans import (CSV_COLUMNS,\n",
    "                                STATS_HEADER)\n",
    "\n",
    "\n",
    "class ScrapePage:\n",
    "\n",
    "    def __init__(self, category_type: str, articles_to_retrieve: int, to_csv: bool=False,\n",
    "                to_database: bool=True, to_statistics: bool=True, searched_article: str='NA', \n",
    "                scrape_continuously: bool=False, scrape_choosen_data: bool=True) -> None:\n",
    "        self.category_type = category_type\n",
    "        self.articles_to_retrieve = articles_to_retrieve\n",
    "        self.to_database = to_database\n",
    "        self.to_csv = to_csv\n",
    "        self.to_statistics = to_statistics\n",
    "        self.start_page = 1\n",
    "        self.searched_article = searched_article\n",
    "        self.scrape_continuously = scrape_continuously\n",
    "        self.scrape_choosen_data = scrape_choosen_data\n",
    "\n",
    "    def scrape_page(self, url_to_scrape: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "        \"\"\"Setting up selenium webdriver, scraping page with bs4.\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options) #for local  \n",
    "                #driver = webdriver.Remote(command_executor=f'http://{CustomEnvironment.get_selenium_container_name()}:4444/wd/hub', options=options) #for docker \n",
    "            driver.set_window_size(1400,1000)\n",
    "            driver.get(url_to_scrape)\n",
    "            time.sleep(0.7)\n",
    "            page = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page, \"html5lib\")\n",
    "            path_to_file = Path(\"source/pepper_app/tests/fixtures/to_test_get_info/soup_with_none.html\")\n",
    "            with open(path_to_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                soup = file.read()\n",
    "            soup = BeautifulSoup(soup, \"html5lib\")\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "    def select_url(self) -> str:\n",
    "        \"\"\"Selection of the website address depending on the type of scraping.\"\"\"\n",
    "        if self.scrape_continuously == True:\n",
    "            url_to_scrape = \"\".join([CustomEnvironment.get_url(), \"nowe\"])\n",
    "            return url_to_scrape\n",
    "        elif self.category_type == \"nowe\" and self.scrape_continuously == False:\n",
    "            url_to_scrape = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?page=\", str(self.start_page)])\n",
    "            return url_to_scrape\n",
    "        elif self.category_type == \"search\" and self.scrape_continuously == False:\n",
    "            searched_article = str(self.searched_article.replace(\" \",\"%20\"))\n",
    "            url_to_scrape = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?q=\",\n",
    "                                    searched_article, \"&page=\", str(self.start_page)])\n",
    "            return url_to_scrape\n",
    "        else:\n",
    "            raise Exception(f\"The variables were defined incorrectly.\")\n",
    "\n",
    "    def infinite_scroll_handling(self) -> List[str]:\n",
    "        \"\"\"Handling scraping through subsequent pages.\"\"\"\n",
    "        try:\n",
    "            flag = True\n",
    "            retrived_articles = list()\n",
    "            while flag:\n",
    "                url_to_scrape = self.select_url()\n",
    "                soup = self.scrape_page(url_to_scrape)\n",
    "                flag_nowe = CheckConditions(soup).check_if_last_page_nowe()\n",
    "                flag_search = CheckConditions(soup).check_if_last_page_search()\n",
    "\n",
    "                if flag_nowe == False or flag_search == False:\n",
    "                    flag = False\n",
    "                flag = CheckConditions(soup).check_if_no_items_found()\n",
    "\n",
    "                articles = soup.find_all('article')\n",
    "                retrived_articles += articles\n",
    "\n",
    "                if len(retrived_articles) >= self.articles_to_retrieve:\n",
    "                    flag = False\n",
    "                else:\n",
    "                    if self.scrape_continuously == True:\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.start_page += 1\n",
    "                \n",
    "                prepared_articles = articles[:self.articles_to_retrieve]\n",
    "                all_items = self.get_items_details(prepared_articles)\n",
    "            return all_items\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Infinite scroll failed:\\\n",
    "                            {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_items_details_depending_on_the_function(self):\n",
    "        \"\"\"Completing the list of articles and extracting data details depending on the type of scraping.\"\"\"\n",
    "        if self.scrape_continuously == True and self.scrape_choosen_data == False:\n",
    "            while True:\n",
    "                all_items = self.infinite_scroll_handling()\n",
    "                return all_items\n",
    "        elif self.scrape_continuously == False and self.scrape_choosen_data == True:\n",
    "            all_items = self.infinite_scroll_handling()\n",
    "            return all_items\n",
    "        else:\n",
    "            raise Exception(f\"Matching get_items_details depending on the selected \\\n",
    "                            functionality failed. \\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    \n",
    "    def get_items_details(self, prepared_articles):\n",
    "        \"\"\"Getting item detailes.\"\"\"\n",
    "        start_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        all_items = list()\n",
    "        try:\n",
    "            for article in prepared_articles:\n",
    "                item = list()\n",
    "                item.extend([GetItemId(article).get_data(), \n",
    "                            GetItemName(article).get_data(),\n",
    "                            GetItemDiscountPrice(article).get_data(),\n",
    "                            GetItemPercentageDiscount(article).get_data(),\n",
    "                            GetItemRegularPrice(article).get_data(),\n",
    "                            GetItemAddedDate(article).get_data(),\n",
    "                            GetItemUrl(article).get_data()])\n",
    "                if item not in all_items:\n",
    "                    all_items.append(item)\n",
    "                if '' in item:\n",
    "                    logging.warning(\"Data retrieving failed. None values detected\")\n",
    "                    break\n",
    "                if self.to_csv:\n",
    "                    self.save_data_to_csv(item)\n",
    "                if self.to_database:\n",
    "                    print(\"to_database\")\n",
    "                    pass\n",
    "                    #LoadItemDetailsToDatabase(item).load_to_db()\n",
    "            return all_items\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item details failed :\\\n",
    "                        {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "        end_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = end_time - start_time\n",
    "\n",
    "        if self.to_statistics:\n",
    "            try:\n",
    "                stats_info = self.get_scraping_stats_info(action_execution_datetime)\n",
    "                pass\n",
    "                #LoadScrapingStatisticsToDatabase(stats_info).load_to_db()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Populating ScrapingStatistics table failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "        return all_items\n",
    "\n",
    "    def save_data_to_csv(self, item) -> None:\n",
    "        \"\"\"Saving data to csv file.\"\"\"\n",
    "        try:\n",
    "            header = False\n",
    "            if not os.path.exists('scraped.csv'):\n",
    "                header = True\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "            else:\n",
    "                header = False\n",
    "                df_e = pd.read_csv('scraped.csv')\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                if df['item_id'][0] not in df_e['item_id'].tolist():\n",
    "                    df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Saving data to csv failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_scraping_stats_info(self, action_execution_datetime: datetime) -> List[Union[str, int, bool, float]]:\n",
    "        \"\"\"Getting scraping stats info.\"\"\"\n",
    "        stats_info = list()\n",
    "\n",
    "        category_type = self.category_type\n",
    "        retrieved_articles_quantity = self.articles_to_retrieve\n",
    "        time_of_the_action = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = action_execution_datetime\n",
    "        searched_article = self.searched_article\n",
    "        to_csv = self.to_csv\n",
    "        to_database  = self.to_database\n",
    "        scrape_continuously = self.scrape_continuously\n",
    "        scrape_choosen_data = self.scrape_choosen_data\n",
    "\n",
    "        stats=[category_type, retrieved_articles_quantity,\n",
    "            time_of_the_action, action_execution_datetime, searched_article,\n",
    "            to_csv, to_database, scrape_continuously, scrape_choosen_data]\n",
    "\n",
    "        for field in stats:\n",
    "            stats_info.append(field)\n",
    "\n",
    "        return stats_info\n",
    "\n",
    "    \n",
    "\n",
    "class CheckConditions:\n",
    "\n",
    "    def __init__(self, soup: BeautifulSoup) -> None:\n",
    "        self.soup = soup\n",
    "\n",
    "    def check_if_last_page_nowe(self) -> bool:\n",
    "        \"\"\"Checking 'nowe' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h1', {\"class\":\"size--all-xl size--fromW3-xxl text--b space--b-2\"})[0].get_text()\n",
    "            if searched_ending_string.startswith(\"Ups\"):\n",
    "                logging.warning(\"No more pages to scrape.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    def check_if_last_page_search(self) -> bool:\n",
    "        \"\"\"Checking 'search' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-l\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"box--all-i size--all-s vAlign--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t Okazje()\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number > 0:\n",
    "                logging.warning(\"No more pages to scrape.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    def check_if_no_items_found(self) -> bool:\n",
    "        \"\"\"Checking if searched item was found.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-l\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"box--all-i size--all-s vAlign--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t Okazje()\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number == 0:\n",
    "                logging.warning(\"The searched item was not found.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "\n",
    "class GetItemName:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            name = self.article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "            return name\n",
    "        except IndexError as e:\n",
    "            name = self.article.find_all(attrs={'class': \"thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "            return name\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item name failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemId:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> int:\n",
    "        try:\n",
    "            item_id = self.article.get(\"id\")\n",
    "            item_id = item_id.strip('thread_')\n",
    "            item_id = int(item_id)\n",
    "            return item_id\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item id failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemDiscountPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "            if len(discount_price) == 0:\n",
    "                discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl text--color-greyShade\"})\n",
    "\n",
    "            if len(discount_price) > 0:\n",
    "                discount_price = discount_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ','')\n",
    "                if discount_price == \"ZADARMO\":\n",
    "                    discount_price = float(0)\n",
    "                else:\n",
    "                    discount_price = float(discount_price)                \n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                discount_price = \"NA\"\n",
    "            return discount_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item discount price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemRegularPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            regular_price = self.article.find_all(attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"})\n",
    "            if len(regular_price) > 0:\n",
    "                regular_price = float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ',''))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                regular_price = \"NA\"\n",
    "            return regular_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item regular price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemPercentageDiscount:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            percentage_discount = self.article.find_all(attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            if len(percentage_discount) > 0:\n",
    "                percentage_discount = float(percentage_discount[0].get_text().strip('%'))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                percentage_discount = \"NA\"\n",
    "            return percentage_discount\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item percentage discount failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemUrl:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            item_url = self.article.find_all('a', {\"class\":\"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['href']\n",
    "            return item_url\n",
    "        except IndexError as e:\n",
    "            item_url = 'NA - Login Required.'\n",
    "            return item_url\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item url failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemAddedDate:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_raw_data(self) -> List[str]:\n",
    "        try:\n",
    "            date_tag = self.article.find_all('div', {\"class\":\"size--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "            raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "            return raw_string_list\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item added date failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            filtered_list = self.clean_list()\n",
    "            filtered_list = self.check_missing_date()\n",
    "            date_string_likely = filtered_list[0]\n",
    "            if date_string_likely == \"NA\":\n",
    "                url_with_item = GetItemUrl.get_data(self)\n",
    "                soup = self.scrape_page(url_with_item)\n",
    "                prepared_data = self.fill_missing_date(soup)\n",
    "                return prepared_data\n",
    "            else:\n",
    "                stripped_date_string_likely = self.strip_date_string(date_string_likely)\n",
    "                prepared_data = self.date_format_conversion(stripped_date_string_likely)\n",
    "            return prepared_data\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name: {e}\")\n",
    "\n",
    "    def strip_date_string(self, date_string_likely: str) -> str:\n",
    "        try:\n",
    "            if date_string_likely.startswith(\"Zaktualizowano \") and date_string_likely.endswith(\" temu\"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                stripped_date_string_likely = stripped_date_string_likely.rstrip(\" temu\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.endswith(\"Lokalnie\"):\n",
    "                stripped_date_string_likely = date_string_likely.rstrip(\"Lokalnie\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.startswith(\"Zaktualizowano \"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                return stripped_date_string_likely\n",
    "            else:\n",
    "                stripped_date_string_likely = date_string_likely\n",
    "                return stripped_date_string_likely\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Stripping date string failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def date_format_conversion(self, stripped_date_string_likely: str) -> str:\n",
    "        try:\n",
    "            if stripped_date_string_likely.endswith(('min', 'g', 's')):\n",
    "                prepared_data = date.today().strftime(\"%Y-%m-%d\")\n",
    "                return prepared_data\n",
    "            elif stripped_date_string_likely.startswith(tuple(Months.keys())) and len(stripped_date_string_likely) < 8:\n",
    "                if len(stripped_date_string_likely[4:]) == 3:\n",
    "                    day = stripped_date_string_likely[4:6]\n",
    "                else:\n",
    "                    day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = str(date.today().year)\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_1, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:6]\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[8:13]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_2, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[7:12]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Data format conversion tailed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def clean_list(self) -> List[str]:\n",
    "        raw_string_list = self.get_raw_data()\n",
    "        items_to_remove = list()\n",
    "        filtered_list = list()\n",
    "        try:\n",
    "            for string in raw_string_list:\n",
    "                if \"/\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if \":\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\", \"Stacjonarnie\"]:\n",
    "                    items_to_remove.append(string)\n",
    "                if string.startswith(\"Wysyłka\"):\n",
    "                    items_to_remove.append(string)\n",
    "            counts = Counter(items_to_remove)\n",
    "            for string in raw_string_list:\n",
    "                if counts[string]:\n",
    "                    counts[string] -= 1\n",
    "                else:\n",
    "                    filtered_list.append(string)\n",
    "            return filtered_list\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "    def check_missing_date(self) -> List[str]:\n",
    "        filtered_list = self.clean_list()\n",
    "        try:\n",
    "            if len(filtered_list) == 0:\n",
    "                filtered_list.append(\"NA\")\n",
    "                return filtered_list\n",
    "            else:\n",
    "                return filtered_list\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "    def fill_missing_date(self, soup) -> str:\n",
    "        try:\n",
    "            date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "            filtered_list = date_string.split()\n",
    "            day_string = filtered_list[0]\n",
    "            month_string = filtered_list[1]\n",
    "            year_string = filtered_list[2].strip(',')\n",
    "            if len(day_string[0]) == 2:\n",
    "                day = day_string\n",
    "            else:\n",
    "                day = day_string.zfill(2)\n",
    "            month = Months.__members__[month_string].value\n",
    "            year = year_string\n",
    "            prepared_data = '-'.join([year, month, day])\n",
    "            return prepared_data\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "    def scrape_page(self, url_with_item: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options) #for local  \n",
    "                #driver = webdriver.Remote(command_executor=f'http://{CustomEnvironment.get_selenium_container_name()}:4444/wd/hub', options=options) #for docker \n",
    "            driver.get(url_with_item)\n",
    "            time.sleep(0.7)\n",
    "            page_with_item = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page_with_item, 'html5lib')\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "\n",
    "category_type = \"nowe\"\n",
    "articles_to_retrieve = 45\n",
    "searched_article = \"S22\"\n",
    "scrape_continuously = False\n",
    "scrape_choosen_data = True\n",
    "to_database = False\n",
    "to_csv = False\n",
    "to_statistics = False\n",
    "\n",
    "\n",
    "\n",
    "path_to_file = Path(\"source/pepper_app/tests/fixtures/to_test_get_info/soup_with_duplicates.html\")\n",
    "\n",
    "with open(path_to_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = file.read()\n",
    "soup = BeautifulSoup(soup, \"html5lib\")\n",
    "articles = soup.find_all('article')\n",
    "retrived_articles = articles[1:]\n",
    "\n",
    "all_items = ScrapePage(category_type=category_type, articles_to_retrieve=articles_to_retrieve, scrape_continuously=scrape_continuously, \\\n",
    "        to_database=to_database, to_csv=to_csv, to_statistics=to_statistics).get_items_details(retrived_articles)\n",
    "\n",
    "print(len(all_items))\n",
    "\n",
    "for i in all_items:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[799284, 'Kod Hebe - 10% lub 20% przy MWZ 99 lub 199 zł - na oznaczone produkty', 'NA', 'NA', 'NA', '2024-02-20', 'https://www.pepper.pl/kupony/kod-hebe-10-lub-20-przy-mwz-99-lub-199-zl-na-oznaczone-produkty-799284']\n",
      "[799283, 'Piła Stołowa DWE7485 (1850 W, 210 mm, maks. zakres cięcia 65 mm)', 1495.06, -26.0, 2022.07, '2024-02-20', 'https://www.pepper.pl/promocje/pila-stolowa-dwe7485-1850-w-210-mm-maks-zakres-ciecia-65-mm-799283']\n",
      "[799277, 'Projektor Lenovo Thinkplus Air H6 1080P HDR10 z EU za $299.99 / ~1206zł', 1206.0, -17.0, 1459.0, '2024-02-20', 'https://www.pepper.pl/promocje/projektor-lenovo-thinkplus-air-h6-z-eu-za-29999-1206zl-799277']\n",
      "[799281, '2 dni w Tropical Islands (bilety wstępu) + nocleg w namiocie ze śniadaniami na terenie parku za 512 zł / 2 osoby @ Travelcircus', 512.0, -32.0, 752.0, '2024-02-20', 'https://www.pepper.pl/promocje/2-dni-w-tropical-islands-bilety-wstepu-nocleg-w-namiocie-ze-sniadaniami-na-terenie-parku-za-512-zl-2-osoby-at-travelcircus-799281']\n",
      "[799279, 'Wkrętak elektryczny XIAOMI AtuMan DUKA E2 210RPM za $25.99 / ~105zł', 105.0, -36.0, 164.0, '2024-02-20', 'https://www.pepper.pl/promocje/wkretak-elektryczny-xiaomi-atuman-duka-e2-210rpm-za-2599-105zl-799279']\n",
      "[799280, 'GoMaihe Pojemniki na zapasy, zestaw 16 sztuk, pudełko do przechowywania, hermetyczne pojemniki', 87.13, 'NA', 'NA', '2024-02-20', 'https://www.pepper.pl/promocje/gomaihe-pojemniki-na-zapasy-zestaw-16-sztuk-pudelko-do-przechowywania-hermetyczne-pojemniki-799280']\n",
      "[790476, 'Nescafe Classic Rozpuszczalna 200g ogólnopolska @Lidl WojnaCenowaLidlBiedra', 7.99, 'NA', 'NA', '2024-02-20', 'https://www.pepper.pl/promocje/nescafe-classic-rozpuszczalna-200g-at-lidl-790476']\n",
      "[799276, 'Fallout S.P.E.C.I.A.L. Anthology PL (PC) - STEAM', 219.0, -12.0, 249.0, '2024-02-20', 'https://www.pepper.pl/promocje/fallout-special-anthology-pl-pc-steam-799276']\n",
      "[799275, 'Piwo Tatra Jasne Pełne, 0,5l, puszka w Biedronka', 1.29, -19.0, 1.59, '2024-02-20', 'https://www.pepper.pl/promocje/piwo-tatra-05l-puszka-w-biedronka-799275']\n",
      "[799274, 'The Damned A Night of a Thousand Vampires 2CD/Bluray', 39.64, 'NA', 'NA', '2024-02-20', 'https://www.pepper.pl/promocje/the-damned-a-night-of-a-thousand-vampires-2cdbluray-799274']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "from datetime import datetime, timedelta, date, timezone\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "import signal\n",
    "import traceback\n",
    "from source.pepper_app.constans import OLD_DATES_DATA_PATTERN_1, OLD_DATES_DATA_PATTERN_2\n",
    "\n",
    "from source.pepper_app.environment_config import CustomEnvironment\n",
    "from source.pepper_app.constans import DATA_HEADER, STATS_HEADER, REQUEST_HEADER, RESPONSE_HEADER\n",
    "\n",
    "from collections import Counter\n",
    "from requests.exceptions import ConnectionError, HTTPError, MissingSchema, ReadTimeout\n",
    "from source.pepper_app.constans import (CSV_COLUMNS,\n",
    "                                STATS_HEADER)\n",
    "\n",
    "class ScrapPage:\n",
    "\n",
    "\n",
    "    def __init__(self, category_type: str, articles_to_retrieve: int, to_csv: bool=False,\n",
    "                to_database: bool=False, to_statistics: bool=True, searched_article: str='NA', \n",
    "                scrap_continuously: bool=False, scrap_choosen_data: bool=True) -> None:\n",
    "        self.category_type = category_type\n",
    "        self.articles_to_retrieve = articles_to_retrieve\n",
    "        self.to_database = to_database\n",
    "        self.to_csv = to_csv\n",
    "        self.to_statistics = to_statistics\n",
    "        self.start_page = 1\n",
    "        self.searched_article = searched_article\n",
    "        self.scrap_continuously = scrap_continuously\n",
    "        self.scrap_choosen_data = scrap_choosen_data\n",
    "\n",
    "    def scrap_page(self, url_to_scrap: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "        \"\"\"Setting up selenium webdriver, scraping page with bs4.\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            #options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options)\n",
    "                #driver = webdriver.Remote(command_executor='http://selenium-hub:4444/wd/hub', options=options)\n",
    "            driver.set_window_size(1400,1000)\n",
    "            driver.get(url_to_scrap)\n",
    "            time.sleep(0.7)\n",
    "            page = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page, \"html5lib\")\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "    def select_url(self) -> str:\n",
    "        \"\"\"Selection of the website address depending on the type of scrapping.\"\"\"\n",
    "        if self.scrap_continuously == True:\n",
    "            url_to_scrap = \"\".join([CustomEnvironment.get_url(), \"nowe\"])\n",
    "            return url_to_scrap\n",
    "        elif self.category_type == \"nowe\" and self.scrap_continuously == False:\n",
    "            url_to_scrap = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?page=\", str(self.start_page)])\n",
    "            return url_to_scrap\n",
    "        elif self.category_type == \"search\" and self.scrap_continuously == False:\n",
    "            searched_article = str(self.searched_article.replace(\" \",\"%20\"))\n",
    "            url_to_scrap = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?q=\",\n",
    "                                    searched_article, \"&page=\", str(self.start_page)])\n",
    "            return url_to_scrap\n",
    "        else:\n",
    "            raise Exception(f\"The variables were defined incorrectly.\")\n",
    "\n",
    "\n",
    "    def infinite_scroll_handling(self) -> List[str]:\n",
    "        \"\"\"Handling scraping through subsequent pages.\"\"\"\n",
    "        try:\n",
    "            flag = True\n",
    "            retrived_articles = list()\n",
    "            while flag:\n",
    "                url_to_scrap = self.select_url()\n",
    "                soup = self.scrap_page(url_to_scrap)\n",
    "                flag_nowe = CheckConditions(soup).check_if_last_page_nowe()\n",
    "                flag_search = CheckConditions(soup).check_if_last_page_search()\n",
    "                if flag_nowe == False or flag_search == False:\n",
    "                    flag = False\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                flag = CheckConditions(soup).check_if_no_items_found()\n",
    "                if flag == False:\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                if flag == True:\n",
    "                    articles = soup.find_all('article')\n",
    "                    retrived_articles += articles\n",
    "                else:\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                if len(retrived_articles) >= int(self.articles_to_retrieve):\n",
    "                    flag = False\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                self.start_page += 1\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Infinite scroll failed:\\\n",
    "                            {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "\n",
    "    def get_items_details_depending_on_the_function(self):\n",
    "        \"\"\"Completing the list of articles and extracting data details depending on the type of scrapping.\"\"\"\n",
    "        if self.scrap_continuously == True and self.scrap_choosen_data == False:\n",
    "            while True:\n",
    "                retrived_articles = self.scrap_continuously_by_refreshing_page()\n",
    "                self.get_items_details(retrived_articles)\n",
    "        elif self.scrap_continuously == False and self.scrap_choosen_data == True:\n",
    "            retrived_articles = self.infinite_scroll_handling()\n",
    "            all_items = self.get_items_details(retrived_articles)\n",
    "            return all_items\n",
    "        else:\n",
    "            raise Exception(f\"Matching get_items_details depending on the selected \\\n",
    "                            functionality failed. \\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "    def get_items_details(self, retrived_articles) -> list():\n",
    "        \"\"\"Getting item detailes.\"\"\"\n",
    "        start_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        all_items = list()\n",
    "        try:\n",
    "            for article in retrived_articles:\n",
    "                item = list()\n",
    "                item.append(GetItemId(article).get_data())\n",
    "                item.append(GetItemName(article).get_data())\n",
    "                item.append(GetItemDiscountPrice(article).get_data())\n",
    "                item.append(GetItemPercentageDiscount(article).get_data())\n",
    "                item.append(GetItemRegularPrice(article).get_data())\n",
    "                item.append(GetItemAddedDate(article).get_data())\n",
    "                item.append(GetItemUrl(article).get_data())\n",
    "                if item not in all_items:\n",
    "                    all_items.append(item)\n",
    "                if '' in item:\n",
    "                    logging.warning(\"Data retrieving failed. None values detected\")\n",
    "                    break\n",
    "                if self.to_csv:\n",
    "                    self.save_data_to_csv(item)\n",
    "                #if self.to_database:\n",
    "                #    LoadItemDetailsToDatabase(item).load_to_db()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item details failed :\\\n",
    "                        {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "        end_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = end_time - start_time\n",
    "\n",
    "        if self.to_statistics:\n",
    "            try:\n",
    "                stats_info = self.get_scraping_stats_info(action_execution_datetime)\n",
    "                #LoadScrapingStatisticsToDatabase(stats_info).load_to_db()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Populating ScrapingStatistics table failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "        return all_items\n",
    "\n",
    "    def save_data_to_csv(self, item) -> None:\n",
    "        \"\"\"Saving data to csv file.\"\"\"\n",
    "        try:\n",
    "            header = False\n",
    "            if not os.path.exists('scraped.csv'):\n",
    "                header = True\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "            else:\n",
    "                header = False\n",
    "                df_e = pd.read_csv('scraped.csv')\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                if df['item_id'][0] not in df_e['item_id'].tolist():\n",
    "                    df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Saving data to csv failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "    def get_scraping_stats_info(self, action_execution_datetime: datetime) -> List[Union[str, int, bool, float]]:\n",
    "        \"\"\"Getting scraping stats info.\"\"\"\n",
    "        stats_info = list()\n",
    "\n",
    "        category_type = self.category_type\n",
    "        retrieved_articles_quantity = self.articles_to_retrieve\n",
    "        time_of_the_action = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = action_execution_datetime\n",
    "        searched_article = self.searched_article\n",
    "        to_csv = self.to_csv\n",
    "        to_database  = self.to_database\n",
    "        scrap_continuously = self.scrap_continuously\n",
    "        scrap_choosen_data = self.scrap_choosen_data\n",
    "\n",
    "        stats=[category_type, retrieved_articles_quantity,\n",
    "            time_of_the_action, action_execution_datetime, searched_article,\n",
    "            to_csv, to_database, scrap_continuously, scrap_choosen_data]\n",
    "\n",
    "        for field in stats:\n",
    "            stats_info.append(field)\n",
    "\n",
    "        return stats_info\n",
    "\n",
    "\n",
    "    def scrap_continuously_by_refreshing_page(self) -> List[str]:\n",
    "        \"\"\"Scraping data function for continuously scraping feature.\"\"\"\n",
    "        retrived_articles = list()\n",
    "\n",
    "        \n",
    "        url_to_scrap = self.select_url()\n",
    "        soup = self.scrap_page(url_to_scrap)\n",
    "        time.sleep(20)\n",
    "        articles = soup.find_all('article')\n",
    "        retrived_articles += articles\n",
    "        print(retrived_articles)\n",
    "\n",
    "        #return retrived_articles\n",
    "\n",
    "\n",
    "class CheckConditions:\n",
    "\n",
    "\n",
    "    def __init__(self, soup: BeautifulSoup) -> None:\n",
    "        self.soup = soup\n",
    "\n",
    "\n",
    "    def check_if_last_page_nowe(self) -> bool:\n",
    "        \"\"\"Checking 'nowe' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h1', {\"class\":\"size--all-xl size--fromW3-xxl text--b space--b-2\"})[0].get_text()\n",
    "            if searched_ending_string.startswith(\"Ups\"):\n",
    "                logging.warning(\"No more pages to scrap.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    def check_if_last_page_search(self) -> bool:\n",
    "        \"\"\"Checking 'search' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-l\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"box--all-i size--all-s vAlign--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t Okazje()\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number > 0:\n",
    "                logging.warning(\"No more pages to scrap.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    def check_if_no_items_found(self) -> bool:\n",
    "        \"\"\"Checking if searched item was found.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-l\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"box--all-i size--all-s vAlign--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t Okazje()\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number == 0:\n",
    "                logging.warning(\"The searched item was not found.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "\n",
    "class GetItemName:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            name = self.article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "            return name\n",
    "        except IndexError as e:\n",
    "            name = self.article.find_all(attrs={'class': \"thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "            return name\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item name failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemId:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> int:\n",
    "        try:\n",
    "            item_id = self.article.get(\"id\")\n",
    "            item_id = item_id.strip('thread_')\n",
    "            item_id = int(item_id)\n",
    "            return item_id\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item id failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemDiscountPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            discount_price = self.article.find_all(\"span\", {'class': \"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\"})\n",
    "            if len(discount_price) == 0:\n",
    "                discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl text--color-greyShade\"})\n",
    "\n",
    "            if len(discount_price) > 0:\n",
    "                discount_price = discount_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ','')\n",
    "                if discount_price == \"ZADARMO\":\n",
    "                    discount_price = float(0)\n",
    "                else:\n",
    "                    discount_price = float(discount_price)                \n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                discount_price = \"NA\"\n",
    "            return discount_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item discount price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "\n",
    "class GetItemRegularPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            regular_price = self.article.find_all(\"span\", {'class': \"mute--text text--lineThrough space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            if len(regular_price) > 0:\n",
    "                regular_price = float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ',''))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                regular_price = \"NA\"\n",
    "            return regular_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item regular price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "\n",
    "class GetItemPercentageDiscount:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            percentage_discount = self.article.find_all(\"span\", {'class': \"text--color-charcoal space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            if len(percentage_discount) > 0:\n",
    "                percentage_discount = float(percentage_discount[0].get_text().strip('%'))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                percentage_discount = \"NA\"\n",
    "            return percentage_discount\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item percentage discount failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemUrl:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            item_url = self.article.find_all(attrs={'class', 'cept-tt thread-link linkPlain thread-title--list js-thread-title'})[0]['href']\n",
    "            return item_url\n",
    "        except IndexError as e:\n",
    "            item_url = 'NA - Login Required.'\n",
    "            return item_url\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item url failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemAddedDate:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_raw_data(self) -> List[str]:\n",
    "\n",
    "        try:\n",
    "            date_tag = self.article.find_all('div', {\"class\":\"flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "            raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "            return raw_string_list\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item added date failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "\n",
    "        try:\n",
    "            filtered_list = self.clean_list()\n",
    "            filtered_list = self.check_missing_date()\n",
    "            date_string_likely = filtered_list[0]\n",
    "            if date_string_likely == \"NA\":\n",
    "                url_with_item = GetItemUrl.get_data(self)\n",
    "                soup = self.scrap_page(url_with_item)\n",
    "                prepared_data = self.fill_missing_date(soup)\n",
    "                return prepared_data\n",
    "            else:\n",
    "                stripped_date_string_likely = self.strip_date_string(date_string_likely)\n",
    "                prepared_data = self.date_format_conversion(stripped_date_string_likely)\n",
    "            return prepared_data\n",
    "\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name: {e}\")\n",
    "\n",
    "    def strip_date_string(self, date_string_likely: str) -> str:\n",
    "\n",
    "        try:\n",
    "            if date_string_likely.startswith(\"Zaktualizowano \") and date_string_likely.endswith(\" temu\"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                stripped_date_string_likely = stripped_date_string_likely.rstrip(\" temu\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.endswith(\"Lokalnie\"):\n",
    "                stripped_date_string_likely = date_string_likely.rstrip(\"Lokalnie\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.startswith(\"Zaktualizowano \"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                return stripped_date_string_likely\n",
    "            else:\n",
    "                stripped_date_string_likely = date_string_likely\n",
    "                return stripped_date_string_likely\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Stripping date string failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def date_format_conversion(self, stripped_date_string_likely: str) -> str:\n",
    "\n",
    "        try:\n",
    "            if stripped_date_string_likely.endswith(('min', 'g', 's')):\n",
    "                prepared_data = date.today().strftime(\"%Y-%m-%d\")\n",
    "                return prepared_data\n",
    "            elif stripped_date_string_likely.startswith(tuple(Months.keys())) and len(stripped_date_string_likely) < 8:\n",
    "                if len(stripped_date_string_likely[4:]) == 3:\n",
    "                    day = stripped_date_string_likely[4:6]\n",
    "                else:\n",
    "                    day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = str(date.today().year)\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_1, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:6]\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[8:13]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_2, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[7:12]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Data format conversion tailed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "    def clean_list(self) -> List[str]:\n",
    "\n",
    "        raw_string_list = self.get_raw_data()\n",
    "        items_to_remove = list()\n",
    "        filtered_list = list()\n",
    "\n",
    "        try:\n",
    "            for string in raw_string_list:\n",
    "                if \"/\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if \":\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\", \"Stacjonarnie\"]:\n",
    "                    items_to_remove.append(string)\n",
    "                if string.startswith(\"Wysyłka\"):\n",
    "                    items_to_remove.append(string)\n",
    "\n",
    "            counts = Counter(items_to_remove)\n",
    "\n",
    "            for string in raw_string_list:\n",
    "                if counts[string]:\n",
    "                    counts[string] -= 1\n",
    "                else:\n",
    "                    filtered_list.append(string)\n",
    "            return filtered_list\n",
    "\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "\n",
    "    def check_missing_date(self) -> List[str]:\n",
    "\n",
    "        filtered_list = self.clean_list()\n",
    "\n",
    "        try:\n",
    "            if len(filtered_list) == 0:\n",
    "                filtered_list.append(\"NA\")\n",
    "                return filtered_list\n",
    "            else:\n",
    "                return filtered_list\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "\n",
    "    def fill_missing_date(self, soup) -> str:\n",
    "\n",
    "        try:\n",
    "            date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "            filtered_list = date_string.split()\n",
    "            day_string = filtered_list[0]\n",
    "            month_string = filtered_list[1]\n",
    "            year_string = filtered_list[2].strip(',')\n",
    "\n",
    "            if len(day_string[0]) == 2:\n",
    "                day = day_string\n",
    "            else:\n",
    "                day = day_string.zfill(2)\n",
    "\n",
    "            month = Months.__members__[month_string].value\n",
    "            year = year_string\n",
    "            prepared_data = '-'.join([year, month, day])\n",
    "            return prepared_data\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "\n",
    "    def scrap_page(self, url_with_item: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options)\n",
    "                #driver = webdriver.Remote(command_executor='http://selenium-hub:4444/wd/hub', options=options)\n",
    "            driver.get(url_with_item)\n",
    "            time.sleep(0.7)\n",
    "            page_with_item = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page_with_item, 'html5lib')\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "category_type = \"nowe\"\n",
    "articles_to_retrieve = 10\n",
    "searched_article = \"S22\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output = ScrapPage(category_type, articles_to_retrieve)\n",
    "all_items = output.get_items_details_depending_on_the_function()\n",
    "\n",
    "\n",
    "for i in all_items:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cfehome'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdjango_for_jupyt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_django\n\u001b[0;32m----> 2\u001b[0m \u001b[43minit_django\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpepper_app\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pepper-best-deals/django_for_jupyt.py:19\u001b[0m, in \u001b[0;36minit_django\u001b[0;34m(project_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDJANGO_ALLOW_ASYNC_UNSAFE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdjango\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mdjango\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pepper-best-deals/env/lib/python3.10/site-packages/django/__init__.py:19\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(set_prefix)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdjango\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01murls\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_script_prefix\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdjango\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m configure_logging\n\u001b[0;32m---> 19\u001b[0m configure_logging(\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLOGGING_CONFIG\u001b[49m, settings\u001b[38;5;241m.\u001b[39mLOGGING)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m set_prefix:\n\u001b[1;32m     21\u001b[0m     set_script_prefix(\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mFORCE_SCRIPT_NAME \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mFORCE_SCRIPT_NAME\n\u001b[1;32m     23\u001b[0m     )\n",
      "File \u001b[0;32m~/pepper-best-deals/env/lib/python3.10/site-packages/django/conf/__init__.py:102\u001b[0m, in \u001b[0;36mLazySettings.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the value of a setting and cache it in self.__dict__.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (_wrapped \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped) \u001b[38;5;129;01mis\u001b[39;00m empty:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     _wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped\n\u001b[1;32m    104\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_wrapped, name)\n",
      "File \u001b[0;32m~/pepper-best-deals/env/lib/python3.10/site-packages/django/conf/__init__.py:89\u001b[0m, in \u001b[0;36mLazySettings._setup\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     81\u001b[0m     desc \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name) \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ImproperlyConfigured(\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, but settings are not configured. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must either define the environment variable \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor call settings.configure() before accessing settings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;241m%\u001b[39m (desc, ENVIRONMENT_VARIABLE)\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped \u001b[38;5;241m=\u001b[39m \u001b[43mSettings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pepper-best-deals/env/lib/python3.10/site-packages/django/conf/__init__.py:217\u001b[0m, in \u001b[0;36mSettings.__init__\u001b[0;34m(self, settings_module)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# store the settings module in case someone later cares\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSETTINGS_MODULE \u001b[38;5;241m=\u001b[39m settings_module\n\u001b[0;32m--> 217\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSETTINGS_MODULE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m tuple_settings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALLOWED_HOSTS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINSTALLED_APPS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSECRET_KEY_FALLBACKS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_explicit_settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cfehome'"
     ]
    }
   ],
   "source": [
    "from django_for_jupyt import init_django\n",
    "init_django(\"pepper_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROMOCJA', 'Samsung', 'Galaxy', 'S23', 'z', 'kodem', 'APP5']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from django.db.models import Q\n",
    "\n",
    "import pandas as pd\n",
    "from source.pepper_app.constans import DATA_HEADER\n",
    "\n",
    "\n",
    "data = [1, 'PROMOCJA Samsung Galaxy S23 z kodem APP5', 2500, 10, 2750, '2024-01-07', 'https://www.pepper.pl/promocje/samsungs23']\n",
    "item_df = pd.DataFrame([data], columns=DATA_HEADER)\n",
    "\n",
    "\n",
    "def conditions(item_df):\n",
    "    # Initialize conditions with a default condition\n",
    "    conditions = Q()\n",
    "\n",
    "    for _, row in item_df.iterrows():\n",
    "        article_name = row[\"name\"]\n",
    "        list_of_words_article_name = article_name.split()\n",
    "\n",
    "        # Create a new condition for each word in the article name\n",
    "    for word in list_of_words_article_name:\n",
    "        conditions &= Q(desired_article__icontains=word)\n",
    "\n",
    "    # Filter UserRequest objects based on the conditions and order by request_time\n",
    "    #results = UserRequest.objects.filter(conditions).order_by('request_time')\n",
    "\n",
    "    return list_of_words_article_name\n",
    "\n",
    "\n",
    "print(conditions(item_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
