{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from enum import Enum, IntEnum\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_scrap = \"\"\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.set_window_size(1400,1000)\n",
    "driver.get(\"https://www.pepper.pl/nowe\")\n",
    "time.sleep(0.7)\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, \"html5lib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1zł\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m name \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind_all(attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcept-tt thread-link linkPlain thread-title--list js-thread-title\u001b[39m\u001b[38;5;124m\"\u001b[39m})[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m discount_price \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdiscount_price\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mget_text())\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "articles = soup.find_all('article')\n",
    "for article in articles:\n",
    "    name = article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "    discount_price = article.find_all(\"span\", {'class': \"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\"})\n",
    "\n",
    "    print(discount_price[0].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m date_tag \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      6\u001b[0m discount_price \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind_all(attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread-price text--b cept-tp size--all-l size--fromW3-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m----> 7\u001b[0m raw_string_list \u001b[38;5;241m=\u001b[39m \u001b[43mdate_tag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m items_to_remove \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m      9\u001b[0m filtered_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "File \u001b[0;32m~/pepper-best-deals/env/lib/python3.10/site-packages/bs4/element.py:2428\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   2427\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   2429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResultSet object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key\n\u001b[1;32m   2430\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "articles = soup.find_all('article')\n",
    "for article in articles:\n",
    "    name = article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "    date_tag = article.find_all('div', {\"class\":\"size--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "    discount_price = article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "    raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "    items_to_remove = list()\n",
    "    filtered_list = list()\n",
    "    for string in raw_string_list:\n",
    "        if \"/\" in string:\n",
    "            items_to_remove.append(string)\n",
    "        if \":\" in string:\n",
    "            items_to_remove.append(string)\n",
    "        if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\"]:\n",
    "            items_to_remove.append(string)\n",
    "        if string.startswith(\"Wysyłka\"):\n",
    "            items_to_remove.append(string)\n",
    "    counts = Counter(items_to_remove)\n",
    "    for string in raw_string_list:\n",
    "        if counts[string]:\n",
    "            counts[string] -= 1\n",
    "        else:\n",
    "            filtered_list.append(string)\n",
    "\n",
    "\n",
    "    print(f\"discount price: {discount_price} date: {filtered_list}, name: {name}\")\n",
    "    #print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-03\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "from typing import List, Union\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.set_window_size(1400,1000)\n",
    "driver.get(\"https://www.pepper.pl/nowe\")\n",
    "time.sleep(0.7)\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, \"html5lib\")\n",
    "\n",
    "\n",
    "date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "filtered_list = date_string.split()\n",
    "day_string = filtered_list[0]\n",
    "month_string = filtered_list[1]\n",
    "year_string = filtered_list[2].strip(',')\n",
    "\n",
    "if len(day_string[0]) == 2:\n",
    "    day = day_string\n",
    "else:\n",
    "    day = day_string.zfill(2)\n",
    "\n",
    "month = Months.__members__[month_string].value\n",
    "year = year_string\n",
    "prepared_data = '-'.join([year, month, day])\n",
    "\n",
    "print(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "to_database\n",
      "[778528, 'Ser żółty w plastrach 1 kg Gouda lub Morski @Kaufland', 16.49, -39.0, 26.94, '2024-01-08', 'https://www.pepper.pl/promocje/ser-zolty-w-plastrach-1-kg-gouda-lub-morski-at-kaufland-778528']\n",
      "[778526, 'Paczka chipsów średnia 1zł KiK 80g', 1.0, 'NA', 'NA', '2024-01-08', 'https://www.pepper.pl/promocje/paczka-chipsow-srednia-1zl-kik-80g-778526']\n",
      "[778525, 'Plecak Miejski The North Face VAULT', 139.99, -35.0, 215.0, '2024-01-08', 'https://www.pepper.pl/promocje/plecak-miejski-the-north-face-vault-778525']\n",
      "[778523, 'Kapusta kiszona 520/400g', 0.99, 'NA', 'NA', '2024-01-08', 'https://www.pepper.pl/promocje/kapusta-kiszona-520400g-778523']\n",
      "[778522, 'Żel do prania Passion Gold 4l. - Aldi', 29.99, 'NA', 'NA', '2024-01-08', 'https://www.pepper.pl/promocje/zel-do-prania-4l-aldi-778522']\n",
      "[778521, 'Pakiet Ashampoo Office 9 w bardzo miłej dla portfela cenie ;) (Szczegóły w opisie!)', 40.0, -74.0, 155.54, '2024-01-08', 'https://www.pepper.pl/promocje/pakiet-ashampoo-office-9-w-bardzo-milej-dla-portfela-cenie-778521']\n",
      "[778520, 'Olej słonecznikowy 1 l (bez aplikacji i limitów) @Kaufland', 2.99, -63.0, 7.99, '2024-01-08', 'https://www.pepper.pl/promocje/olej-slonecznikowy-1-l-bez-aplikacji-i-limitow-at-kaufland-778520']\n",
      "[778519, 'Papier toaletowy 3 war. (24 rolki) - Aldi', 17.99, 'NA', 'NA', '2024-01-08', 'https://www.pepper.pl/promocje/papier-toaletowy-3-war-24-rolki-aldi-778519']\n",
      "[778518, 'Bulleit Bourbon Frontier Whiskey (Whisky) 0.7L- 45%- -Stokrotka', 76.99, 'NA', 'NA', '2024-01-08', 'https://www.pepper.pl/promocje/bulleit-bourbon-frontier-whiskey-whisky-07l-45-stokrotka-778518']\n",
      "[778517, 'Kawy z palarni Blue Orca Coffee: Mexicana, Cubana, Brazilliana w Kaufland', 45.0, -35.0, 68.99, '2024-01-08', 'https://www.pepper.pl/promocje/kawy-z-palarni-blue-orca-coffee-mexicana-cubana-brazilliana-w-kaufland-778517']\n",
      "[778516, 'Mleko UHT 1 l 3,2% (bez aplikacji i limitów) @Kaufland', 1.99, -50.0, 3.99, '2024-01-08', 'https://www.pepper.pl/promocje/mleko-uht-1-l-32-bez-aplikacji-i-limitow-at-kaufland-778516']\n",
      "[778515, 'Gostyńskie mleko w tubie zagęszczone słodzone o smaku karmelowym - Auchan Trzy Stawy', 2.06, 'NA', 'NA', '2024-01-08', 'https://www.pepper.pl/promocje/gostynskie-mleko-w-tubie-zageszczone-slodzone-o-smaku-karmelowym-auchan-trzy-stawy-778515']\n",
      "[778514, 'Gra PC - Firewatch za 9,19 zł @ Steam', 9.19, -49.0, 17.99, '2024-01-08', 'https://www.pepper.pl/promocje/gra-pc-firewatch-za-919-zl-at-steam-778514']\n",
      "[778513, 'LEGO DREAMZzz 71458, Krokodylowy samochód,', 214.0, -2.0, 218.0, '2024-01-08', 'https://www.pepper.pl/promocje/lego-dreamzzz-krokodylowy-samochod-71458-778513']\n",
      "[778511, 'Afnan 9 PM woda perfumowana 100 ml', 109.0, -34.0, 165.0, '2024-01-08', 'https://www.pepper.pl/promocje/afnan-9-pm-woda-perfumowana-100-ml-778511']\n",
      "[766800, \"Cukierki M&M's - orzechowe draże, 300g / darmowa dostawa od 50zł\", 6.79, 'NA', 'NA', '2024-01-08', 'https://www.pepper.pl/promocje/cukierki-mms-orzechowe-draze-300g-766800']\n",
      "[778510, 'Twaróg sernikowy premium Mlekovita Wypasiony 1 kg @Biedronka', 8.99, -44.0, 15.99, '2024-01-08', 'https://www.pepper.pl/promocje/twarog-sernikowy-premium-mlekovita-wypasiony-1-kg-at-biedronka-778510']\n",
      "[778508, 'DMUCHANY ŚLIZG ŚNIEŻNY SANKI PINGWIN METEOR PVC 70x100x40 z uchwytami €14,72', 61.0, -29.0, 86.0, '2024-01-08', 'https://www.pepper.pl/promocje/dmuchany-slizg-sniezny-sanki-pingwin-meteor-pvc-70x100x40-z-uchwytami-778508']\n",
      "[778509, 'Tacx T2092 uchwyt na tablet do roweru wpiętego w trenażer', 176.95, -51.0, 360.0, '2024-01-08', 'https://www.pepper.pl/promocje/tacx-t2092-uchwyt-na-tablet-do-roweru-do-trenazera-778509']\n",
      "[778507, 'Kneipp Krem do kąpieli, mleczko migdałowe + olejek migdałowy 400 ml', 13.7, 'NA', 'NA', '2024-01-08', 'https://www.pepper.pl/promocje/kneipp-krem-do-kapieli-mleczko-migdalowe-olejek-migdalowy-400-ml-778507']\n",
      "[778506, 'Bilet na FUKS 2 do Cinema City dla pierwszych 100 gości Suntago', 'NA', 'NA', 'NA', '2024-01-08', 'https://www.pepper.pl/promocje/bilet-na-fuks-2-do-cinema-city-dla-pierwszych-100-klientow-suntago-778506']\n",
      "[778505, 'HOMLA DO -40% na meble, tutaj Krzesło ARVEN szare 50x56x87 cm', 179.0, 'NA', 'NA', '2024-01-08', 'https://www.pepper.pl/promocje/homla-do-40-na-meble-tutaj-krzeslo-arven-szare-50x56x87-cm-778505']\n",
      "[778500, 'Darmowy pomiar kamerą termowizyjną dla mieszkańców Rybnika', 'NA', 'NA', 'NA', '2024-01-08', 'https://www.pepper.pl/promocje/darmowy-pomiar-kamera-termowizyjna-dla-mieszkancow-rybnika-778500']\n",
      "[778499, 'Gratis do zakupów w sklepie Aleeksalkohole, codziennie coś innego', 'NA', 'NA', 'NA', '2024-01-08', 'https://www.pepper.pl/promocje/w-sklepie-aleeksalkohole-rozdaja-butelki-za-zakupy-jezeli-dobrze-rozumiem-to-codziennie-bedzie-inna-778499']\n",
      "[778497, '2x płyta CD Metallica - Through the Never za 25,29 zł / CD Beyond Magnetic za 26,96 zł', 25.29, -12.0, 28.58, '2024-01-08', 'https://www.pepper.pl/promocje/2x-plyta-cd-metallica-through-the-never-beyond-magnetic-za-2696-zl-778497']\n",
      "[778495, 'Old Spice Whitewater Dezodorant w sztyfcie dla mężczyzn 6 x 50 ml', 74.99, -21.0, 94.99, '2024-01-08', 'https://www.pepper.pl/promocje/old-spice-whitewater-dezodorant-w-sztyfcie-dla-mezczyzn-6-x-50-ml-778495']\n",
      "[778493, 'Kołki Fischer Duopower 8x65, 50 sztuk', 20.6, 'NA', 'NA', '2024-01-08', 'https://www.pepper.pl/promocje/kolki-fischer-duopower-8x65-50-sztuk-778493']\n",
      "[778487, 'Mandarynki opak. 1 kg @Lidl', 2.99, -63.0, 7.99, '2024-01-08', 'https://www.pepper.pl/promocje/mandarynki-opak-1-kg-at-lidl-778487']\n",
      "[778481, 'Starter, Booster, 3000A, 23800mAh, Przetestowany, film z rozbiórki co jest w środku. Produkt to prawdopodobnie dawne KROAK', 225.0, -50.0, 449.99, '2024-01-08', 'https://www.pepper.pl/promocje/starter-booster-3000a-23800mah-przetestowany-film-z-rozbiorki-co-jest-w-srodku-produkt-to-prawdopodobnie-dawne-kroak-778481']\n",
      "[778486, 'Herbata sypana KINGSLEAF Earl Grey/English Breakfast/Zielona 100g lub 25saszetek x2g od 1.97zł', 3.11, -56.0, 6.99, '2024-01-08', 'https://www.pepper.pl/promocje/herbata-sypana-kingsleaf-earl-greyenglish-breakfastzielona-100g-lub-25saszetek-x2g-od-197zl-778486']\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from enum import Enum, IntEnum\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from typing import List, Union\n",
    "from datetime import datetime, timedelta, date, timezone\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "import signal\n",
    "import traceback\n",
    "from source.pepper_app.constans import OLD_DATES_DATA_PATTERN_1, OLD_DATES_DATA_PATTERN_2\n",
    "\n",
    "from source.pepper_app.environment_config import CustomEnvironment\n",
    "from source.pepper_app.constans import DATA_HEADER, STATS_HEADER, REQUEST_HEADER, RESPONSE_HEADER\n",
    "\n",
    "from collections import Counter\n",
    "from requests.exceptions import ConnectionError, HTTPError, MissingSchema, ReadTimeout\n",
    "from source.pepper_app.constans import (CSV_COLUMNS,\n",
    "                                STATS_HEADER)\n",
    "\n",
    "\n",
    "class ScrapePage:\n",
    "\n",
    "    def __init__(self, category_type: str, articles_to_retrieve: int, to_csv: bool=False,\n",
    "                to_database: bool=True, to_statistics: bool=True, searched_article: str='NA', \n",
    "                scrape_continuously: bool=False, scrape_choosen_data: bool=True) -> None:\n",
    "        self.category_type = category_type\n",
    "        self.articles_to_retrieve = articles_to_retrieve\n",
    "        self.to_database = to_database\n",
    "        self.to_csv = to_csv\n",
    "        self.to_statistics = to_statistics\n",
    "        self.start_page = 1\n",
    "        self.searched_article = searched_article\n",
    "        self.scrape_continuously = scrape_continuously\n",
    "        self.scrape_choosen_data = scrape_choosen_data\n",
    "\n",
    "    def scrape_page(self, url_to_scrape: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "        \"\"\"Setting up selenium webdriver, scraping page with bs4.\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options) #for local  \n",
    "                #driver = webdriver.Remote(command_executor=f'http://{CustomEnvironment.get_selenium_container_name()}:4444/wd/hub', options=options) #for docker \n",
    "            driver.set_window_size(1400,1000)\n",
    "            driver.get(url_to_scrape)\n",
    "            time.sleep(0.7)\n",
    "            page = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page, \"html5lib\")\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "    def select_url(self) -> str:\n",
    "        \"\"\"Selection of the website address depending on the type of scraping.\"\"\"\n",
    "        if self.scrape_continuously == True:\n",
    "            url_to_scrape = \"\".join([CustomEnvironment.get_url(), \"nowe\"])\n",
    "            return url_to_scrape\n",
    "        elif self.category_type == \"nowe\" and self.scrape_continuously == False:\n",
    "            url_to_scrape = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?page=\", str(self.start_page)])\n",
    "            return url_to_scrape\n",
    "        elif self.category_type == \"search\" and self.scrape_continuously == False:\n",
    "            searched_article = str(self.searched_article.replace(\" \",\"%20\"))\n",
    "            url_to_scrape = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?q=\",\n",
    "                                    searched_article, \"&page=\", str(self.start_page)])\n",
    "            return url_to_scrape\n",
    "        else:\n",
    "            raise Exception(f\"The variables were defined incorrectly.\")\n",
    "\n",
    "    def infinite_scroll_handling(self) -> List[str]:\n",
    "        \"\"\"Handling scraping through subsequent pages.\"\"\"\n",
    "        try:\n",
    "            flag = True\n",
    "            retrived_articles = list()\n",
    "            while flag:\n",
    "                url_to_scrape = self.select_url()\n",
    "                soup = self.scrape_page(url_to_scrape)\n",
    "                flag_nowe = CheckConditions(soup).check_if_last_page_nowe()\n",
    "                flag_search = CheckConditions(soup).check_if_last_page_search()\n",
    "\n",
    "                if flag_nowe == False or flag_search == False:\n",
    "                    flag = False\n",
    "                flag = CheckConditions(soup).check_if_no_items_found()\n",
    "\n",
    "                articles = soup.find_all('article')\n",
    "                retrived_articles += articles\n",
    "\n",
    "                if len(retrived_articles) >= self.articles_to_retrieve:\n",
    "                    flag = False\n",
    "                else:\n",
    "                    if self.scrape_continuously == True:\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.start_page += 1\n",
    "                \n",
    "                prepared_articles = articles[:self.articles_to_retrieve]\n",
    "                all_items = self.get_items_details(prepared_articles)\n",
    "            return all_items\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Infinite scroll failed:\\\n",
    "                            {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_items_details_depending_on_the_function(self):\n",
    "        \"\"\"Completing the list of articles and extracting data details depending on the type of scraping.\"\"\"\n",
    "        if self.scrape_continuously == True and self.scrape_choosen_data == False:\n",
    "            while True:\n",
    "                all_items = self.infinite_scroll_handling()\n",
    "                return all_items\n",
    "        elif self.scrape_continuously == False and self.scrape_choosen_data == True:\n",
    "            all_items = self.infinite_scroll_handling()\n",
    "            return all_items\n",
    "        else:\n",
    "            raise Exception(f\"Matching get_items_details depending on the selected \\\n",
    "                            functionality failed. \\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    \n",
    "    def get_items_details(self, prepared_articles):\n",
    "        \"\"\"Getting item detailes.\"\"\"\n",
    "        start_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        all_items = list()\n",
    "        try:\n",
    "            for article in prepared_articles:\n",
    "                item = list()\n",
    "                item.extend([GetItemId(article).get_data(), \n",
    "                            GetItemName(article).get_data(),\n",
    "                            GetItemDiscountPrice(article).get_data(),\n",
    "                            GetItemPercentageDiscount(article).get_data(),\n",
    "                            GetItemRegularPrice(article).get_data(),\n",
    "                            GetItemAddedDate(article).get_data(),\n",
    "                            GetItemUrl(article).get_data()])\n",
    "                if item not in all_items:\n",
    "                    all_items.append(item)\n",
    "                if '' in item:\n",
    "                    logging.warning(\"Data retrieving failed. None values detected\")\n",
    "                    break\n",
    "                if self.to_csv:\n",
    "                    self.save_data_to_csv(item)\n",
    "                if self.to_database:\n",
    "                    print(\"to_database\")\n",
    "                    pass\n",
    "                    #LoadItemDetailsToDatabase(item).load_to_db()\n",
    "            return all_items\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item details failed :\\\n",
    "                        {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "        end_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = end_time - start_time\n",
    "\n",
    "        if self.to_statistics:\n",
    "            try:\n",
    "                stats_info = self.get_scraping_stats_info(action_execution_datetime)\n",
    "                pass\n",
    "                #LoadScrapingStatisticsToDatabase(stats_info).load_to_db()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Populating ScrapingStatistics table failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "        return all_items\n",
    "\n",
    "    def save_data_to_csv(self, item) -> None:\n",
    "        \"\"\"Saving data to csv file.\"\"\"\n",
    "        try:\n",
    "            header = False\n",
    "            if not os.path.exists('scraped.csv'):\n",
    "                header = True\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "            else:\n",
    "                header = False\n",
    "                df_e = pd.read_csv('scraped.csv')\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                if df['item_id'][0] not in df_e['item_id'].tolist():\n",
    "                    df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Saving data to csv failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_scraping_stats_info(self, action_execution_datetime: datetime) -> List[Union[str, int, bool, float]]:\n",
    "        \"\"\"Getting scraping stats info.\"\"\"\n",
    "        stats_info = list()\n",
    "\n",
    "        category_type = self.category_type\n",
    "        retrieved_articles_quantity = self.articles_to_retrieve\n",
    "        time_of_the_action = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = action_execution_datetime\n",
    "        searched_article = self.searched_article\n",
    "        to_csv = self.to_csv\n",
    "        to_database  = self.to_database\n",
    "        scrape_continuously = self.scrape_continuously\n",
    "        scrape_choosen_data = self.scrape_choosen_data\n",
    "\n",
    "        stats=[category_type, retrieved_articles_quantity,\n",
    "            time_of_the_action, action_execution_datetime, searched_article,\n",
    "            to_csv, to_database, scrape_continuously, scrape_choosen_data]\n",
    "\n",
    "        for field in stats:\n",
    "            stats_info.append(field)\n",
    "\n",
    "        return stats_info\n",
    "\n",
    "    \n",
    "\n",
    "class CheckConditions:\n",
    "\n",
    "    def __init__(self, soup: BeautifulSoup) -> None:\n",
    "        self.soup = soup\n",
    "\n",
    "    def check_if_last_page_nowe(self) -> bool:\n",
    "        \"\"\"Checking 'nowe' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h1', {\"class\":\"size--all-xl size--fromW3-xxl text--b space--b-2\"})[0].get_text()\n",
    "            if searched_ending_string.startswith(\"Ups\"):\n",
    "                logging.warning(\"No more pages to scrape.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    def check_if_last_page_search(self) -> bool:\n",
    "        \"\"\"Checking 'search' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-l\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"box--all-i size--all-s vAlign--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t Okazje()\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number > 0:\n",
    "                logging.warning(\"No more pages to scrape.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    def check_if_no_items_found(self) -> bool:\n",
    "        \"\"\"Checking if searched item was found.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-l\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"box--all-i size--all-s vAlign--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t Okazje()\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number == 0:\n",
    "                logging.warning(\"The searched item was not found.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "\n",
    "class GetItemName:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            name = self.article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "            return name\n",
    "        except IndexError as e:\n",
    "            name = self.article.find_all(attrs={'class': \"thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "            return name\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item name failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemId:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> int:\n",
    "        try:\n",
    "            item_id = self.article.get(\"id\")\n",
    "            item_id = item_id.strip('thread_')\n",
    "            item_id = int(item_id)\n",
    "            return item_id\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item id failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemDiscountPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "            if len(discount_price) == 0:\n",
    "                discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl text--color-greyShade\"})\n",
    "\n",
    "            if len(discount_price) > 0:\n",
    "                discount_price = discount_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ','')\n",
    "                if discount_price == \"ZADARMO\":\n",
    "                    discount_price = float(0)\n",
    "                else:\n",
    "                    discount_price = float(discount_price)                \n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                discount_price = \"NA\"\n",
    "            return discount_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item discount price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemRegularPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            regular_price = self.article.find_all(attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"})\n",
    "            if len(regular_price) > 0:\n",
    "                regular_price = float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ',''))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                regular_price = \"NA\"\n",
    "            return regular_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item regular price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemPercentageDiscount:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            percentage_discount = self.article.find_all(attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            if len(percentage_discount) > 0:\n",
    "                percentage_discount = float(percentage_discount[0].get_text().strip('%'))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                percentage_discount = \"NA\"\n",
    "            return percentage_discount\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item percentage discount failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemUrl:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            item_url = self.article.find_all('a', {\"class\":\"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['href']\n",
    "            return item_url\n",
    "        except IndexError as e:\n",
    "            item_url = 'NA - Login Required.'\n",
    "            return item_url\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item url failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemAddedDate:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_raw_data(self) -> List[str]:\n",
    "        try:\n",
    "            date_tag = self.article.find_all('div', {\"class\":\"size--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "            raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "            return raw_string_list\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item added date failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            filtered_list = self.clean_list()\n",
    "            filtered_list = self.check_missing_date()\n",
    "            date_string_likely = filtered_list[0]\n",
    "            if date_string_likely == \"NA\":\n",
    "                url_with_item = GetItemUrl.get_data(self)\n",
    "                soup = self.scrape_page(url_with_item)\n",
    "                prepared_data = self.fill_missing_date(soup)\n",
    "                return prepared_data\n",
    "            else:\n",
    "                stripped_date_string_likely = self.strip_date_string(date_string_likely)\n",
    "                prepared_data = self.date_format_conversion(stripped_date_string_likely)\n",
    "            return prepared_data\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name: {e}\")\n",
    "\n",
    "    def strip_date_string(self, date_string_likely: str) -> str:\n",
    "        try:\n",
    "            if date_string_likely.startswith(\"Zaktualizowano \") and date_string_likely.endswith(\" temu\"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                stripped_date_string_likely = stripped_date_string_likely.rstrip(\" temu\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.endswith(\"Lokalnie\"):\n",
    "                stripped_date_string_likely = date_string_likely.rstrip(\"Lokalnie\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.startswith(\"Zaktualizowano \"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                return stripped_date_string_likely\n",
    "            else:\n",
    "                stripped_date_string_likely = date_string_likely\n",
    "                return stripped_date_string_likely\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Stripping date string failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def date_format_conversion(self, stripped_date_string_likely: str) -> str:\n",
    "        try:\n",
    "            if stripped_date_string_likely.endswith(('min', 'g', 's')):\n",
    "                prepared_data = date.today().strftime(\"%Y-%m-%d\")\n",
    "                return prepared_data\n",
    "            elif stripped_date_string_likely.startswith(tuple(Months.keys())) and len(stripped_date_string_likely) < 8:\n",
    "                if len(stripped_date_string_likely[4:]) == 3:\n",
    "                    day = stripped_date_string_likely[4:6]\n",
    "                else:\n",
    "                    day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = str(date.today().year)\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_1, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:6]\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[8:13]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_2, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[7:12]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Data format conversion tailed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def clean_list(self) -> List[str]:\n",
    "        raw_string_list = self.get_raw_data()\n",
    "        items_to_remove = list()\n",
    "        filtered_list = list()\n",
    "        try:\n",
    "            for string in raw_string_list:\n",
    "                if \"/\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if \":\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\", \"Stacjonarnie\"]:\n",
    "                    items_to_remove.append(string)\n",
    "                if string.startswith(\"Wysyłka\"):\n",
    "                    items_to_remove.append(string)\n",
    "            counts = Counter(items_to_remove)\n",
    "            for string in raw_string_list:\n",
    "                if counts[string]:\n",
    "                    counts[string] -= 1\n",
    "                else:\n",
    "                    filtered_list.append(string)\n",
    "            return filtered_list\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "    def check_missing_date(self) -> List[str]:\n",
    "        filtered_list = self.clean_list()\n",
    "        try:\n",
    "            if len(filtered_list) == 0:\n",
    "                filtered_list.append(\"NA\")\n",
    "                return filtered_list\n",
    "            else:\n",
    "                return filtered_list\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "    def fill_missing_date(self, soup) -> str:\n",
    "        try:\n",
    "            date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "            filtered_list = date_string.split()\n",
    "            day_string = filtered_list[0]\n",
    "            month_string = filtered_list[1]\n",
    "            year_string = filtered_list[2].strip(',')\n",
    "            if len(day_string[0]) == 2:\n",
    "                day = day_string\n",
    "            else:\n",
    "                day = day_string.zfill(2)\n",
    "            month = Months.__members__[month_string].value\n",
    "            year = year_string\n",
    "            prepared_data = '-'.join([year, month, day])\n",
    "            return prepared_data\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "    def scrape_page(self, url_with_item: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options) #for local  \n",
    "                #driver = webdriver.Remote(command_executor=f'http://{CustomEnvironment.get_selenium_container_name()}:4444/wd/hub', options=options) #for docker \n",
    "            driver.get(url_with_item)\n",
    "            time.sleep(0.7)\n",
    "            page_with_item = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page_with_item, 'html5lib')\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "\n",
    "category_type = \"nowe\"\n",
    "articles_to_retrieve = 45\n",
    "searched_article = \"S22\"\n",
    "scrape_continuously = False\n",
    "scrape_choosen_data = True\n",
    "\n",
    "output = ScrapePage(category_type, articles_to_retrieve, scrape_continuously=scrape_continuously, scrape_choosen_data=scrape_choosen_data)\n",
    "all_items = output.get_items_details_depending_on_the_function()\n",
    "\n",
    "\n",
    "for i in all_items:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[795895, 'AKTYWUJ MULTIEFEKT NIVEA SERUM W DUECIE', 'NA', 'NA', 'NA', '2024-02-13', 'https://www.pepper.pl/promocje/aktywuj-multiefekt-nivea-serum-w-duecie-795895']\n",
      "[795897, 'Pepsi/mirinda/7up za 5,99 2l przy zakupie 2szt w topazie', 5.99, 'NA', 'NA', '2024-02-13', 'https://www.pepper.pl/promocje/pepsimirinda7up-za-599-2l-przy-zakupie-2szt-w-topazie-795897']\n",
      "[795894, 'Czekolada gorzka nadziewana E.Wedel o smaku Pina Colada lub Tropical Sunrise, 100g', 2.5, 'NA', 'NA', '2024-02-13', 'https://www.pepper.pl/promocje/czekolada-gorzka-nadziewana-ewedel-o-smaku-pina-cada-lub-tropical-sunrise-100g-795894']\n",
      "[795892, 'Telewizor Panasonic TX-65MZ1500E', 9999.0, 'NA', 'NA', '2024-02-13', 'https://www.pepper.pl/promocje/panasonic-tx-65mz1500e-795892']\n",
      "[795890, 'SONGMICS Biurko elektryczne - 2 silniki - 120kg', 637.31, 'NA', 'NA', '2024-02-13', 'https://www.pepper.pl/promocje/songmics-biurko-elektryczne-2-silniki-120kg-795890']\n",
      "[795888, 'Mikrofon Razer Seiren Mini Mercury White', 174.0, 'NA', 'NA', '2024-02-13', 'https://www.pepper.pl/promocje/razer-seiren-mini-mercury-white-795888']\n",
      "[795889, 'MINI NAWILŻACZ POWIETRZA UROCZY KOTEK DYFUZOR AROMATERAPIA LAMPKA NOCNA LED', 18.9, 'NA', 'NA', '2024-02-13', 'https://www.pepper.pl/promocje/mini-nawilzacz-powietrza-uroczy-kotek-dyfuzor-aromaterapia-lampka-nocna-led-795889']\n",
      "[795887, 'Zestaw Ręczników Wielokolorowych FROTTE 50x100 cm 6 sztuk', 60.0, 'NA', 'NA', '2024-02-13', 'https://www.pepper.pl/promocje/zestaw-recznikowwielokolorowych-frotte-50x100-cm-6-sztuk-795887']\n",
      "[795883, 'Rajstopy Carla Bonetti -70% przy zakupie dwóch', 1.5, 'NA', 'NA', '2024-02-13', 'https://www.pepper.pl/promocje/rajstopy-carla-bonetti-70-przy-zakupie-dwoch-795883']\n",
      "[795886, 'Kosmos ebook Tomasz Rożek', 13.9, 'NA', 'NA', '2024-02-13', 'https://www.pepper.pl/promocje/kosmos-ebook-tomasz-rozek-numer-1-wsrod-polskich-popularyzatorow-wiedzy-795886']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "from datetime import datetime, timedelta, date, timezone\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "import signal\n",
    "import traceback\n",
    "from source.pepper_app.constans import OLD_DATES_DATA_PATTERN_1, OLD_DATES_DATA_PATTERN_2\n",
    "\n",
    "from source.pepper_app.environment_config import CustomEnvironment\n",
    "from source.pepper_app.constans import DATA_HEADER, STATS_HEADER, REQUEST_HEADER, RESPONSE_HEADER\n",
    "\n",
    "from collections import Counter\n",
    "from requests.exceptions import ConnectionError, HTTPError, MissingSchema, ReadTimeout\n",
    "from source.pepper_app.constans import (CSV_COLUMNS,\n",
    "                                STATS_HEADER)\n",
    "\n",
    "class ScrapPage:\n",
    "\n",
    "\n",
    "    def __init__(self, category_type: str, articles_to_retrieve: int, to_csv: bool=False,\n",
    "                to_database: bool=False, to_statistics: bool=True, searched_article: str='NA', \n",
    "                scrap_continuously: bool=False, scrap_choosen_data: bool=True) -> None:\n",
    "        self.category_type = category_type\n",
    "        self.articles_to_retrieve = articles_to_retrieve\n",
    "        self.to_database = to_database\n",
    "        self.to_csv = to_csv\n",
    "        self.to_statistics = to_statistics\n",
    "        self.start_page = 1\n",
    "        self.searched_article = searched_article\n",
    "        self.scrap_continuously = scrap_continuously\n",
    "        self.scrap_choosen_data = scrap_choosen_data\n",
    "\n",
    "    def scrap_page(self, url_to_scrap: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "        \"\"\"Setting up selenium webdriver, scraping page with bs4.\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            #options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options)\n",
    "                #driver = webdriver.Remote(command_executor='http://selenium-hub:4444/wd/hub', options=options)\n",
    "            driver.set_window_size(1400,1000)\n",
    "            driver.get(url_to_scrap)\n",
    "            time.sleep(0.7)\n",
    "            page = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page, \"html5lib\")\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "    def select_url(self) -> str:\n",
    "        \"\"\"Selection of the website address depending on the type of scrapping.\"\"\"\n",
    "        if self.scrap_continuously == True:\n",
    "            url_to_scrap = \"\".join([CustomEnvironment.get_url(), \"nowe\"])\n",
    "            return url_to_scrap\n",
    "        elif self.category_type == \"nowe\" and self.scrap_continuously == False:\n",
    "            url_to_scrap = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?page=\", str(self.start_page)])\n",
    "            return url_to_scrap\n",
    "        elif self.category_type == \"search\" and self.scrap_continuously == False:\n",
    "            searched_article = str(self.searched_article.replace(\" \",\"%20\"))\n",
    "            url_to_scrap = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?q=\",\n",
    "                                    searched_article, \"&page=\", str(self.start_page)])\n",
    "            return url_to_scrap\n",
    "        else:\n",
    "            raise Exception(f\"The variables were defined incorrectly.\")\n",
    "\n",
    "\n",
    "    def infinite_scroll_handling(self) -> List[str]:\n",
    "        \"\"\"Handling scraping through subsequent pages.\"\"\"\n",
    "        try:\n",
    "            flag = True\n",
    "            retrived_articles = list()\n",
    "            while flag:\n",
    "                url_to_scrap = self.select_url()\n",
    "                soup = self.scrap_page(url_to_scrap)\n",
    "                flag_nowe = CheckConditions(soup).check_if_last_page_nowe()\n",
    "                flag_search = CheckConditions(soup).check_if_last_page_search()\n",
    "                if flag_nowe == False or flag_search == False:\n",
    "                    flag = False\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                flag = CheckConditions(soup).check_if_no_items_found()\n",
    "                if flag == False:\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                if flag == True:\n",
    "                    articles = soup.find_all('article')\n",
    "                    retrived_articles += articles\n",
    "                else:\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                if len(retrived_articles) >= int(self.articles_to_retrieve):\n",
    "                    flag = False\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                self.start_page += 1\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Infinite scroll failed:\\\n",
    "                            {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "\n",
    "    def get_items_details_depending_on_the_function(self):\n",
    "        \"\"\"Completing the list of articles and extracting data details depending on the type of scrapping.\"\"\"\n",
    "        if self.scrap_continuously == True and self.scrap_choosen_data == False:\n",
    "            while True:\n",
    "                retrived_articles = self.scrap_continuously_by_refreshing_page()\n",
    "                self.get_items_details(retrived_articles)\n",
    "        elif self.scrap_continuously == False and self.scrap_choosen_data == True:\n",
    "            retrived_articles = self.infinite_scroll_handling()\n",
    "            all_items = self.get_items_details(retrived_articles)\n",
    "            return all_items\n",
    "        else:\n",
    "            raise Exception(f\"Matching get_items_details depending on the selected \\\n",
    "                            functionality failed. \\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "    def get_items_details(self, retrived_articles) -> list():\n",
    "        \"\"\"Getting item detailes.\"\"\"\n",
    "        start_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        all_items = list()\n",
    "        try:\n",
    "            for article in retrived_articles:\n",
    "                item = list()\n",
    "                item.append(GetItemId(article).get_data())\n",
    "                item.append(GetItemName(article).get_data())\n",
    "                item.append(GetItemDiscountPrice(article).get_data())\n",
    "                item.append(GetItemPercentageDiscount(article).get_data())\n",
    "                item.append(GetItemRegularPrice(article).get_data())\n",
    "                item.append(GetItemAddedDate(article).get_data())\n",
    "                item.append(GetItemUrl(article).get_data())\n",
    "                if item not in all_items:\n",
    "                    all_items.append(item)\n",
    "                if '' in item:\n",
    "                    logging.warning(\"Data retrieving failed. None values detected\")\n",
    "                    break\n",
    "                if self.to_csv:\n",
    "                    self.save_data_to_csv(item)\n",
    "                #if self.to_database:\n",
    "                #    LoadItemDetailsToDatabase(item).load_to_db()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item details failed :\\\n",
    "                        {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "        end_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = end_time - start_time\n",
    "\n",
    "        if self.to_statistics:\n",
    "            try:\n",
    "                stats_info = self.get_scraping_stats_info(action_execution_datetime)\n",
    "                #LoadScrapingStatisticsToDatabase(stats_info).load_to_db()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Populating ScrapingStatistics table failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "        return all_items\n",
    "\n",
    "    def save_data_to_csv(self, item) -> None:\n",
    "        \"\"\"Saving data to csv file.\"\"\"\n",
    "        try:\n",
    "            header = False\n",
    "            if not os.path.exists('scraped.csv'):\n",
    "                header = True\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "            else:\n",
    "                header = False\n",
    "                df_e = pd.read_csv('scraped.csv')\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                if df['item_id'][0] not in df_e['item_id'].tolist():\n",
    "                    df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Saving data to csv failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "    def get_scraping_stats_info(self, action_execution_datetime: datetime) -> List[Union[str, int, bool, float]]:\n",
    "        \"\"\"Getting scraping stats info.\"\"\"\n",
    "        stats_info = list()\n",
    "\n",
    "        category_type = self.category_type\n",
    "        retrieved_articles_quantity = self.articles_to_retrieve\n",
    "        time_of_the_action = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = action_execution_datetime\n",
    "        searched_article = self.searched_article\n",
    "        to_csv = self.to_csv\n",
    "        to_database  = self.to_database\n",
    "        scrap_continuously = self.scrap_continuously\n",
    "        scrap_choosen_data = self.scrap_choosen_data\n",
    "\n",
    "        stats=[category_type, retrieved_articles_quantity,\n",
    "            time_of_the_action, action_execution_datetime, searched_article,\n",
    "            to_csv, to_database, scrap_continuously, scrap_choosen_data]\n",
    "\n",
    "        for field in stats:\n",
    "            stats_info.append(field)\n",
    "\n",
    "        return stats_info\n",
    "\n",
    "\n",
    "    def scrap_continuously_by_refreshing_page(self) -> List[str]:\n",
    "        \"\"\"Scraping data function for continuously scraping feature.\"\"\"\n",
    "        retrived_articles = list()\n",
    "\n",
    "        \n",
    "        url_to_scrap = self.select_url()\n",
    "        soup = self.scrap_page(url_to_scrap)\n",
    "        time.sleep(20)\n",
    "        articles = soup.find_all('article')\n",
    "        retrived_articles += articles\n",
    "        print(retrived_articles)\n",
    "\n",
    "        #return retrived_articles\n",
    "\n",
    "\n",
    "class CheckConditions:\n",
    "\n",
    "\n",
    "    def __init__(self, soup: BeautifulSoup) -> None:\n",
    "        self.soup = soup\n",
    "\n",
    "\n",
    "    def check_if_last_page_nowe(self) -> bool:\n",
    "        \"\"\"Checking 'nowe' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h1', {\"class\":\"size--all-xl size--fromW3-xxl text--b space--b-2\"})[0].get_text()\n",
    "            if searched_ending_string.startswith(\"Ups\"):\n",
    "                logging.warning(\"No more pages to scrap.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    def check_if_last_page_search(self) -> bool:\n",
    "        \"\"\"Checking 'search' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-l\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"box--all-i size--all-s vAlign--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t Okazje()\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number > 0:\n",
    "                logging.warning(\"No more pages to scrap.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    def check_if_no_items_found(self) -> bool:\n",
    "        \"\"\"Checking if searched item was found.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-l\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"box--all-i size--all-s vAlign--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t Okazje()\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number == 0:\n",
    "                logging.warning(\"The searched item was not found.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "\n",
    "class GetItemName:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            name = self.article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "            return name\n",
    "        except IndexError as e:\n",
    "            name = self.article.find_all(attrs={'class': \"thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "            return name\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item name failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemId:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> int:\n",
    "        try:\n",
    "            item_id = self.article.get(\"id\")\n",
    "            item_id = item_id.strip('thread_')\n",
    "            item_id = int(item_id)\n",
    "            return item_id\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item id failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemDiscountPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            discount_price = self.article.find_all(\"span\", {'class': \"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\"})\n",
    "            if len(discount_price) == 0:\n",
    "                discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl text--color-greyShade\"})\n",
    "\n",
    "            if len(discount_price) > 0:\n",
    "                discount_price = discount_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ','')\n",
    "                if discount_price == \"ZADARMO\":\n",
    "                    discount_price = float(0)\n",
    "                else:\n",
    "                    discount_price = float(discount_price)                \n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                discount_price = \"NA\"\n",
    "            return discount_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item discount price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "\n",
    "class GetItemRegularPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            regular_price = self.article.find_all(attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"})\n",
    "            if len(regular_price) > 0:\n",
    "                regular_price = float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ',''))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                regular_price = \"NA\"\n",
    "            return regular_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item regular price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "\n",
    "class GetItemPercentageDiscount:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            percentage_discount = self.article.find_all(attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            if len(percentage_discount) > 0:\n",
    "                percentage_discount = float(percentage_discount[0].get_text().strip('%'))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                percentage_discount = \"NA\"\n",
    "            return percentage_discount\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item percentage discount failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemUrl:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            item_url = self.article.find_all(attrs={'class', 'cept-tt thread-link linkPlain thread-title--list js-thread-title'})[0]['href']\n",
    "            return item_url\n",
    "        except IndexError as e:\n",
    "            item_url = 'NA - Login Required.'\n",
    "            return item_url\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item url failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemAddedDate:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_raw_data(self) -> List[str]:\n",
    "\n",
    "        try:\n",
    "            date_tag = self.article.find_all('div', {\"class\":\"flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "            raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "            return raw_string_list\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item added date failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "\n",
    "        try:\n",
    "            filtered_list = self.clean_list()\n",
    "            filtered_list = self.check_missing_date()\n",
    "            date_string_likely = filtered_list[0]\n",
    "            if date_string_likely == \"NA\":\n",
    "                url_with_item = GetItemUrl.get_data(self)\n",
    "                soup = self.scrap_page(url_with_item)\n",
    "                prepared_data = self.fill_missing_date(soup)\n",
    "                return prepared_data\n",
    "            else:\n",
    "                stripped_date_string_likely = self.strip_date_string(date_string_likely)\n",
    "                prepared_data = self.date_format_conversion(stripped_date_string_likely)\n",
    "            return prepared_data\n",
    "\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name: {e}\")\n",
    "\n",
    "    def strip_date_string(self, date_string_likely: str) -> str:\n",
    "\n",
    "        try:\n",
    "            if date_string_likely.startswith(\"Zaktualizowano \") and date_string_likely.endswith(\" temu\"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                stripped_date_string_likely = stripped_date_string_likely.rstrip(\" temu\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.endswith(\"Lokalnie\"):\n",
    "                stripped_date_string_likely = date_string_likely.rstrip(\"Lokalnie\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.startswith(\"Zaktualizowano \"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                return stripped_date_string_likely\n",
    "            else:\n",
    "                stripped_date_string_likely = date_string_likely\n",
    "                return stripped_date_string_likely\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Stripping date string failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def date_format_conversion(self, stripped_date_string_likely: str) -> str:\n",
    "\n",
    "        try:\n",
    "            if stripped_date_string_likely.endswith(('min', 'g', 's')):\n",
    "                prepared_data = date.today().strftime(\"%Y-%m-%d\")\n",
    "                return prepared_data\n",
    "            elif stripped_date_string_likely.startswith(tuple(Months.keys())) and len(stripped_date_string_likely) < 8:\n",
    "                if len(stripped_date_string_likely[4:]) == 3:\n",
    "                    day = stripped_date_string_likely[4:6]\n",
    "                else:\n",
    "                    day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = str(date.today().year)\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_1, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:6]\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[8:13]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_2, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[7:12]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Data format conversion tailed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "    def clean_list(self) -> List[str]:\n",
    "\n",
    "        raw_string_list = self.get_raw_data()\n",
    "        items_to_remove = list()\n",
    "        filtered_list = list()\n",
    "\n",
    "        try:\n",
    "            for string in raw_string_list:\n",
    "                if \"/\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if \":\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\", \"Stacjonarnie\"]:\n",
    "                    items_to_remove.append(string)\n",
    "                if string.startswith(\"Wysyłka\"):\n",
    "                    items_to_remove.append(string)\n",
    "\n",
    "            counts = Counter(items_to_remove)\n",
    "\n",
    "            for string in raw_string_list:\n",
    "                if counts[string]:\n",
    "                    counts[string] -= 1\n",
    "                else:\n",
    "                    filtered_list.append(string)\n",
    "            return filtered_list\n",
    "\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "\n",
    "    def check_missing_date(self) -> List[str]:\n",
    "\n",
    "        filtered_list = self.clean_list()\n",
    "\n",
    "        try:\n",
    "            if len(filtered_list) == 0:\n",
    "                filtered_list.append(\"NA\")\n",
    "                return filtered_list\n",
    "            else:\n",
    "                return filtered_list\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "\n",
    "    def fill_missing_date(self, soup) -> str:\n",
    "\n",
    "        try:\n",
    "            date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "            filtered_list = date_string.split()\n",
    "            day_string = filtered_list[0]\n",
    "            month_string = filtered_list[1]\n",
    "            year_string = filtered_list[2].strip(',')\n",
    "\n",
    "            if len(day_string[0]) == 2:\n",
    "                day = day_string\n",
    "            else:\n",
    "                day = day_string.zfill(2)\n",
    "\n",
    "            month = Months.__members__[month_string].value\n",
    "            year = year_string\n",
    "            prepared_data = '-'.join([year, month, day])\n",
    "            return prepared_data\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "\n",
    "    def scrap_page(self, url_with_item: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options)\n",
    "                #driver = webdriver.Remote(command_executor='http://selenium-hub:4444/wd/hub', options=options)\n",
    "            driver.get(url_with_item)\n",
    "            time.sleep(0.7)\n",
    "            page_with_item = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page_with_item, 'html5lib')\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "category_type = \"nowe\"\n",
    "articles_to_retrieve = 10\n",
    "searched_article = \"S22\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output = ScrapPage(category_type, articles_to_retrieve)\n",
    "all_items = output.get_items_details_depending_on_the_function()\n",
    "\n",
    "\n",
    "for i in all_items:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cfehome'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdjango_for_jupyt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_django\n\u001b[0;32m----> 2\u001b[0m \u001b[43minit_django\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpepper_app\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pepper-best-deals/django_for_jupyt.py:19\u001b[0m, in \u001b[0;36minit_django\u001b[0;34m(project_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDJANGO_ALLOW_ASYNC_UNSAFE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdjango\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mdjango\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pepper-best-deals/env/lib/python3.10/site-packages/django/__init__.py:19\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(set_prefix)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdjango\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01murls\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_script_prefix\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdjango\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m configure_logging\n\u001b[0;32m---> 19\u001b[0m configure_logging(\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLOGGING_CONFIG\u001b[49m, settings\u001b[38;5;241m.\u001b[39mLOGGING)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m set_prefix:\n\u001b[1;32m     21\u001b[0m     set_script_prefix(\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mFORCE_SCRIPT_NAME \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mFORCE_SCRIPT_NAME\n\u001b[1;32m     23\u001b[0m     )\n",
      "File \u001b[0;32m~/pepper-best-deals/env/lib/python3.10/site-packages/django/conf/__init__.py:102\u001b[0m, in \u001b[0;36mLazySettings.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the value of a setting and cache it in self.__dict__.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (_wrapped \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped) \u001b[38;5;129;01mis\u001b[39;00m empty:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     _wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped\n\u001b[1;32m    104\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_wrapped, name)\n",
      "File \u001b[0;32m~/pepper-best-deals/env/lib/python3.10/site-packages/django/conf/__init__.py:89\u001b[0m, in \u001b[0;36mLazySettings._setup\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     81\u001b[0m     desc \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name) \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ImproperlyConfigured(\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, but settings are not configured. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must either define the environment variable \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor call settings.configure() before accessing settings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;241m%\u001b[39m (desc, ENVIRONMENT_VARIABLE)\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped \u001b[38;5;241m=\u001b[39m \u001b[43mSettings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pepper-best-deals/env/lib/python3.10/site-packages/django/conf/__init__.py:217\u001b[0m, in \u001b[0;36mSettings.__init__\u001b[0;34m(self, settings_module)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# store the settings module in case someone later cares\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSETTINGS_MODULE \u001b[38;5;241m=\u001b[39m settings_module\n\u001b[0;32m--> 217\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSETTINGS_MODULE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m tuple_settings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALLOWED_HOSTS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINSTALLED_APPS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSECRET_KEY_FALLBACKS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_explicit_settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cfehome'"
     ]
    }
   ],
   "source": [
    "from django_for_jupyt import init_django\n",
    "init_django(\"pepper_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROMOCJA', 'Samsung', 'Galaxy', 'S23', 'z', 'kodem', 'APP5']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from django.db.models import Q\n",
    "\n",
    "import pandas as pd\n",
    "from source.pepper_app.constans import DATA_HEADER\n",
    "\n",
    "\n",
    "data = [1, 'PROMOCJA Samsung Galaxy S23 z kodem APP5', 2500, 10, 2750, '2024-01-07', 'https://www.pepper.pl/promocje/samsungs23']\n",
    "item_df = pd.DataFrame([data], columns=DATA_HEADER)\n",
    "\n",
    "\n",
    "def conditions(item_df):\n",
    "    # Initialize conditions with a default condition\n",
    "    conditions = Q()\n",
    "\n",
    "    for _, row in item_df.iterrows():\n",
    "        article_name = row[\"name\"]\n",
    "        list_of_words_article_name = article_name.split()\n",
    "\n",
    "        # Create a new condition for each word in the article name\n",
    "    for word in list_of_words_article_name:\n",
    "        conditions &= Q(desired_article__icontains=word)\n",
    "\n",
    "    # Filter UserRequest objects based on the conditions and order by request_time\n",
    "    #results = UserRequest.objects.filter(conditions).order_by('request_time')\n",
    "\n",
    "    return list_of_words_article_name\n",
    "\n",
    "\n",
    "print(conditions(item_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
