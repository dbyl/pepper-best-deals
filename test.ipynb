{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from enum import Enum, IntEnum\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_scrap = \"\"\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.set_window_size(1400,1000)\n",
    "driver.get(\"https://www.pepper.pl/nowe\")\n",
    "time.sleep(0.7)\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, \"html5lib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date: ['9 s'], name: LEGO Technic 42129 Ciężarówka Mercedes-Benz Zetros z napędem na 4 koła\n",
      "date: ['3 min'], name: Dysk SSD Crucial P3 Plus 2Tb M.2\n",
      "date: ['Stacjonarnie'], name: @Lidl Kong strong clasic/zero 1+1 gratis\n",
      "date: ['14 min'], name: 4 uchwyty na mopa lub miotłę, samoprzylepne, ścienne, bez wiercenia, szary\n",
      "date: ['16 min'], name: Metal Gear Solid Master Collection Volume 1 PS5\n",
      "date: ['24 min'], name: Moneta Innova Patelnia grillowa 28 x 28 cm, naturalna powłoka FineGres (bez PFAS), również do kuchenek indukcyjnych, wykonana we Włoszech\n",
      "date: ['24 min'], name: WIEDŹMIN III 3 DZIKI GON GOTY - EDYCJA KOMPLETNA / NOWA / PL / PS5\n",
      "date: ['33 min'], name: Laptop Acer Swift 3 OLED | i7-12700H | 14.0''-WQXGA+ | 16GB | 1TB | Win 11 | szary\n",
      "date: ['38 min'], name: Ekspres kolbowy Lelit Anna PL41EM\n",
      "date: ['39 min'], name: Dofamine za 8,50 zł z Islandzkiego Xbox Store @ Xbox One / Xbox Series\n",
      "date: ['44 min'], name: Skarpety dziecięce 5 par 24-27\n",
      "date: ['44 min'], name: Call of Duty: Vanguard Cross-Gen Edition XBOX SERIES X/S I ONE TURCJA\n",
      "date: ['45 min'], name: Szczoteczka Oclean X pro Digital + etui i 3 końcówki\n",
      "date: ['45 min'], name: Spersonalizowane wideo życzenia świąteczne\n",
      "date: ['50 min'], name: Gra Uno junior\n",
      "date: ['53 min', 'Stacjonarnie'], name: Autobus SOS - bezpłatna pomoc dla osób bezdomnych w Gdańsku\n",
      "date: ['56 min'], name: This War of Mine @ Google Play\n",
      "date: ['1 g, 10 min'], name: Wózek warsztatowy Scheppach TW1300 4 szuflady Wózek narzędziowy z matą antypoślizgową 99.95 €\n",
      "date: ['1 g, 10 min', 'Stacjonarnie'], name: Tablet SAMSUNG Galaxy Tab S8+ 12.4\" 8/128 GB Wi-Fi Grafitowy + Rysik S Pen\n",
      "date: ['1 g, 27 min'], name: Dron DJI Mini 2 SE | Amazon | 303,95€ [1314zł] | Fly More Combo 406,60€ [1760zł]\n",
      "date: ['1 g, 29 min'], name: Kompresor 100l olejowy Hecht 2355\n",
      "date: ['1 g, 31 min'], name: Inwee pokrętło do usuwania ciasta, do Thermomix TM6, TM5 - pomaga łatwo usunąć ciasto poprzez obracanie noża\n",
      "date: ['1 g, 42 min'], name: Karta graficzna Sapphire Radeon RX 6700 XT Pulse Gaming 12GB GDDR6 (11306-02-20G) @ Morele\n",
      "date: ['1 g, 57 min'], name: Zestaw kredek w 300 kolorach w solidnym opakowaniu.\n",
      "date: ['1 g, 59 min'], name: Smartwatch dla dzieci KidiZ TOP różowy\n",
      "date: ['2 g, 2 min'], name: Apple Watch SE 2-gen 44 mm czarny\n",
      "date: ['2 g, 2 min'], name: Gra Ori the collection nintendo switch\n",
      "date: ['2 g, 9 min'], name: Plecak turystyczny Quechua NH Escape 500 23 litry / plecak kabinowy\n",
      "date: ['Stacjonarnie', '2 g, 10 min'], name: Pepsi, Pepsi Max, Mirinda, 7up 2x2.5L\n",
      "date: ['2 g, 14 min'], name: 8 dni w Japonii za 3322zł. Bezpośrednie loty z Warszawy do Tokio i noclegi w hotelu w terminie 6.02-14.02\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "articles = soup.find_all('article')\n",
    "for article in articles:\n",
    "    name = article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "    date_tag = article.find_all('div', {\"class\":\"size--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "    raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "    items_to_remove = list()\n",
    "    filtered_list = list()\n",
    "    for string in raw_string_list:\n",
    "        if \"/\" in string:\n",
    "            items_to_remove.append(string)\n",
    "        if \":\" in string:\n",
    "            items_to_remove.append(string)\n",
    "        if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\"]:\n",
    "            items_to_remove.append(string)\n",
    "        if string.startswith(\"Wysyłka\"):\n",
    "            items_to_remove.append(string)\n",
    "    counts = Counter(items_to_remove)\n",
    "    for string in raw_string_list:\n",
    "        if counts[string]:\n",
    "            counts[string] -= 1\n",
    "        else:\n",
    "            filtered_list.append(string)\n",
    "\n",
    "\n",
    "    print(f\"date: {filtered_list}, name: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-03\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "from typing import List, Union\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.set_window_size(1400,1000)\n",
    "driver.get(\"https://www.pepper.pl/promocje/akcja-swiec-blaskiem-z-odblaskiem-darmowe-odblaski-w-warszawie-761817\")\n",
    "time.sleep(0.7)\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, \"html5lib\")\n",
    "\n",
    "\n",
    "date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "filtered_list = date_string.split()\n",
    "day_string = filtered_list[0]\n",
    "month_string = filtered_list[1]\n",
    "year_string = filtered_list[2].strip(',')\n",
    "\n",
    "if len(day_string[0]) == 2:\n",
    "    day = day_string\n",
    "else:\n",
    "    day = day_string.zfill(2)\n",
    "\n",
    "month = Months.__members__[month_string].value\n",
    "year = year_string\n",
    "prepared_data = '-'.join([year, month, day])\n",
    "\n",
    "print(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from datetime import datetime, timedelta, date, timezone\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "import signal\n",
    "import traceback\n",
    "from source.pepper_app.constans import OLD_DATES_DATA_PATTERN_1, OLD_DATES_DATA_PATTERN_2\n",
    "\n",
    "from source.pepper_app.environment_config import CustomEnvironment\n",
    "from source.pepper_app.constans import DATA_HEADER, STATS_HEADER, REQUEST_HEADER, RESPONSE_HEADER\n",
    "\n",
    "from collections import Counter\n",
    "from requests.exceptions import ConnectionError, HTTPError, MissingSchema, ReadTimeout\n",
    "from source.pepper_app.constans import (CSV_COLUMNS,\n",
    "                                STATS_HEADER)\n",
    "\n",
    "class ScrapPage:\n",
    "\n",
    "\n",
    "    def __init__(self, category_type: str, articles_to_retrieve: int, to_csv: bool=False,\n",
    "                to_database: bool=False, to_statistics: bool=True, searched_article: str='NA', \n",
    "                scrap_continuously: bool=False, scrap_choosen_data: bool=True) -> None:\n",
    "        self.category_type = category_type\n",
    "        self.articles_to_retrieve = articles_to_retrieve\n",
    "        self.to_database = to_database\n",
    "        self.to_csv = to_csv\n",
    "        self.to_statistics = to_statistics\n",
    "        self.start_page = 1\n",
    "        self.searched_article = searched_article\n",
    "        self.scrap_continuously = scrap_continuously\n",
    "        self.scrap_choosen_data = scrap_choosen_data\n",
    "\n",
    "    def scrap_page(self, url_to_scrap: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "        \"\"\"Setting up selenium webdriver, scraping page with bs4.\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options)\n",
    "                #driver = webdriver.Remote(command_executor='http://selenium-hub:4444/wd/hub', options=options)\n",
    "            driver.set_window_size(1400,1000)\n",
    "            driver.get(url_to_scrap)\n",
    "            time.sleep(0.7)\n",
    "            page = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page, \"html5lib\")\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "    def select_url(self) -> str:\n",
    "        \"\"\"Selection of the website address depending on the type of scrapping.\"\"\"\n",
    "        if self.scrap_continuously == True:\n",
    "            url_to_scrap = \"\".join([CustomEnvironment.get_url(), \"nowe\"])\n",
    "            return url_to_scrap\n",
    "        elif self.category_type == \"nowe\" and self.scrap_continuously == False:\n",
    "            url_to_scrap = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?page=\", str(self.start_page)])\n",
    "            return url_to_scrap\n",
    "        elif self.category_type == \"search\" and self.scrap_continuously == False:\n",
    "            searched_article = str(self.searched_article.replace(\" \",\"%20\"))\n",
    "            url_to_scrap = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?q=\",\n",
    "                                    searched_article, \"&page=\", str(self.start_page)])\n",
    "            return url_to_scrap\n",
    "        else:\n",
    "            raise Exception(f\"The variables were defined incorrectly.\")\n",
    "\n",
    "\n",
    "    def infinite_scroll_handling(self) -> List[str]:\n",
    "        \"\"\"Handling scraping through subsequent pages.\"\"\"\n",
    "        try:\n",
    "            flag = True\n",
    "            retrived_articles = list()\n",
    "            while flag:\n",
    "                url_to_scrap = self.select_url()\n",
    "                soup = self.scrap_page(url_to_scrap)\n",
    "                flag_nowe = CheckConditions(soup).check_if_last_page_nowe()\n",
    "                flag_search = CheckConditions(soup).check_if_last_page_search()\n",
    "                if flag_nowe == False or flag_search == False:\n",
    "                    flag = False\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                flag = CheckConditions(soup).check_if_no_items_found()\n",
    "                if flag == False:\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                if flag == True:\n",
    "                    articles = soup.find_all('article')\n",
    "                    retrived_articles += articles\n",
    "                else:\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                if len(retrived_articles) >= self.articles_to_retrieve:\n",
    "                    flag = False\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "                self.start_page += 1\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Infinite scroll failed:\\\n",
    "                            {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "\n",
    "    def get_items_details_depending_on_the_function(self):\n",
    "        \"\"\"Completing the list of articles and extracting data details depending on the type of scrapping.\"\"\"\n",
    "        if self.scrap_continuously == True and self.scrap_choosen_data == False:\n",
    "            while True:\n",
    "                retrived_articles = self.scrap_continuously_by_refreshing_page()\n",
    "                self.get_items_details(retrived_articles)\n",
    "        elif self.scrap_continuously == False and self.scrap_choosen_data == True:\n",
    "            retrived_articles = self.infinite_scroll_handling()\n",
    "            all_items = self.get_items_details(retrived_articles)\n",
    "            return all_items\n",
    "        else:\n",
    "            raise Exception(f\"Matching get_items_details depending on the selected \\\n",
    "                            functionality failed. \\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "    def get_items_details(self, retrived_articles) -> list():\n",
    "        \"\"\"Getting item detailes.\"\"\"\n",
    "        start_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        all_items = list()\n",
    "        try:\n",
    "            for article in retrived_articles:\n",
    "                item = list()\n",
    "                item.append(GetItemId(article).get_data())\n",
    "                item.append(GetItemName(article).get_data())\n",
    "                item.append(GetItemDiscountPrice(article).get_data())\n",
    "                item.append(GetItemPercentageDiscount(article).get_data())\n",
    "                item.append(GetItemRegularPrice(article).get_data())\n",
    "                item.append(GetItemAddedDate(article).get_data())\n",
    "                item.append(GetItemUrl(article).get_data())\n",
    "                if item not in all_items:\n",
    "                    all_items.append(item)\n",
    "                if '' in item:\n",
    "                    logging.warning(\"Data retrieving failed. None values detected\")\n",
    "                    break\n",
    "                if self.to_csv:\n",
    "                    self.save_data_to_csv(item)\n",
    "                #if self.to_database:\n",
    "                #    LoadItemDetailsToDatabase(item).load_to_db()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item details failed :\\\n",
    "                        {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "        end_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = end_time - start_time\n",
    "\n",
    "        if self.to_statistics:\n",
    "            try:\n",
    "                stats_info = self.get_scraping_stats_info(action_execution_datetime)\n",
    "                #LoadScrapingStatisticsToDatabase(stats_info).load_to_db()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Populating ScrapingStatistics table failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "        return all_items\n",
    "\n",
    "    def save_data_to_csv(self, item) -> None:\n",
    "        \"\"\"Saving data to csv file.\"\"\"\n",
    "        try:\n",
    "            header = False\n",
    "            if not os.path.exists('scraped.csv'):\n",
    "                header = True\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "            else:\n",
    "                header = False\n",
    "                df_e = pd.read_csv('scraped.csv')\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                if df['item_id'][0] not in df_e['item_id'].tolist():\n",
    "                    df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Saving data to csv failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "    def get_scraping_stats_info(self, action_execution_datetime: datetime) -> List[Union[str, int, bool, float]]:\n",
    "        \"\"\"Getting scraping stats info.\"\"\"\n",
    "        stats_info = list()\n",
    "\n",
    "        category_type = self.category_type\n",
    "        retrieved_articles_quantity = self.articles_to_retrieve\n",
    "        time_of_the_action = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = action_execution_datetime\n",
    "        searched_article = self.searched_article\n",
    "        to_csv = self.to_csv\n",
    "        to_database  = self.to_database\n",
    "        scrap_continuously = self.scrap_continuously\n",
    "        scrap_choosen_data = self.scrap_choosen_data\n",
    "\n",
    "        stats=[category_type, retrieved_articles_quantity,\n",
    "            time_of_the_action, action_execution_datetime, searched_article,\n",
    "            to_csv, to_database, scrap_continuously, scrap_choosen_data]\n",
    "\n",
    "        for field in stats:\n",
    "            stats_info.append(field)\n",
    "\n",
    "        return stats_info\n",
    "\n",
    "\n",
    "    def scrap_continuously_by_refreshing_page(self) -> List[str]:\n",
    "        \"\"\"Scraping data function for continuously scraping feature.\"\"\"\n",
    "        retrived_articles = list()\n",
    "\n",
    "        soup = self.scrap_page()\n",
    "        time.sleep(20)\n",
    "        articles = soup.find_all('article')\n",
    "        retrived_articles += articles\n",
    "\n",
    "        return retrived_articles\n",
    "\n",
    "\n",
    "class CheckConditions:\n",
    "\n",
    "\n",
    "    def __init__(self, soup: BeautifulSoup) -> None:\n",
    "        self.soup = soup\n",
    "\n",
    "\n",
    "    def check_if_last_page_nowe(self) -> bool:\n",
    "        \"\"\"Checking 'nowe' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h1', {\"class\":\"size--all-xl size--fromW3-xxl text--b space--b-2\"})[0].get_text()\n",
    "            if searched_ending_string.startswith(\"Ups\"):\n",
    "                logging.warning(\"No more pages to scrap.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    def check_if_last_page_search(self) -> bool:\n",
    "        \"\"\"Checking 'search' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-l\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"box--all-i size--all-s vAlign--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t Okazje()\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number > 0:\n",
    "                logging.warning(\"No more pages to scrap.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    def check_if_no_items_found(self) -> bool:\n",
    "        \"\"\"Checking if searched item was found.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-l\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"box--all-i size--all-s vAlign--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t Okazje()\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number == 0:\n",
    "                logging.warning(\"The searched item was not found.\")\n",
    "                return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "\n",
    "class GetItemName:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            name = self.article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "            return name\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item name failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemId:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> int:\n",
    "        try:\n",
    "            item_id = self.article.get(\"id\")\n",
    "            item_id = item_id.strip('thread_')\n",
    "            item_id = int(item_id)\n",
    "            return item_id\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item id failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemDiscountPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "            if len(discount_price) > 0:\n",
    "                discount_price = discount_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ','')\n",
    "                if discount_price == \"ZADARMO\":\n",
    "                    discount_price = float(0)\n",
    "                else:\n",
    "                    discount_price = float(discount_price)\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                discount_price = \"NA\"\n",
    "            return discount_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item discount price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "\n",
    "class GetItemRegularPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            regular_price = self.article.find_all(attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"})\n",
    "            if len(regular_price) > 0:\n",
    "                regular_price = float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ',''))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                regular_price = \"NA\"\n",
    "            return regular_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item regular price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "\n",
    "class GetItemPercentageDiscount:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            percentage_discount = self.article.find_all(attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            if len(percentage_discount) > 0:\n",
    "                percentage_discount = float(percentage_discount[0].get_text().strip('%'))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                percentage_discount = \"NA\"\n",
    "            return percentage_discount\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item percentage discount failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemUrl:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            item_url = self.article.find_all('a', {\"class\":\"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['href']\n",
    "            return item_url\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item url failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemAddedDate:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_raw_data(self) -> List[str]:\n",
    "\n",
    "        try:\n",
    "            date_tag = self.article.find_all('div', {\"class\":\"size--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "            raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "            return raw_string_list\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item added date failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "\n",
    "        try:\n",
    "            filtered_list = self.clean_list()\n",
    "            filtered_list = self.check_missing_date()\n",
    "            date_string_likely = filtered_list[0]\n",
    "            if date_string_likely == \"NA\":\n",
    "                url_with_item = GetItemUrl.get_data(self)\n",
    "                soup = self.scrap_page(url_with_item)\n",
    "                prepared_data = self.fill_missing_date(soup)\n",
    "                return prepared_data\n",
    "            else:\n",
    "                stripped_date_string_likely = self.strip_date_string(date_string_likely)\n",
    "                prepared_data = self.date_format_conversion(stripped_date_string_likely)\n",
    "            return prepared_data\n",
    "\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name: {e}\")\n",
    "\n",
    "    def strip_date_string(self, date_string_likely: str) -> str:\n",
    "\n",
    "        try:\n",
    "            if date_string_likely.startswith(\"Zaktualizowano \") and date_string_likely.endswith(\" temu\"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                stripped_date_string_likely = stripped_date_string_likely.rstrip(\" temu\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.endswith(\"Lokalnie\"):\n",
    "                stripped_date_string_likely = date_string_likely.rstrip(\"Lokalnie\")\n",
    "                return stripped_date_string_likely\n",
    "            else:\n",
    "                stripped_date_string_likely = date_string_likely\n",
    "                return stripped_date_string_likely\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Stripping date string failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def date_format_conversion(self, stripped_date_string_likely: str) -> str:\n",
    "\n",
    "        try:\n",
    "            if stripped_date_string_likely.endswith(('min', 'g', 's')):\n",
    "                prepared_data = date.today().strftime(\"%Y-%m-%d\")\n",
    "                return prepared_data\n",
    "            elif stripped_date_string_likely.startswith(tuple(Months.keys())) and len(stripped_date_string_likely) < 8:\n",
    "                if len(stripped_date_string_likely[4:]) == 3:\n",
    "                    day = stripped_date_string_likely[4:6]\n",
    "                else:\n",
    "                    day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = str(date.today().year)\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_1, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:6]\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[8:13]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_2, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[7:12]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Data format conversion tailed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "    def clean_list(self) -> List[str]:\n",
    "\n",
    "        raw_string_list = self.get_raw_data()\n",
    "        items_to_remove = list()\n",
    "        filtered_list = list()\n",
    "\n",
    "        try:\n",
    "            for string in raw_string_list:\n",
    "                if \"/\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if \":\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\", \"Stacjonarnie\"]:\n",
    "                    items_to_remove.append(string)\n",
    "                if string.startswith(\"Wysyłka\"):\n",
    "                    items_to_remove.append(string)\n",
    "\n",
    "            counts = Counter(items_to_remove)\n",
    "\n",
    "            for string in raw_string_list:\n",
    "                if counts[string]:\n",
    "                    counts[string] -= 1\n",
    "                else:\n",
    "                    filtered_list.append(string)\n",
    "\n",
    "            return filtered_list\n",
    "\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "\n",
    "    def check_missing_date(self) -> List[str]:\n",
    "\n",
    "        filtered_list = self.clean_list()\n",
    "\n",
    "        try:\n",
    "            if len(filtered_list) == 0:\n",
    "                filtered_list.append(\"NA\")\n",
    "                return filtered_list\n",
    "            else:\n",
    "                return filtered_list\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "\n",
    "    def fill_missing_date(self, soup) -> str:\n",
    "\n",
    "        try:\n",
    "            date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "            filtered_list = date_string.split()\n",
    "            day_string = filtered_list[0]\n",
    "            month_string = filtered_list[1]\n",
    "            year_string = filtered_list[2].strip(',')\n",
    "\n",
    "            if len(day_string[0]) == 2:\n",
    "                day = day_string\n",
    "            else:\n",
    "                day = day_string.zfill(2)\n",
    "\n",
    "            month = Months.__members__[month_string].value\n",
    "            year = year_string\n",
    "            prepared_data = '-'.join([year, month, day])\n",
    "            return prepared_data\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "\n",
    "    def scrap_page(self, url_with_item: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options)\n",
    "                #driver = webdriver.Remote(command_executor='http://selenium-hub:4444/wd/hub', options=options)\n",
    "            driver.get(url_with_item)\n",
    "            time.sleep(0.7)\n",
    "            page_with_item = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page_with_item, 'html5lib')\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "category_type = \"nowe\"\n",
    "articles_to_retrieve = 100\n",
    "\n",
    "output = ScrapPage(category_type, articles_to_retrieve)\n",
    "all_items = output.get_items_details_depending_on_the_function()\n",
    "\n",
    "for i in all_items:\n",
    "    if i[5] == None:\n",
    "        print(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
