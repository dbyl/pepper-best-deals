{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from enum import Enum, IntEnum\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_scrap = \"\"\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.set_window_size(1400,1000)\n",
    "driver.get(\"https://www.pepper.pl/search?q=s21&page=19\")\n",
    "time.sleep(0.7)\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page, \"html5lib\")\n",
    "\n",
    "with open(\"soup_searched_item_last_page.html\", \"w\") as file:\n",
    "    file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "387\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "path_to_file = Path(\"source/pepper_app/tests/fixtures/to_test_scrape/soup_searched_item_found.html\")\n",
    "with open(path_to_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = file.read()\n",
    "soup = BeautifulSoup(soup, \"html5lib\")\n",
    "articles = soup.find_all('article')\n",
    "retrived_articles = articles[:]\n",
    "\n",
    "searched_ending_string = soup.find_all('h3', {\"class\":\"size--all-xl text--b\"})\n",
    "searched_articles_number = soup.find_all('span', {\"class\":\"text--color-charcoalTint size--all-m\"})[0].get_text()\n",
    "searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t okazje\"))\n",
    "\n",
    "print(searched_ending_string)\n",
    "print(searched_articles_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">211,72zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">19,99zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">707,90zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">342zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">659,99zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">11,99zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">199zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">299,19zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">2 029zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">278zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">260,90zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">168,25zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">279,99zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">498zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">13,50zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">2 499zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">139zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">709,78zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">709,78zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">1,99zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">4,99zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">1zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">80,98zł</span>]\n",
      "[]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">14,95zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">2,99zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">7zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">249zł</span>]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">84zł</span>]\n",
      "[]\n",
      "[<span class=\"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\">5,99zł</span>]\n"
     ]
    }
   ],
   "source": [
    "articles = soup.find_all('article')\n",
    "\n",
    "article = articles[15]\n",
    "article_id =  article.get(\"id\")\n",
    "name = article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "discount_price = article.find_all(\"span\", {'class': \"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\"})\n",
    "regular_price = article.find_all(\"span\", {'class': \"mute--text text--lineThrough space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "percentage_discount = article.find_all(\"span\", {'class': \"text--color-charcoal space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "date_tag = article.find_all('div', {\"class\":\"flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "url = article.find_all('a', {\"class\":\"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['href']\n",
    "\n",
    "\"\"\"\n",
    "print(article_id)\n",
    "print(name)\n",
    "print(discount_price)\n",
    "print(regular_price)\n",
    "print(percentage_discount)\n",
    "print(date_tag)\n",
    "print(raw_string_list)\n",
    "print(url)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for article in articles:\n",
    "    article_id =  article.get(\"id\")\n",
    "    name = article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "    discount_price = article.find_all(\"span\", {'class': \"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\"})\n",
    "    regular_price = article.find_all(\"span\", {'class': \"mute--text text--lineThrough space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "    percentage_discount = article.find_all(\"span\", {'class': \"text--color-charcoal space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "    date_tag = article.find_all('div', {\"class\":\"flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "    raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "    \n",
    "\n",
    "\n",
    "    #print(f\"{raw_string_list}\")\n",
    "    print(discount_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m date_tag \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      6\u001b[0m discount_price \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind_all(attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread-price text--b cept-tp size--all-l size--fromW3-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m----> 7\u001b[0m raw_string_list \u001b[38;5;241m=\u001b[39m \u001b[43mdate_tag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m items_to_remove \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m      9\u001b[0m filtered_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "File \u001b[0;32m~/pepper-best-deals/env/lib/python3.10/site-packages/bs4/element.py:2428\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   2427\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   2429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResultSet object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key\n\u001b[1;32m   2430\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "articles = soup.find_all('article')\n",
    "for article in articles:\n",
    "    name = article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title']\n",
    "    date_tag = article.find_all('div', {\"class\":\"size--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "    discount_price = article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "    raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "    items_to_remove = list()\n",
    "    filtered_list = list()\n",
    "    for string in raw_string_list:\n",
    "        if \"/\" in string:\n",
    "            items_to_remove.append(string)\n",
    "        if \":\" in string:\n",
    "            items_to_remove.append(string)\n",
    "        if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\"]:\n",
    "            items_to_remove.append(string)\n",
    "        if string.startswith(\"Wysyłka\"):\n",
    "            items_to_remove.append(string)\n",
    "    counts = Counter(items_to_remove)\n",
    "    for string in raw_string_list:\n",
    "        if counts[string]:\n",
    "            counts[string] -= 1\n",
    "        else:\n",
    "            filtered_list.append(string)\n",
    "\n",
    "\n",
    "    print(f\"discount price: {discount_price} date: {filtered_list}, name: {name}\")\n",
    "    #print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 50\u001b[0m\n\u001b[1;32m     45\u001b[0m page \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n\u001b[1;32m     47\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml5lib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m date_string \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspace--mv-3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     51\u001b[0m filtered_list \u001b[38;5;241m=\u001b[39m date_string\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     52\u001b[0m day_string \u001b[38;5;241m=\u001b[39m filtered_list[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "from typing import List, Union\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.set_window_size(1400,1000)\n",
    "driver.get(\"https://www.pepper.pl/search?q=s20&page=5\")\n",
    "time.sleep(0.7)\n",
    "page = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(page, \"html5lib\")\n",
    "\n",
    "\n",
    "date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "filtered_list = date_string.split()\n",
    "day_string = filtered_list[0]\n",
    "month_string = filtered_list[1]\n",
    "year_string = filtered_list[2].strip(',')\n",
    "\n",
    "if len(day_string[0]) == 2:\n",
    "    day = day_string\n",
    "else:\n",
    "    day = day_string.zfill(2)\n",
    "\n",
    "month = Months.__members__[month_string].value\n",
    "year = year_string\n",
    "prepared_data = '-'.join([year, month, day])\n",
    "\n",
    "print(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 169 (155229559.py, line 172)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 172\u001b[0;36m\u001b[0m\n\u001b[0;31m    return all_items\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 169\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from enum import Enum, IntEnum\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from typing import List, Union\n",
    "from datetime import datetime, timedelta, date, timezone\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "import signal\n",
    "import traceback\n",
    "from source.pepper_app.constans import OLD_DATES_DATA_PATTERN_1, OLD_DATES_DATA_PATTERN_2\n",
    "\n",
    "from source.pepper_app.environment_config import CustomEnvironment\n",
    "from source.pepper_app.constans import DATA_HEADER, STATS_HEADER, REQUEST_HEADER, RESPONSE_HEADER\n",
    "\n",
    "from collections import Counter\n",
    "from requests.exceptions import ConnectionError, HTTPError, MissingSchema, ReadTimeout\n",
    "from source.pepper_app.constans import (CSV_COLUMNS,\n",
    "                                STATS_HEADER)\n",
    "\n",
    "\n",
    "class ScrapePage:\n",
    "\n",
    "    def __init__(self, category_type: str, articles_to_retrieve: int, to_csv: bool=False,\n",
    "                to_database: bool=True, to_statistics: bool=True, searched_article: str='NA', \n",
    "                scrape_continuously: bool=False, scrape_choosen_data: bool=True) -> None:\n",
    "        self.category_type = category_type\n",
    "        self.articles_to_retrieve = articles_to_retrieve\n",
    "        self.to_csv = to_csv\n",
    "        self.to_database = to_database\n",
    "        self.to_statistics = to_statistics\n",
    "        self.start_page = 1\n",
    "        self.searched_article = searched_article\n",
    "        self.scrape_continuously = scrape_continuously\n",
    "        self.scrape_choosen_data = scrape_choosen_data\n",
    "\n",
    "    def scrape_page(self, url_to_scrape: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "        \"\"\"Setting up selenium webdriver, scraping page with bs4.\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            #options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options) #for local  \n",
    "                #driver = webdriver.Remote(command_executor=f'http://{CustomEnvironment.get_selenium_container_name()}:4444/wd/hub', options=options) #for docker \n",
    "            driver.set_window_size(1400,1000)\n",
    "            driver.get(url_to_scrape)\n",
    "            select = Select(driver.find_element_by_id('select-ctrl input clickable'))\n",
    "            select.select_by_value('0')\n",
    "            time.sleep(0.7)\n",
    "            page = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page, \"html5lib\")\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "    def select_url(self) -> str:\n",
    "        \"\"\"Selection of the website address depending on the type of scraping.\"\"\"\n",
    "        if self.scrape_continuously == True:\n",
    "            url_to_scrape = \"\".join([CustomEnvironment.get_url(), \"nowe\"])\n",
    "            return url_to_scrape\n",
    "        elif self.category_type == \"nowe\" and self.scrape_continuously == False:\n",
    "            url_to_scrape = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?page=\", str(self.start_page)])\n",
    "            return url_to_scrape\n",
    "        elif self.category_type == \"search\" and self.scrape_continuously == False:\n",
    "            searched_article = str(self.searched_article.replace(\" \",\"%20\"))\n",
    "            url_to_scrape = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?q=\",\n",
    "                                    searched_article, \"&page=\", str(self.start_page)])\n",
    "            return url_to_scrape\n",
    "        else:\n",
    "            raise Exception(f\"The variables were defined incorrectly.\")\n",
    "\n",
    "    def infinite_scroll_handling(self) -> List[str]:\n",
    "        \"\"\"Handling scraping through subsequent pages.\"\"\"\n",
    "        try:\n",
    "            flag = True\n",
    "            retrived_articles = list()\n",
    "            while flag:\n",
    "                url_to_scrape = self.select_url()\n",
    "                soup = self.scrape_page(url_to_scrape)\n",
    "                flag_nowe = CheckConditions(soup, self.start_page).check_if_last_page_nowe()\n",
    "                flag_search = CheckConditions(soup, self.start_page).check_if_last_page_search()\n",
    "\n",
    "                if flag_nowe == False or flag_search == False:\n",
    "                    flag = False\n",
    "\n",
    "                if self.category_type == \"search\":\n",
    "                    flag_found = CheckConditions(soup, self.start_page).check_if_no_items_found()\n",
    "                    if flag_found == False:\n",
    "                        all_items = 0\n",
    "                        return all_items\n",
    "\n",
    "                articles = soup.find_all('article')\n",
    "                retrived_articles += articles\n",
    "\n",
    "                if len(retrived_articles) >= self.articles_to_retrieve:\n",
    "                    flag = False\n",
    "                else:\n",
    "                    if self.scrape_continuously == True:\n",
    "                        time.sleep(10)\n",
    "                    else:\n",
    "                        self.start_page += 1\n",
    "                \n",
    "                prepared_articles = articles[:self.articles_to_retrieve]\n",
    "                all_items = self.get_items_details(prepared_articles)\n",
    "            return all_items\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Infinite scroll failed:\\\n",
    "                            {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "        \n",
    "\n",
    "    def get_items_details_depending_on_the_function(self):\n",
    "        \"\"\"Completing the list of articles and extracting data details depending on the type of scraping.\"\"\"\n",
    "        if self.scrape_continuously == True and self.scrape_choosen_data == False:\n",
    "            while True:\n",
    "                all_items = self.infinite_scroll_handling()\n",
    "                return all_items\n",
    "        elif self.scrape_continuously == False and self.scrape_choosen_data == True:\n",
    "            all_items = self.infinite_scroll_handling()\n",
    "            return all_items\n",
    "        else:\n",
    "            raise Exception(f\"Matching get_items_details depending on the selected \\\n",
    "                            functionality failed. \\n Tracking: {traceback.format_exc()}\")\n",
    "        \n",
    "    \n",
    "    def get_items_details(self, prepared_articles):\n",
    "        \"\"\"Getting item detailes.\"\"\"\n",
    "        start_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        all_items = list()\n",
    "        try:\n",
    "            for article in prepared_articles:\n",
    "                item = list()\n",
    "                item.extend([GetItemId(article).get_data(), \n",
    "                            GetItemName(article).get_data(),\n",
    "                            GetItemDiscountPrice(article).get_data(),\n",
    "                            GetItemPercentageDiscount(article).get_data(),\n",
    "                            GetItemRegularPrice(article).get_data(),\n",
    "                            GetItemAddedDate(article).get_data(),\n",
    "                            GetItemUrl(article).get_data()])\n",
    "                if item not in all_items:\n",
    "                    all_items.append(item)\n",
    "                if '' in item:\n",
    "                    logging.warning(\"Data retrieving failed. None values detected\")\n",
    "                    break\n",
    "                if self.to_csv:\n",
    "                    self.save_data_to_csv(item)\n",
    "                if self.to_database:\n",
    "                    #LoadItemDetailsToDatabase(item).load_to_db()\n",
    "                    if self.scrape_continuously == True:\n",
    "                        #RequestChecking(item).matching_request()\n",
    "                \n",
    "            return all_items\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item details failed :\\\n",
    "                        {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "        end_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = end_time - start_time\n",
    "\n",
    "        if self.to_statistics:\n",
    "            try:\n",
    "                stats_info = self.get_scraping_stats_info(action_execution_datetime)\n",
    "                #LoadScrapingStatisticsToDatabase(stats_info).load_to_db()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Populating ScrapingStatistics table failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "        return all_items\n",
    "\n",
    "    def save_data_to_csv(self, item) -> None:\n",
    "        \"\"\"Saving data to csv file.\"\"\"\n",
    "        try:\n",
    "            header = False\n",
    "            if not os.path.exists('scraped.csv'):\n",
    "                header = True\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "            else:\n",
    "                header = False\n",
    "                df_e = pd.read_csv('scraped.csv')\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                if df['item_id'][0] not in df_e['item_id'].tolist():\n",
    "                    df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Saving data to csv failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_scraping_stats_info(self, action_execution_datetime: datetime) -> List[Union[str, int, bool, float]]:\n",
    "        \"\"\"Getting scraping stats info.\"\"\"\n",
    "        stats_info = list()\n",
    "\n",
    "        category_type = self.category_type\n",
    "        retrieved_articles_quantity = self.articles_to_retrieve\n",
    "        time_of_the_action = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = action_execution_datetime\n",
    "        searched_article = self.searched_article\n",
    "        to_csv = self.to_csv\n",
    "        to_database  = self.to_database\n",
    "        scrape_continuously = self.scrape_continuously\n",
    "        scrape_choosen_data = self.scrape_choosen_data\n",
    "\n",
    "        stats=[category_type, retrieved_articles_quantity,\n",
    "            time_of_the_action, action_execution_datetime, searched_article,\n",
    "            to_csv, to_database, scrape_continuously, scrape_choosen_data]\n",
    "\n",
    "        for field in stats:\n",
    "            stats_info.append(field)\n",
    "\n",
    "        return stats_info\n",
    "    \n",
    "\n",
    "class CheckConditions:\n",
    "\n",
    "    def __init__(self, soup: BeautifulSoup, start_page: int) -> None:\n",
    "        self.soup = soup\n",
    "        self.start_page = start_page\n",
    "\n",
    "    def check_if_last_page_nowe(self) -> bool:\n",
    "        \"\"\"Checking 'nowe' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            if self.start_page == 335:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Checking if page in nowe category is last failed:\\\n",
    "                            {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "        \n",
    "\n",
    "    def check_if_last_page_search(self) -> bool:\n",
    "        \"\"\"Checking 'search' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-xl text--b\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"text--color-charcoalTint size--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t okazje\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number > 0:\n",
    "                logging.warning(\"No more pages to scrape.\")\n",
    "                return False\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "\n",
    "    def check_if_no_items_found(self) -> bool:\n",
    "        \"\"\"Checking if searched item was found.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-xl text--b\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"text--color-charcoalTint size--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t okazje\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number == 0:\n",
    "                logging.warning(\"The searched item was not found.\")\n",
    "                return False\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "\n",
    "class GetItemName:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            name = self.article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title'].lower()\n",
    "            return name\n",
    "        except IndexError as e:\n",
    "            name = self.article.find_all(attrs={'class': \"thread-link linkPlain thread-title--list js-thread-title\"})[0]['title'].lower()\n",
    "            return name\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item name failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemId:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> int:\n",
    "        try:\n",
    "            item_id = self.article.get(\"id\")\n",
    "            item_id = item_id.strip('thread_')\n",
    "            item_id = int(item_id)\n",
    "            return item_id\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item id failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemDiscountPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            discount_price = self.article.find_all(\"span\", {'class': \"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\"})\n",
    "            if len(discount_price) == 0:\n",
    "                discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl text--color-greyShade\"})\n",
    "\n",
    "            if len(discount_price) > 0:\n",
    "                discount_price = discount_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ','')\n",
    "                if discount_price == \"ZADARMO\":\n",
    "                    discount_price = float(0)\n",
    "                else:\n",
    "                    discount_price = float(discount_price)                \n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                discount_price = \"NA\"\n",
    "            return discount_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item discount price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemRegularPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            regular_price = self.article.find_all(\"span\", {'class': \"mute--text text--lineThrough space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            if len(regular_price) > 0:\n",
    "                regular_price = float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ',''))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                regular_price = \"NA\"\n",
    "            return regular_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item regular price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemPercentageDiscount:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            percentage_discount = self.article.find_all(\"span\", {'class': \"text--color-charcoal space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            if len(percentage_discount) > 0:\n",
    "                percentage_discount = float(percentage_discount[0].get_text().strip('%'))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                percentage_discount = \"NA\"\n",
    "            return percentage_discount\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item percentage discount failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemUrl:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            item_url = self.article.find_all('a', {\"class\":\"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['href']\n",
    "            return item_url\n",
    "        except IndexError as e:\n",
    "            item_url = 'NA - Login Required.'\n",
    "            return item_url\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item url failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemAddedDate:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_raw_data(self) -> List[str]:\n",
    "        try:\n",
    "            date_tag = self.article.find_all('div', {\"class\":\"flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "            raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "            return raw_string_list\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item added date failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            filtered_list = self.clean_list()\n",
    "            filtered_list = self.check_missing_date()\n",
    "            date_string_likely = filtered_list[0]\n",
    "            if date_string_likely == \"NA\":\n",
    "                url_with_item = GetItemUrl.get_data(self)\n",
    "                soup = self.scrape_page(url_with_item)\n",
    "                prepared_data = self.fill_missing_date(soup)\n",
    "                return prepared_data\n",
    "            else:\n",
    "                stripped_date_string_likely = self.strip_date_string(date_string_likely)\n",
    "                prepared_data = self.date_format_conversion(stripped_date_string_likely)\n",
    "            return prepared_data\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name: {e}\")\n",
    "\n",
    "    def strip_date_string(self, date_string_likely: str) -> str:\n",
    "        try:\n",
    "            if date_string_likely.startswith(\"Zaktualizowano \") and date_string_likely.endswith(\" temu\"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                stripped_date_string_likely = stripped_date_string_likely.rstrip(\" temu\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.endswith(\"Lokalnie\"):\n",
    "                stripped_date_string_likely = date_string_likely.rstrip(\"Lokalnie\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.startswith(\"Zaktualizowano \"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                return stripped_date_string_likely\n",
    "            else:\n",
    "                stripped_date_string_likely = date_string_likely\n",
    "                return stripped_date_string_likely\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Stripping date string failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def date_format_conversion(self, stripped_date_string_likely: str) -> str:\n",
    "        try:\n",
    "            if stripped_date_string_likely.endswith(('min', 'g', 's')):\n",
    "                prepared_data = date.today().strftime(\"%Y-%m-%d\")\n",
    "                return prepared_data\n",
    "            elif stripped_date_string_likely.startswith(tuple(Months.keys())) and len(stripped_date_string_likely) < 8:\n",
    "                if len(stripped_date_string_likely[4:]) == 3:\n",
    "                    day = stripped_date_string_likely[4:6]\n",
    "                else:\n",
    "                    day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = str(date.today().year)\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_1, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:6]\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[8:13]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_2, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[7:12]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Data format conversion failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def clean_list(self) -> List[str]:\n",
    "        raw_string_list = self.get_raw_data()\n",
    "        items_to_remove = list()\n",
    "        filtered_list = list()\n",
    "        try:\n",
    "            for string in raw_string_list:\n",
    "                if \"/\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if \":\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\", \"Stacjonarnie\"]:\n",
    "                    items_to_remove.append(string)\n",
    "                if string.startswith(\"Wysyłka\"):\n",
    "                    items_to_remove.append(string)\n",
    "            counts = Counter(items_to_remove)\n",
    "            for string in raw_string_list:\n",
    "                if counts[string]:\n",
    "                    counts[string] -= 1\n",
    "                else:\n",
    "                    filtered_list.append(string)\n",
    "            return filtered_list\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "    def check_missing_date(self) -> List[str]:\n",
    "        filtered_list = self.clean_list()\n",
    "        try:\n",
    "            if len(filtered_list) == 0:\n",
    "                filtered_list.append(\"NA\")\n",
    "                return filtered_list\n",
    "            else:\n",
    "                return filtered_list\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "    def fill_missing_date(self, soup) -> str:\n",
    "        try:\n",
    "            date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "            filtered_list = date_string.split()\n",
    "            day_string = filtered_list[0]\n",
    "            month_string = filtered_list[1]\n",
    "            year_string = filtered_list[2].strip(',')\n",
    "            if len(day_string[0]) == 2:\n",
    "                day = day_string\n",
    "            else:\n",
    "                day = day_string.zfill(2)\n",
    "            month = Months.__members__[month_string].value\n",
    "            year = year_string\n",
    "            prepared_data = '-'.join([year, month, day])\n",
    "            return prepared_data\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "    def scrape_page(self, url_with_item: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options) #for local  \n",
    "                #driver = webdriver.Remote(command_executor=f'http://{CustomEnvironment.get_selenium_container_name()}:4444/wd/hub', options=options) #for docker \n",
    "            driver.get(url_with_item)\n",
    "            time.sleep(0.7)\n",
    "            page_with_item = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page_with_item, 'html5lib')\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "\n",
    "\n",
    "category_type = \"search\"\n",
    "articles_to_retrieve = 30\n",
    "searched_article = \"S21\"\n",
    "scrape_continuously = False\n",
    "scrape_choosen_data = True\n",
    "to_database = False\n",
    "to_csv = False\n",
    "to_statistics = False\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "path_to_file = Path(\"source/pepper_app/tests/fixtures/to_test_get_info/soup_with_duplicates.html\")\n",
    "\n",
    "with open(path_to_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = file.read()\n",
    "soup = BeautifulSoup(soup, \"html5lib\")\n",
    "articles = soup.find_all('article')\n",
    "retrived_articles = articles[:]\"\"\"\n",
    "\n",
    "all_items = ScrapePage(category_type=category_type, articles_to_retrieve=articles_to_retrieve, scrape_continuously=scrape_continuously, \\\n",
    "        to_database=to_database, to_csv=to_csv, to_statistics=to_statistics).get_items_details(retrived_articles)\n",
    "\n",
    "\n",
    "for i in all_items:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gorące\n",
      "https://www.pepper.pl/gorące?page=1\n",
      "ulr: https://www.pepper.pl/gorące?page=1\n",
      "[807367, 'apple iphone 13 128gb - różne kolory', 2649.0, 'NA', 'NA', '2024-03-09', 'https://www.pepper.pl/promocje/apple-iphone-13-128gb-rozne-kolory-807367']\n",
      "[807363, 'pochodnie lampy solarne ogrodowe 8 sztuk', 56.99, -29.0, 80.0, '2024-03-09', 'https://www.pepper.pl/promocje/pochodnie-lampy-solarne-ogrodowe-8-sztuk-807363']\n",
      "[807345, 'bourbon buffalo trace-whiskey oraz komes porter amburana @auchan komorniki', 80.0, -29.0, 111.99, '2024-03-09', 'https://www.pepper.pl/promocje/bourbon-buffalo-trace-whiskey-oraz-komes-porter-amburana-807345']\n",
      "[807477, 'kabel usb c - usb c sbs 1.5m usb 3.1', 0.99, 'NA', 'NA', '2024-03-09', 'https://www.pepper.pl/promocje/kabel-usb-c-usb-c-sbs-15m-usb-31-807477']\n",
      "[807234, 'smartfon motorola g73 5g 8/256gb - 160,02 €', 687.86, 'NA', 'NA', '2024-03-09', 'https://www.pepper.pl/promocje/smartfon-motorola-g73-5g-8256gb-16002-eur-807234']\n",
      "[807402, 'kosz na pranie songmics - 4 zdejmowane przegródki, stelaż na kółkach, 140l (cena z prime)', 85.49, -43.0, 150.1, '2024-03-09', 'https://www.pepper.pl/promocje/kosz-na-pranie-songmics-4-przegrodki-z-wyjmowanym-wkladem-stelaz-na-kolkach-140l-807402']\n",
      "[807377, 'project screwed - steam - gra za darmo', 0.0, 'NA', 63.63, '2024-03-09', 'https://www.pepper.pl/promocje/project-screwed-steam-gra-za-darmo-807377']\n",
      "[807446, 'kupon 10zł w inpost fresh mwz 60zł', 'NA', 'NA', 'NA', '2024-03-09', 'https://www.pepper.pl/kupony/kupon-10zl-w-inpost-fresh-807446']\n",
      "[807470, 'smartfon samsung galaxy s23 256gb ładowarka 45w w zestawie 629,11 €', 2710.0, 'NA', 'NA', '2024-03-09', 'https://www.pepper.pl/promocje/smartfon-samsung-galaxy-s23-256gb-ladowarka-w-zestawie-62911-eur-807470']\n",
      "[807360, 'takeme koszulka i stringi', 22.99, -30.0, 32.99, '2024-03-09', 'https://www.pepper.pl/promocje/takeme-koszulka-i-stringi-807360']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "from datetime import datetime, timedelta, date, timezone\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "import signal\n",
    "import traceback\n",
    "from source.pepper_app.constans import OLD_DATES_DATA_PATTERN_1, OLD_DATES_DATA_PATTERN_2\n",
    "\n",
    "from source.pepper_app.environment_config import CustomEnvironment\n",
    "from source.pepper_app.constans import DATA_HEADER, STATS_HEADER, REQUEST_HEADER, RESPONSE_HEADER\n",
    "\n",
    "from collections import Counter\n",
    "from requests.exceptions import ConnectionError, HTTPError, MissingSchema, ReadTimeout\n",
    "from source.pepper_app.constans import (CSV_COLUMNS,\n",
    "                                STATS_HEADER)\n",
    "\n",
    "class ScrapePage:\n",
    "\n",
    "    def __init__(self, category_type: str, articles_to_retrieve: int, to_csv: bool=False,\n",
    "                to_database: bool=True, to_statistics: bool=True, searched_article: str='NA', \n",
    "                scrape_continuously: bool=False, scrape_choosen_data: bool=True) -> None:\n",
    "        self.category_type = category_type\n",
    "        self.articles_to_retrieve = articles_to_retrieve\n",
    "        self.to_csv = to_csv\n",
    "        self.to_database = to_database\n",
    "        self.to_statistics = to_statistics\n",
    "        self.start_page = 1\n",
    "        self.searched_article = searched_article\n",
    "        self.scrape_continuously = scrape_continuously\n",
    "        self.scrape_choosen_data = scrape_choosen_data\n",
    "\n",
    "    def scrape_page(self, url_to_scrape: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "        \"\"\"Setting up selenium webdriver, scraping page with bs4.\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            #options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--disable-notifications\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options) #for local  \n",
    "                #driver = webdriver.Remote(command_executor=f'http://{CustomEnvironment.get_selenium_container_name()}:4444/wd/hub', options=options) #for docker \n",
    "            driver.set_window_size(1920,1080)\n",
    "            print(f\"ulr: {url_to_scrape}\")\n",
    "            driver.get(url_to_scrape)\n",
    "            if self.category_type == \"search\":\n",
    "                self.accept_cookies_and_change_interval_if_searching(driver)\n",
    "            time.sleep(1)\n",
    "            page = driver.page_source\n",
    "            time.sleep(1)\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page, \"html5lib\")\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "        \n",
    "    def accept_cookies_and_change_interval_if_searching(self, driver) -> None:\n",
    "        \"\"\"Function accepts cookies and sets the correct time interval to search for all articles\"\"\"\n",
    "        try:\n",
    "            time.sleep(0.7)\n",
    "            driver.find_element(By.XPATH, \"//button[contains(@class, 'overflow--wrap-on flex--grow-1 flex--fromW3-grow-0 width--fromW3-ctrl-m space--mb-3 \\n space--fromW3-mb-0 space--fromW3-mr-2 button button--shape-circle button--type-primary button--mode-brand')]\").click()\n",
    "            select = Select(driver.find_element(By.XPATH,\"//div[contains(@class, 'space--mt-3 space--t-3 space--r-3 space--l-3 border--t border--color-greyBackground')]//select[contains(@class, 'select-ctrl input clickable')]\"))\n",
    "            select.select_by_value('0')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Accepting cookiens and changing time interval failed :\\\n",
    "                        {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def select_url(self) -> str:\n",
    "        \"\"\"Selection of the website address depending on the type of scraping.\"\"\"\n",
    "        if self.scrape_continuously == True:\n",
    "            url_to_scrape = \"\".join([CustomEnvironment.get_url(), \"nowe\"])\n",
    "            return url_to_scrape\n",
    "        elif self.category_type == \"nowe\" and self.scrape_continuously == False:\n",
    "            url_to_scrape = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?page=\", str(self.start_page)])\n",
    "            return url_to_scrape\n",
    "        elif self.category_type == \"gorące\" and self.scrape_continuously == False:\n",
    "            print(self.category_type)\n",
    "            url_to_scrape = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?page=\", str(self.start_page)])\n",
    "            print(url_to_scrape)\n",
    "            return url_to_scrape\n",
    "        elif self.category_type == \"search\" and self.scrape_continuously == False:\n",
    "            searched_article = str(self.searched_article.replace(\" \",\"%20\"))\n",
    "            url_to_scrape = \"\".join([CustomEnvironment.get_url(), self.category_type, \"?q=\",\n",
    "                                    searched_article, \"&page=\", str(self.start_page)])\n",
    "            \n",
    "            return url_to_scrape\n",
    "        else:\n",
    "            raise Exception(f\"The variables were defined incorrectly.\")\n",
    "\n",
    "    def infinite_scroll_handling(self) -> List[str]:\n",
    "        \"\"\"Handling scraping through subsequent pages.\"\"\"\n",
    "        try:\n",
    "            flag = True\n",
    "            retrived_articles = list()\n",
    "            while flag:\n",
    "                url_to_scrape = self.select_url()\n",
    "                soup = self.scrape_page(url_to_scrape)\n",
    "                flag_nowe = CheckConditions(soup, self.start_page).check_if_last_page_nowe()\n",
    "                flag_search = CheckConditions(soup, self.start_page).check_if_last_page_search()\n",
    "\n",
    "                if flag_nowe == False or flag_search == False:\n",
    "                    flag = False\n",
    "\n",
    "                if self.category_type == \"search\":\n",
    "                    flag_found = CheckConditions(soup, self.start_page).check_if_no_items_found()\n",
    "                    if flag_found == False:\n",
    "                        all_items = 0\n",
    "                        return all_items\n",
    "\n",
    "                articles = soup.find_all('article')\n",
    "                retrived_articles += articles\n",
    "\n",
    "                if len(retrived_articles) >= self.articles_to_retrieve:\n",
    "                    flag = False\n",
    "                else:\n",
    "                    if self.scrape_continuously == True:\n",
    "                        time.sleep(10)\n",
    "                    else:\n",
    "                        self.start_page += 1\n",
    "                \n",
    "                prepared_articles = articles[:self.articles_to_retrieve]\n",
    "                all_items = self.get_items_details(prepared_articles)\n",
    "            return all_items\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Infinite scroll failed:\\\n",
    "                            {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "        \n",
    "\n",
    "    def get_items_details_depending_on_the_function(self):\n",
    "        \"\"\"Completing the list of articles and extracting data details depending on the type of scraping.\"\"\"\n",
    "        if self.scrape_continuously == True and self.scrape_choosen_data == False:\n",
    "            while True:\n",
    "                all_items = self.infinite_scroll_handling()\n",
    "                return all_items\n",
    "        elif self.scrape_continuously == False and self.scrape_choosen_data == True:\n",
    "            all_items = self.infinite_scroll_handling()\n",
    "            return all_items\n",
    "        else:\n",
    "            raise Exception(f\"Matching get_items_details depending on the selected \\\n",
    "                            functionality failed. \\n Tracking: {traceback.format_exc()}\")\n",
    "        \n",
    "    \n",
    "    def get_items_details(self, prepared_articles):\n",
    "        \"\"\"Getting item detailes.\"\"\"\n",
    "        start_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        all_items = list()\n",
    "        try:\n",
    "            for article in prepared_articles:\n",
    "                item = list()\n",
    "                item.extend([GetItemId(article).get_data(), \n",
    "                            GetItemName(article).get_data(),\n",
    "                            GetItemDiscountPrice(article).get_data(),\n",
    "                            GetItemPercentageDiscount(article).get_data(),\n",
    "                            GetItemRegularPrice(article).get_data(),\n",
    "                            GetItemAddedDate(article).get_data(),\n",
    "                            GetItemUrl(article).get_data()])\n",
    "                if item not in all_items:\n",
    "                    all_items.append(item)\n",
    "                if '' in item:\n",
    "                    logging.warning(\"Data retrieving failed. None values detected\")\n",
    "                    break\n",
    "                if self.to_csv:\n",
    "                    self.save_data_to_csv(item)\n",
    "                #if self.to_database:\n",
    "                    #LoadItemDetailsToDatabase(item).load_to_db()\n",
    "                    #if self.scrape_continuously == True:\n",
    "                        #RequestChecking(item).matching_request()\n",
    "                \n",
    "            return all_items\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item details failed :\\\n",
    "                        {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "        end_time = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = end_time - start_time\n",
    "\n",
    "        if self.to_statistics:\n",
    "            try:\n",
    "                stats_info = self.get_scraping_stats_info(action_execution_datetime)\n",
    "                #LoadScrapingStatisticsToDatabase(stats_info).load_to_db()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Populating ScrapingStatistics table failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "        return all_items\n",
    "\n",
    "    def save_data_to_csv(self, item) -> None:\n",
    "        \"\"\"Saving data to csv file.\"\"\"\n",
    "        try:\n",
    "            header = False\n",
    "            if not os.path.exists('scraped.csv'):\n",
    "                header = True\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "            else:\n",
    "                header = False\n",
    "                df_e = pd.read_csv('scraped.csv')\n",
    "                df = pd.DataFrame([item], columns=CSV_COLUMNS)\n",
    "                if df['item_id'][0] not in df_e['item_id'].tolist():\n",
    "                    df.to_csv('scraped.csv', header=header, index=False, mode='a')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Saving data to csv failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_scraping_stats_info(self, action_execution_datetime: datetime) -> List[Union[str, int, bool, float]]:\n",
    "        \"\"\"Getting scraping stats info.\"\"\"\n",
    "        stats_info = list()\n",
    "\n",
    "        category_type = self.category_type\n",
    "        retrieved_articles_quantity = self.articles_to_retrieve\n",
    "        time_of_the_action = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        action_execution_datetime = action_execution_datetime\n",
    "        searched_article = self.searched_article\n",
    "        to_csv = self.to_csv\n",
    "        to_database  = self.to_database\n",
    "        scrape_continuously = self.scrape_continuously\n",
    "        scrape_choosen_data = self.scrape_choosen_data\n",
    "\n",
    "        stats=[category_type, retrieved_articles_quantity,\n",
    "            time_of_the_action, action_execution_datetime, searched_article,\n",
    "            to_csv, to_database, scrape_continuously, scrape_choosen_data]\n",
    "\n",
    "        for field in stats:\n",
    "            stats_info.append(field)\n",
    "\n",
    "        return stats_info\n",
    "    \n",
    "\n",
    "class CheckConditions:\n",
    "\n",
    "    def __init__(self, soup: BeautifulSoup, start_page: int) -> None:\n",
    "        self.soup = soup\n",
    "        self.start_page = start_page\n",
    "\n",
    "    def check_if_last_page_nowe(self) -> bool:\n",
    "        \"\"\"Checking 'nowe' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            if self.start_page == 335:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Checking if page in nowe category is last failed:\\\n",
    "                            {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "        \n",
    "\n",
    "    def check_if_last_page_search(self) -> bool:\n",
    "        \"\"\"Checking 'search' category to verify if the scraped page is the last one.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-xl text--b\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"text--color-charcoalTint size--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t okazje\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number > 0:\n",
    "                logging.warning(\"No more pages to scrape.\")\n",
    "                return False\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "\n",
    "    def check_if_no_items_found(self) -> bool:\n",
    "        \"\"\"Checking if searched item was found.\"\"\"\n",
    "        try:\n",
    "            searched_ending_string = self.soup.find_all('h3', {\"class\":\"size--all-xl text--b\"})[0].get_text()\n",
    "            searched_articles_number = self.soup.find_all('span', {\"class\":\"text--color-charcoalTint size--all-m\"})[0].get_text()\n",
    "            searched_articles_number = int(searched_articles_number.replace(\" \",\"\").strip(\"\\n\\t okazje\"))\n",
    "            if searched_ending_string.startswith(\"Ups\") and searched_articles_number == 0:\n",
    "                logging.warning(\"The searched item was not found.\")\n",
    "                return False\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "\n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "\n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "\n",
    "class GetItemName:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            name = self.article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['title'].lower()\n",
    "            return name\n",
    "        except IndexError as e:\n",
    "            name = self.article.find_all(attrs={'class': \"thread-link linkPlain thread-title--list js-thread-title\"})[0]['title'].lower()\n",
    "            return name\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item name failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemId:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> int:\n",
    "        try:\n",
    "            item_id = self.article.get(\"id\")\n",
    "            item_id = item_id.strip('thread_')\n",
    "            item_id = int(item_id)\n",
    "            return item_id\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item id failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemDiscountPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            discount_price = self.article.find_all(\"span\", {'class': \"threadItemCard-price text--b thread-price size--all-l size--fromW3-xl space--mr-0\"})\n",
    "            if len(discount_price) == 0:\n",
    "                discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl text--color-greyShade\"})\n",
    "\n",
    "            if len(discount_price) > 0:\n",
    "                discount_price = discount_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ','')\n",
    "                if discount_price == \"ZADARMO\":\n",
    "                    discount_price = float(0)\n",
    "                else:\n",
    "                    discount_price = float(discount_price)                \n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                discount_price = \"NA\"\n",
    "            return discount_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item discount price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemRegularPrice:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            regular_price = self.article.find_all(\"span\", {'class': \"mute--text text--lineThrough space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            if len(regular_price) > 0:\n",
    "                regular_price = float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.').replace(' ',''))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                regular_price = \"NA\"\n",
    "            return regular_price\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item regular price failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemPercentageDiscount:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> Union[float, str]:\n",
    "        try:\n",
    "            percentage_discount = self.article.find_all(\"span\", {'class': \"text--color-charcoal space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            if len(percentage_discount) > 0:\n",
    "                percentage_discount = float(percentage_discount[0].get_text().strip('%'))\n",
    "            else:\n",
    "                \"\"\"The attribute does not exist or the class name is invalid.\"\"\"\n",
    "                percentage_discount = \"NA\"\n",
    "            return percentage_discount\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item percentage discount failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemUrl:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            item_url = self.article.find_all('a', {\"class\":\"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0]['href']\n",
    "            return item_url\n",
    "        except IndexError as e:\n",
    "            item_url = 'NA - Login Required.'\n",
    "            return item_url\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item url failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "class GetItemAddedDate:\n",
    "\n",
    "    def __init__(self, article: Tag) -> None:\n",
    "        self.article = article\n",
    "\n",
    "    def get_raw_data(self) -> List[str]:\n",
    "        try:\n",
    "            date_tag = self.article.find_all('div', {\"class\":\"flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "            raw_string_list = date_tag[0].get_text(strip=True, separator='_').split('_')\n",
    "            return raw_string_list\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Getting item added date failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def get_data(self) -> str:\n",
    "        try:\n",
    "            filtered_list = self.clean_list()\n",
    "            filtered_list = self.check_missing_date()\n",
    "            date_string_likely = filtered_list[0]\n",
    "            if date_string_likely == \"NA\":\n",
    "                url_with_item = GetItemUrl.get_data(self)\n",
    "                soup = self.scrape_page(url_with_item)\n",
    "                prepared_data = self.fill_missing_date(soup)\n",
    "                return prepared_data\n",
    "            else:\n",
    "                stripped_date_string_likely = self.strip_date_string(date_string_likely)\n",
    "                prepared_data = self.date_format_conversion(stripped_date_string_likely)\n",
    "            return prepared_data\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name: {e}\")\n",
    "\n",
    "    def strip_date_string(self, date_string_likely: str) -> str:\n",
    "        try:\n",
    "            if date_string_likely.startswith(\"Zaktualizowano \") and date_string_likely.endswith(\" temu\"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                stripped_date_string_likely = stripped_date_string_likely.rstrip(\" temu\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.endswith(\"Lokalnie\"):\n",
    "                stripped_date_string_likely = date_string_likely.rstrip(\"Lokalnie\")\n",
    "                return stripped_date_string_likely\n",
    "            elif date_string_likely.startswith(\"Zaktualizowano \"):\n",
    "                stripped_date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \")\n",
    "                return stripped_date_string_likely\n",
    "            else:\n",
    "                stripped_date_string_likely = date_string_likely\n",
    "                return stripped_date_string_likely\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Stripping date string failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def date_format_conversion(self, stripped_date_string_likely: str) -> str:\n",
    "        try:\n",
    "            if stripped_date_string_likely.endswith(('min', 'g', 's')):\n",
    "                prepared_data = date.today().strftime(\"%Y-%m-%d\")\n",
    "                return prepared_data\n",
    "            elif stripped_date_string_likely.startswith(tuple(Months.keys())) and len(stripped_date_string_likely) < 8:\n",
    "                if len(stripped_date_string_likely[4:]) == 3:\n",
    "                    day = stripped_date_string_likely[4:6]\n",
    "                else:\n",
    "                    day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = str(date.today().year)\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_1, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:6]\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[8:13]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(OLD_DATES_DATA_PATTERN_2, stripped_date_string_likely)):\n",
    "                day = stripped_date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[stripped_date_string_likely[0:3]].value\n",
    "                year = stripped_date_string_likely[7:12]\n",
    "                prepared_data = '-'.join([year, month, day])\n",
    "                return prepared_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Data format conversion failed: {e}\\n Tracking: {traceback.format_exc()}\")\n",
    "\n",
    "    def clean_list(self) -> List[str]:\n",
    "        raw_string_list = self.get_raw_data()\n",
    "        items_to_remove = list()\n",
    "        filtered_list = list()\n",
    "        try:\n",
    "            for string in raw_string_list:\n",
    "                if \"/\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if \":\" in string:\n",
    "                    items_to_remove.append(string)\n",
    "                if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\", \"Stacjonarnie\"]:\n",
    "                    items_to_remove.append(string)\n",
    "                if string.startswith(\"Wysyłka\"):\n",
    "                    items_to_remove.append(string)\n",
    "            counts = Counter(items_to_remove)\n",
    "            for string in raw_string_list:\n",
    "                if counts[string]:\n",
    "                    counts[string] -= 1\n",
    "                else:\n",
    "                    filtered_list.append(string)\n",
    "            return filtered_list\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "    def check_missing_date(self) -> List[str]:\n",
    "        filtered_list = self.clean_list()\n",
    "        try:\n",
    "            if len(filtered_list) == 0:\n",
    "                filtered_list.append(\"NA\")\n",
    "                return filtered_list\n",
    "            else:\n",
    "                return filtered_list\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "    def fill_missing_date(self, soup) -> str:\n",
    "        try:\n",
    "            date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "            filtered_list = date_string.split()\n",
    "            day_string = filtered_list[0]\n",
    "            month_string = filtered_list[1]\n",
    "            year_string = filtered_list[2].strip(',')\n",
    "            if len(day_string[0]) == 2:\n",
    "                day = day_string\n",
    "            else:\n",
    "                day = day_string.zfill(2)\n",
    "            month = Months.__members__[month_string].value\n",
    "            year = year_string\n",
    "            prepared_data = '-'.join([year, month, day])\n",
    "            return prepared_data\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "    def scrape_page(self, url_with_item: str, driver: webdriver=None) -> BeautifulSoup:\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            if driver is None:\n",
    "                driver = webdriver.Chrome(options=options) #for local  \n",
    "                #driver = webdriver.Remote(command_executor=f'http://{CustomEnvironment.get_selenium_container_name()}:4444/wd/hub', options=options) #for docker \n",
    "            driver.get(url_with_item)\n",
    "            time.sleep(0.7)\n",
    "            page_with_item = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(page_with_item, 'html5lib')\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            raise ConnectionError(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            raise MissingSchema(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            raise HTTPError(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            raise ReadTimeout(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "category_type = \"gorące\"\n",
    "articles_to_retrieve = 10\n",
    "searched_article = \"S21\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output = ScrapePage(category_type=category_type, articles_to_retrieve=articles_to_retrieve)\n",
    "all_items = output.get_items_details_depending_on_the_function()\n",
    "\n",
    "\n",
    "for i in all_items:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROMOCJA', 'Samsung', 'Galaxy', 'S23', 'z', 'kodem', 'APP5']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from django.db.models import Q\n",
    "\n",
    "import pandas as pd\n",
    "from source.pepper_app.constans import DATA_HEADER\n",
    "\n",
    "\n",
    "data = [1, 'PROMOCJA Samsung Galaxy S23 z kodem APP5', 2500, 10, 2750, '2024-01-07', 'https://www.pepper.pl/promocje/samsungs23']\n",
    "item_df = pd.DataFrame([data], columns=DATA_HEADER)\n",
    "\n",
    "\n",
    "def conditions(item_df):\n",
    "    # Initialize conditions with a default condition\n",
    "    conditions = Q()\n",
    "\n",
    "    for _, row in item_df.iterrows():\n",
    "        article_name = row[\"name\"]\n",
    "        list_of_words_article_name = article_name.split()\n",
    "\n",
    "        # Create a new condition for each word in the article name\n",
    "    for word in list_of_words_article_name:\n",
    "        conditions &= Q(desired_article__icontains=word)\n",
    "\n",
    "    # Filter UserRequest objects based on the conditions and order by request_time\n",
    "    #results = UserRequest.objects.filter(conditions).order_by('request_time')\n",
    "\n",
    "    return list_of_words_article_name\n",
    "\n",
    "\n",
    "print(conditions(item_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
