{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import ConnectionError, HTTPError, MissingSchema, ReadTimeout\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta, date\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from enum import Enum, IntEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important! in the pipeline\n",
    "\n",
    "action_type = \"/nowe?page=\"\n",
    "start_page = 25\n",
    "website_url = \"https://www.pepper.pl\"\n",
    "articles_to_retrieve = 61\n",
    "\n",
    "\n",
    "\n",
    "class ScrapWebpage:\n",
    "\n",
    "    def __init__(self, website_url, action_type, articles_to_retrieve, start_page=1):\n",
    "        self.website_url = website_url\n",
    "        self.action_type = action_type\n",
    "        self.articles_to_retrieve = articles_to_retrieve\n",
    "        self.start_page = start_page\n",
    "\n",
    "\n",
    "    def scrap_data(self):\n",
    "\n",
    "        try:\n",
    "            url_to_scrap = self.website_url + self.action_type + str(self.start_page)\n",
    "            driver = webdriver.Chrome('./chromedriver') \n",
    "            driver.set_window_size(1400,1000)\n",
    "            driver.get(url_to_scrap) \n",
    "            time.sleep(0.7)\n",
    "            page = driver.page_source\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            print(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            print(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            print(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            print(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "\n",
    "    def infinite_scroll_handling(self):\n",
    "\n",
    "        try:\n",
    "            flag = True\n",
    "            retrived_articles = list()\n",
    "\n",
    "            while flag:\n",
    "                soup = self.scrap_data()\n",
    "                articles = soup.find_all('article')\n",
    "                retrived_articles += articles\n",
    "\n",
    "                if len(retrived_articles) >= self.articles_to_retrieve:\n",
    "                    flag = False\n",
    "                    return retrived_articles[:self.articles_to_retrieve]\n",
    "\n",
    "                self.start_page += 1\n",
    "\n",
    "        except IndexError as e:\n",
    "            raise IndexError(\"There aren't that many articles, try retrieve lower quantity of articles\")\n",
    "\n",
    "        \n",
    "    \n",
    "    def get_items_details(self):\n",
    "\n",
    "\n",
    "        retrived_articles = self.infinite_scroll_handling()\n",
    "\n",
    "        all_items = list()\n",
    "\n",
    "        #print(retrived_articles)\n",
    "\n",
    "        for article in retrived_articles:\n",
    "            item = list()\n",
    "            item.append(GetItemId(article).get_data())\n",
    "            item.append(GetItemName(article).get_data())\n",
    "            item.append(GetItemDiscountPrice(article).get_data())\n",
    "            item.append(GetItemPercentageDiscount(article).get_data())\n",
    "            item.append(GetItemRegularPrice(article).get_data())\n",
    "            #item.append(GetItemAddedDate(article).get_data())\n",
    "            item.append(GetItemUrl(article).get_data())\n",
    "            all_items.append(item)\n",
    "\n",
    "        return all_items\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "#output = ScrapWebpage(website_url, action_type, articles_to_retrieve)\n",
    "\n",
    "#print(output.get_items_details())\n",
    "#retrived = output.infinite_scroll_handling()\n",
    "#print(retrived)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important! in the pipeline\n",
    "\n",
    "\n",
    "class GetItemName:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            name = self.article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})\n",
    "            name = name[0].get_text()\n",
    "            return name\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_name): {e}\")\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_name): {e}\")\n",
    "\n",
    "\n",
    "class GetItemId:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self):\n",
    "        try:\n",
    "            item_id = self.article[\"id\"]\n",
    "            item_id = item_id.strip('thread_')\n",
    "            return item_id\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_id): {e}\")\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_id): {e}\")\n",
    "\n",
    "\n",
    "class GetItemDiscountPrice:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "            discount_price = float(discount_price[0].get_text().strip('zł').replace('.','').replace(',','.'))\n",
    "            return discount_price\n",
    "        except IndexError as e:\n",
    "            return \"NA\"\n",
    "        except ValueError as e:\n",
    "            return \"NA\"\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_discount_price): {e}\")\n",
    "\n",
    "\n",
    "\n",
    "class GetItemRegularPrice:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            regular_price = self.article.find_all(attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"})\n",
    "            regular_price = float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.'))\n",
    "            return regular_price \n",
    "        except IndexError as e:\n",
    "            return \"NA\"\n",
    "        except ValueError as e:\n",
    "            return \"NA\"\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_regular_price): {e}\")\n",
    "\n",
    "\n",
    "class GetItemPercentageDiscount:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            percentage_discount = self.article.find_all(attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            percentage_discount = float(percentage_discount[0].get_text().strip('%'))\n",
    "            return percentage_discount\n",
    "        except IndexError as e:\n",
    "            return \"NA\"\n",
    "        except ValueError as e:\n",
    "            return \"NA\"\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_percentage_discount): {e}\")\n",
    "\n",
    "\n",
    "class GetItemUrl:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            item_url = self.article.find_all('a', href=True, text=True)\n",
    "            item_url = item_url[0]['href']\n",
    "            return item_url\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_url): {e}\")\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_url): {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_235/4038450164.py:23: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('./chromedriver')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GetItemAddedDate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_235/3395377114.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfinite_scroll_handling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetItemAddedDate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#print(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GetItemAddedDate' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "action_type = \"/nowe?page=\"\n",
    "start_page = 25\n",
    "website_url = \"https://www.pepper.pl\"\n",
    "articles_to_retrieve = 61\n",
    "\n",
    "\n",
    "data = ScrapWebpage(website_url, action_type, articles_to_retrieve)\n",
    "\n",
    "article = data.infinite_scroll_handling()[60]\n",
    "\n",
    "output = GetItemAddedDate(article).get_data()\n",
    "#print(output)\n",
    "\n",
    "out = GetItemAddedDate(article)\n",
    "date_tag = out.get_data()\n",
    "\n",
    "\n",
    "print(date_tag)\n",
    "#d = out.find_true_date(date_tag)\n",
    "#print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important! in the pipeline\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "    \n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "    \n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "class GetItemAddedDate:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self):\n",
    "        try:\n",
    "            date_tag = self.article.find_all(attrs={'class': \"metaRibbon lbox--v-1 boxAlign-ai--all-c overflow--wrap-off space--l-3 text--color-greyShade\"})\n",
    "            #date = self.find_true_date(date_tag)\n",
    "            return date_tag\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_url): {e}\")\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_url): {e}\")\n",
    "\n",
    "    def find_true_date(self, date_tag):\n",
    "\n",
    "\n",
    "        try:\n",
    "            true_data = self.first_index_date_searching(date_tag)\n",
    "            print(\"1\")\n",
    "            return true_data\n",
    "        except Exception:\n",
    "            print(\"bad1\")\n",
    "            try:\n",
    "                true_data = self.second_index_date_searching(date_tag)\n",
    "                print(\"2\")\n",
    "                return true_data\n",
    "            except Exception:\n",
    "                print(\"bad2\")\n",
    "                try:\n",
    "                    true_data = self.third_index_date_searching(date_tag)\n",
    "                    print(\"3\")\n",
    "                    return true_data\n",
    "                except Exception:\n",
    "                    print(\"bad3\")\n",
    "\n",
    "\n",
    "    def data_format_conversion(self, date_string_likely):\n",
    "\n",
    "        old_dates_data_pattern = \"[A-Za-z]+\\s\\d\\d\\.\\s[0-9]+\"\n",
    "\n",
    "        try:\n",
    "            if date_string_likely.startswith(\"Zaktualizowano\"):\n",
    "                date_string_likely = date_string_likely.lstrip(\"Zaktualizowano \") \n",
    "            elif date_string_likely.endswith(\"Lokalnie\"):\n",
    "                date_string_likely = date_string_likely.rstrip(\"Lokalnie\") \n",
    "        except Exception:\n",
    "            return date_string_likely\n",
    "\n",
    "        try:\n",
    "            if date_string_likely.endswith(('min', 'g', 's', 'temu')):\n",
    "                prepared_data = date.today().strftime(\"%d-%m-%Y\")\n",
    "                return prepared_data\n",
    "            elif date_string_likely.startswith(tuple(Months.keys())) and len(date_tag) < 8:      \n",
    "                if len(date_string_likely[4:]) == 3:\n",
    "                    day = date_string_likely[4:6]\n",
    "                else:\n",
    "                    day = date_string_likely[4:5].zfill(2)\n",
    "                month = Months.__members__[date_string_likely[0:3]].value\n",
    "                year = str(date.today().year)\n",
    "                prepared_data = '-'.join([str(day), month, year])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(old_dates_data_pattern, date_string_likely)):\n",
    "                day = date_string_likely[4:6]\n",
    "                month = Months.__members__[date_string_likely[0:3]].value\n",
    "                year = date_string_likely[8:13]\n",
    "                prepared_data = '-'.join([day, month, year])\n",
    "                return prepared_data\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Invalid name of the month {e}\")\n",
    "\n",
    "\n",
    "    def first_index_date_searching(self, date_tag):\n",
    "\n",
    "        output_data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "        \n",
    "        #date = self.data_format_conversion(date_string_likely)\n",
    "\n",
    "        try:\n",
    "            date_string_likely = date_tag[0].get_text()\n",
    "            formatted_data = self.data_format_conversion(date_string_likely)\n",
    "            if bool(re.search(output_data_pattern, formatted_data)):\n",
    "                return formatted_data\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def second_index_date_searching(self, date_tag):\n",
    "        \n",
    "        output_data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "        \n",
    "        #date = self.data_format_conversion(date_string_likely)\n",
    "\n",
    "        try:\n",
    "            date_string_likely = date_tag[1].get_text()\n",
    "            formatted_data = self.data_format_conversion(date_string_likely)\n",
    "            if bool(re.search(output_data_pattern, formatted_data)):\n",
    "                return formatted_data\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def third_index_date_searching(self, date_tag):\n",
    "        \n",
    "        output_data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "        \n",
    "        #date = self.data_format_conversion(date_string_likely)\n",
    "\n",
    "        try:\n",
    "            date_string_likely = date_tag[2].get_text()\n",
    "            formatted_data = self.data_format_conversion(date_string_likely)\n",
    "            if bool(re.search(output_data_pattern, formatted_data)):\n",
    "                return formatted_data\n",
    "            else:\n",
    "                raise Exception\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_278/2052179023.py:23: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('./chromedriver')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "action_type = \"/nowe?page=\"\n",
    "start_page = 1\n",
    "website_url = \"https://www.pepper.pl\"\n",
    "articles_to_retrieve = 200\n",
    "\n",
    "\n",
    "data = ScrapWebpage(website_url, action_type, articles_to_retrieve, start_page)\n",
    "\n",
    "articles = data.infinite_scroll_handling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14 min\n"
     ]
    }
   ],
   "source": [
    "article_67 = articles[67]\n",
    "article_2 = articles[2]\n",
    "\n",
    "n = 1 \n",
    "from collections import Counter \n",
    "\n",
    "def get_strings_list_to_filter(article):\n",
    "\n",
    "    try:\n",
    "        date_class = article.find_all('div', {\"class\":\"size--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "        all_strings_list = date_class[0].get_text(strip=True, separator='_').split('_')\n",
    "\n",
    "        return all_strings_list\n",
    "    except TypeError as e:\n",
    "        raise TypeError(f\"Invalid html class name (item_url): {e}\")\n",
    "\n",
    "def clean_list(all_strings_list):\n",
    "\n",
    "    items_to_remove = list()\n",
    "    counts = Counter(items_to_remove)\n",
    "    filtered_list = list()\n",
    "\n",
    "    try:\n",
    "        for string in all_strings_list:\n",
    "            if \"/\" in string:\n",
    "                items_to_remove.append(string)\n",
    "            if string in [\"Jutro\", \"DZISIAJ\", \"Lokalnie\"]:\n",
    "                items_to_remove.append(string)\n",
    "            if string.startswith(\"Wysyłka\"):\n",
    "                items_to_remove.append(string)\n",
    "        \n",
    "        counts = Counter(items_to_remove)\n",
    "\n",
    "        for string in all_strings_list:\n",
    "            if counts[string]:\n",
    "                counts[string] -= 1\n",
    "            else:\n",
    "                filtered_list.append(string)\n",
    "        \n",
    "        return filtered_list\n",
    "\n",
    "    except TypeError as e:\n",
    "        raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "\n",
    "def check_missing_date(filtered_list):\n",
    "    \n",
    "    try:\n",
    "        if len(filtered_list) == 0:\n",
    "            filtered_list.append(\"NA\")\n",
    "            return filtered_list\n",
    "        else:\n",
    "            return filtered_list\n",
    "    except TypeError as e:\n",
    "        raise TypeError(f\"Input data must be a list: {e}\")\n",
    "\n",
    "def get_from_date_tag(date_tag):\n",
    "\n",
    "    try:\n",
    "        date_class = date_tag[0].get_text()\n",
    "        all_strings_list = date_class[0].get_text(strip=True, separator='_').split('_')\n",
    "\n",
    "        return all_strings_list\n",
    "    except TypeError as e:\n",
    "        raise TypeError(f\"Invalid html class name (item_url): {e}\")\n",
    "\n",
    "\n",
    "\n",
    "previous_date = list()\n",
    "for article in articles:\n",
    "    out = GetItemAddedDate(article)\n",
    "    date_tag = out.get_data()\n",
    "\n",
    "    name = article.find_all('a', {\"class\":\"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})[0].get_text()\n",
    "    #all_strings_list = get_from_date_tag(date_tag)\n",
    "    all_strings_list = get_strings_list_to_filter(article)\n",
    "    filtered_list = clean_list(all_strings_list)\n",
    "    filtered_list = check_missing_date(filtered_list)\n",
    "\n",
    "   \n",
    "\n",
    "    #print(name.title())\n",
    "    #print(filtered_list)\n",
    "    #print(date_tag)\n",
    "    #print(f\"len: {len(spans)}\")\n",
    "    #print(f\"number: {n}\")\n",
    "    n += 1\n",
    "\n",
    "all_strings_67 = get_strings_list_to_filter(article_67)\n",
    "#print(article_67)\n",
    "\n",
    "date_class_67 = article_67.find('div', {\"class\":\"size--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "date_class_2 = article_2.find('div', {\"class\":\"size--all-s flex boxAlign-jc--all-fe boxAlign-ai--all-c flex--grow-1 overflow--hidden\"})\n",
    "\n",
    "#print(date_class_67)\n",
    "#print(date_class_2)\n",
    "\n",
    "\n",
    "all_strings_list_67 = date_class_67.select('span')[0].get_text()\n",
    "all_strings_list_2 = date_class_2.select('span')[1].get_text()\n",
    "\n",
    "print(all_strings_list_67)\n",
    "print(all_strings_list_2)\n",
    "\n",
    "#print(all_strings_1)\n",
    "\n",
    "#print(date_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/06/2023\n",
      "27/06/2023\n",
      "none\n",
      "1\n",
      "31/07/2023\n",
      "31/07/2023\n",
      "none\n",
      "2\n",
      "14 min\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_278/2461308189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mspans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"span\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "#out = GetItemAddedDate(article)\n",
    "#date_tag = out.get_data()\n",
    "#print(date_tag)\n",
    "\n",
    "output_data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "number = 1\n",
    "\n",
    "for a in articles: \n",
    "    out = GetItemAddedDate(a)\n",
    "    date_tag = out.get_data()\n",
    "    spans = date_tag[0].find_all(\"span\")\n",
    "    print(spans[0].get_text())\n",
    "    print(spans[1].get_text())\n",
    "    try:\n",
    "        print(spans[2].get_text())\n",
    "    except:\n",
    "        print(\"none\")\n",
    "    #print(date_tag)\n",
    "    #print(date_tag[0].get_text())\n",
    "    print(number)\n",
    "    number += 1\n",
    "    \"\"\"#date = out.first_index_date_searching(date_tag)\n",
    "\n",
    "    try:\n",
    "        if bool(re.search(output_data_pattern, out.first_index_date_searching(date_tag))): \n",
    "            print(f\"{out.first_index_date_searching(date_tag)} + {number}\" )\n",
    "    except Exception:\n",
    "        try:\n",
    "            if bool(re.search(output_data_pattern, out.second_index_date_searching(date_tag))):\n",
    "                print(f\"{out.second_index_date_searching(date_tag)} + {number}\" )\n",
    "        except Exception:\n",
    "            try: \n",
    "                if bool(re.search(output_data_pattern, out.third_index_date_searching(date_tag))):\n",
    "                    print(f\"{out.third_index_date_searching(date_tag)} + {number}\" )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    number += 1\"\"\"\n",
    "\n",
    "    #print(f\"{out.first_index_date_searching(date_tag)} + {number}\" )\n",
    "    #print(f\"{date_tag} + {number}\" )\n",
    "\"\"\"\n",
    "print(out.first_index_date_searching(date_tag))\n",
    "print(out.second_index_date_searching(date_tag))\n",
    "print(out.third_index_date_searching(date_tag))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19484/995706346.py:34: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('./chromedriver')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29-06-2023\n"
     ]
    }
   ],
   "source": [
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "    \n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "    \n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "\n",
    "url_with_item = 'https://www.pepper.pl/promocje/swiezy-filet-z-piersi-z-kurczaka-pakowany-prozniowo-mega-paka-1kg-1399-z-karta-692146'\n",
    "driver = webdriver.Chrome('./chromedriver')\n",
    "\n",
    "driver.get(url_with_item)\n",
    "time.sleep(0.7)\n",
    "page_with_item = driver.page_source\n",
    "soup = BeautifulSoup(page_with_item, 'html.parser')\n",
    "date_string = soup.find_all('div', {\"class\":\"space--mv-3\"})[0].find('span')['title']\n",
    "date_string_to_list = date_string.split()\n",
    "if len(date_string_to_list[0]) == 2:\n",
    "    day = date_string_to_list[0]\n",
    "else:\n",
    "    day = date_string_to_list[0].zfill(2)\n",
    "\n",
    "month = Months.__members__[date_string_to_list[1]].value\n",
    "year = date_string_to_list[2][:4]\n",
    "prepared_data = '-'.join([str(day), month, year])\n",
    "#span = date_class\n",
    "print(prepared_data)\n",
    "#raw_string_list = date_class.title()#[0].get_text(strip=True, separator='_').split('_')\n",
    "#print(span)\n",
    "#date_string_likely = raw_string_list[0]\n",
    "#print(date_string_likely)\n",
    "#prepared_data = data_format_conversion(date_string_likely)\n",
    "\n",
    "#print(prepared_data)\n",
    "#filtered_list.append(\"NA\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e6a7b6723b894902baae12ce751b40f53dc6201cc32eda169f42fc9fc428a6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
