{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import ConnectionError, HTTPError, MissingSchema, ReadTimeout\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta, date\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from enum import Enum, IntEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_all_content_infinite_scroll(scroll_pause_time=1):\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3) #time to load a page\n",
    "    \n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "    i = 1\n",
    "\n",
    "    #infinite scroll loop\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "        if (screen_height) * i > scroll_height:\n",
    "            break\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    return soup.main.prettify()\n",
    "\n",
    "\n",
    "print(get_all_content_infinite_scroll(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_all_article_id():\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3) #time to load a page\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    articles = soup.find_all('article')\n",
    "\n",
    "    list_of_article_id = [x[\"id\"] for x in articles] \n",
    "    \n",
    "    return list_of_article_id\n",
    "\n",
    "print(get_all_article_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_all_article_id_many_subpages(start_page_number=1, end_page_number=3):\n",
    "\n",
    "    page_number = 1\n",
    "    list_of_article_id = []\n",
    "    all_articles = []\n",
    "    \n",
    "\n",
    "    try:\n",
    "        while page_number != end_page_number:\n",
    "\n",
    "            page_url = f\"https://www.pepper.pl/nowe?page={page_number}\"\n",
    "\n",
    "            driver = webdriver.Chrome('./chromedriver') \n",
    "            driver.get(page_url) \n",
    "            time.sleep(1) #time to load a page\n",
    "\n",
    "            page = driver.page_source\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "\n",
    "            articles_from_page = soup.find_all('article')\n",
    "            #all_articles.append(articles_from_page)\n",
    "            print(articles_from_page)\n",
    "\n",
    "            print(len(all_articles))\n",
    "            print(len(soup))\n",
    "\n",
    "\n",
    "            for i in articles:\n",
    "                if i[\"id\"] not in list_of_article_id:\n",
    "                    list_of_article_id.append(i[\"id\"])\n",
    "            \n",
    "            page_number += 1\n",
    "    except:\n",
    "        print(\"Exception occured\")\n",
    "\n",
    "    return list_of_article_id\n",
    "\n",
    "print(get_all_article_id_many_subpages(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_item_name_like(item):\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3) #time to load a page\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    list_all_items_info = soup.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})\n",
    "\n",
    "    list_all_items_names = [x.get_text() for x in list_all_items_info]\n",
    "    \n",
    "    matches = [x for x in list_all_items_names if item.lower() in x.lower()]\n",
    "\n",
    "    print(matches)\n",
    "\n",
    "\n",
    "get_item_name_like(\"Monitor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_content_item_tag():\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3)\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    all_tags_content = soup.find_all(\"article\")\n",
    "    \n",
    "    for i in all_tags_content:\n",
    "        item_name = i.find_all(\"cept-tt thread-link linkPlain thread-title--list js-thread-title\")\n",
    "        discount_price = i.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "        regular_price = i.find_all(attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"})\n",
    "        percentage_discount = i.find_all(attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "        item_link = i.find_all('a', href=True, text=True)\n",
    "        \n",
    "        #(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"}, href=True)\n",
    "        \n",
    "\n",
    "        article_id = i[\"id\"]\n",
    "        print(item_name)\n",
    "        try:    \n",
    "            print(float(discount_price[0].get_text().strip('zł').replace('.','').replace(',','.')))\n",
    "        except: \n",
    "            print(\"N/A\")\n",
    "        try:\n",
    "            print(float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.')))\n",
    "        except:\n",
    "            print(\"N/A\")\n",
    "        try:\n",
    "            print(float(percentage_discount[0].get_text().strip('%')))\n",
    "        except:\n",
    "            print(\"N/A\")\n",
    "        print(item_link[0]['href'])\n",
    "        print(article_id.strip('thread_'))\n",
    "\n",
    "\n",
    "    #print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "get_content_item_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "    \n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "    \n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "def data_conversion(publication_date):\n",
    "\n",
    "    try:\n",
    "        if publication_date.endswith(('min', 'g', 's', 'temu')):\n",
    "            prepared_data = date.today().strftime(\"%d-%m-%Y\")\n",
    "            return prepared_data\n",
    "        elif publication_date.startswith(tuple(Months.keys())) and len(publication_date) < 8:      \n",
    "            if len(publication_date[4:]) == 3:\n",
    "                day = publication_date[4:6]\n",
    "            else:\n",
    "                day = publication_date[4:5].zfill(2)\n",
    "            month = Months.__members__[publication_date[0:3]].value\n",
    "            year = str(date.today().year)\n",
    "            prepared_data = '-'.join([str(day), month, year])\n",
    "            return prepared_data\n",
    "        else:\n",
    "            day = publication_date[4:6]\n",
    "            month = Months.__members__[publication_date[0:3]].value\n",
    "            year = publication_date[7:13]\n",
    "            prepared_data = '-'.join([day, month, year])\n",
    "            return prepared_data\n",
    "    except KeyError as e:\n",
    "        return f'Invalid name of the month: {e}'\n",
    "    except:\n",
    "        print('error')\n",
    "\n",
    "\n",
    "\n",
    "def get_article_data():\n",
    "\n",
    "    data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe?page=25\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3)\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    all_tags_content = soup.find_all(lambda tag:tag.name==\"article\")\n",
    "\n",
    "    n=1\n",
    "\n",
    "    for i in all_tags_content:\n",
    "        publication_date_tag = i.find_all(attrs={'class': \"hide--fromW3\"})\n",
    "        item_name = i.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})\n",
    "        print(f\"item number {n}\")\n",
    "        n+=1\n",
    "        print(item_name[0].get_text()) \n",
    "\n",
    "        print(publication_date_tag)\n",
    "        potentially_data_string_1 = publication_date_tag[0].get_text()\n",
    "        potentially_data_string_2 = publication_date_tag[1].get_text()\n",
    "        try:\n",
    "            formatted_data_1 = data_conversion(potentially_data_string_1)\n",
    "\n",
    "            if bool(re.search(data_pattern, formatted_data_1)):\n",
    "                print(data_conversion(potentially_data_string_1) + \" string 1 success\")\n",
    "            else:\n",
    "                raise Exception(\"didnt work\")\n",
    "        except:\n",
    "            try: \n",
    "                formatted_data_2 = data_conversion(potentially_data_string_2)\n",
    "\n",
    "                if bool(re.search(data_pattern, formatted_data_2)):\n",
    "                    print(data_conversion(potentially_data_string_2) + \" string 2 success\")\n",
    "                else:\n",
    "                    raise Exception(\"didnt work second position\")\n",
    "            except:\n",
    "                potentially_data_string_3 = publication_date_tag[2].get_text()\n",
    "                print(data_conversion(potentially_data_string_3) + \" string 3 success\")\n",
    " \n",
    "       \n",
    "\n",
    "print(get_article_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_277/2855117944.py:24: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('./chromedriver')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n",
      "error1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Index out of the range (item_url): list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_277/3617739050.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mitem_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mitem_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_url\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mitem_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_277/2855117944.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScrapWebpage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebsite_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticles_to_retrieve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_items_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_277/2855117944.py\u001b[0m in \u001b[0;36mget_items_details\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetItemRegularPrice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetItemAddedDate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetItemUrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mall_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_277/3617739050.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mitem_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Index out of the range (item_url): {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid html class name (item_url): {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Index out of the range (item_url): list index out of range"
     ]
    }
   ],
   "source": [
    "#important! in the pipeline\n",
    "\n",
    "action_type = \"/nowe?page=\"\n",
    "start_page = 25\n",
    "end_page = 25\n",
    "website_url = \"https://www.pepper.pl\"\n",
    "articles_to_retrieve = 61\n",
    "\n",
    "\n",
    "\n",
    "class ScrapWebpage:\n",
    "\n",
    "    def __init__(self, website_url, action_type, articles_to_retrieve, start_page=1):\n",
    "        self.website_url = website_url\n",
    "        self.action_type = action_type\n",
    "        self.articles_to_retrieve = articles_to_retrieve\n",
    "        self.start_page = start_page\n",
    "\n",
    "\n",
    "    def scrap_data(self):\n",
    "\n",
    "        try:\n",
    "            url_to_scrap = self.website_url + self.action_type + str(self.start_page)\n",
    "            driver = webdriver.Chrome('./chromedriver') \n",
    "            driver.get(url_to_scrap) \n",
    "            time.sleep(0.7)\n",
    "            page = driver.page_source\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            print(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            print(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            print(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            print(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "\n",
    "    def infinite_scroll_handling(self):\n",
    "\n",
    "        try:\n",
    "            flag = True\n",
    "            retrived_articles = list()\n",
    "\n",
    "            while flag:\n",
    "                soup = self.scrap_data()\n",
    "                articles = soup.find_all('article')\n",
    "                retrived_articles += articles\n",
    "\n",
    "                if len(retrived_articles) >= self.articles_to_retrieve:\n",
    "                    flag = False\n",
    "\n",
    "                self.start_page += 1\n",
    "\n",
    "            return retrived_articles[:self.articles_to_retrieve]\n",
    "        except IndexError as e:\n",
    "            raise IndexError(\"There aren't that many articles, try retrieve lower quantity of articles\")\n",
    "\n",
    "        \n",
    "    \n",
    "    def get_items_details(self):\n",
    "\n",
    "\n",
    "        retrived_articles = self.infinite_scroll_handling()\n",
    "\n",
    "        all_items = list()\n",
    "\n",
    "        for article in retrived_articles:\n",
    "            item = list()\n",
    "            item.append(GetItemId(article).get_data())\n",
    "            item.append(GetItemName(article).get_data())\n",
    "            item.append(GetItemDiscountPrice(article).get_data())\n",
    "            item.append(GetItemPercentageDiscount(article).get_data())\n",
    "            item.append(GetItemRegularPrice(article).get_data())\n",
    "            item.append(GetItemAddedDate(article).get_data())\n",
    "            item.append(GetItemUrl(article).get_data())\n",
    "            all_items.append(item)\n",
    "\n",
    "        return all_items\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "output = ScrapWebpage(website_url, action_type, articles_to_retrieve)\n",
    "output.get_items_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important! in the pipeline\n",
    "\n",
    "\n",
    "class GetItemName:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            name = self.article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})\n",
    "            name = name[0].get_text()\n",
    "            return name\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_name): {e}\")\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_name): {e}\")\n",
    "\n",
    "\n",
    "class GetItemId:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self):\n",
    "        try:\n",
    "            item_id = self.article[\"id\"]\n",
    "            item_id = item_id.strip('thread_')\n",
    "            return item_id\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_id): {e}\")\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_id): {e}\")\n",
    "\n",
    "\n",
    "class GetItemDiscountPrice:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "            discount_price = float(discount_price[0].get_text().strip('zł').replace('.','').replace(',','.'))\n",
    "            return discount_price\n",
    "        except IndexError as e:\n",
    "            return \"NA\"\n",
    "        except ValueError as e:\n",
    "            return \"NA\"\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_discount_price): {e}\")\n",
    "\n",
    "\n",
    "\n",
    "class GetItemRegularPrice:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            regular_price = self.article.find_all(attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"})\n",
    "            regular_price = float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.'))\n",
    "            return regular_price \n",
    "        except IndexError as e:\n",
    "            return \"NA\"\n",
    "        except ValueError as e:\n",
    "            return \"NA\"\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_regular_price): {e}\")\n",
    "\n",
    "\n",
    "class GetItemPercentageDiscount:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            percentage_discount = self.article.find_all(attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            percentage_discount = float(percentage_discount[0].get_text().strip('%'))\n",
    "            return percentage_discount\n",
    "        except IndexError as e:\n",
    "            return \"NA\"\n",
    "        except ValueError as e:\n",
    "            return \"NA\"\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_percentage_discount): {e}\")\n",
    "\n",
    "\n",
    "class GetItemUrl:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            item_url = self.article.find_all('a', href=True, text=True)\n",
    "            item_url = item_url[0]['href']\n",
    "            return item_url\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_url): {e}\")\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_url): {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important! in the pipeline\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "    \n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "    \n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "class GetItemAddedDate:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self):\n",
    "        try:\n",
    "            date_tag = self.article.find_all(attrs={'class': \"hide--fromW3\"})\n",
    "            date = self.find_true_date(date_tag)\n",
    "            return date\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_url): {e}\")\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_url): {e}\")\n",
    "\n",
    "    def find_true_date(self, date_tag):\n",
    "\n",
    "        flag = False\n",
    "\n",
    "        try:\n",
    "            self.first_index_date_searching()\n",
    "            if flag == True:\n",
    "                return date\n",
    "            elif flag == False:\n",
    "                self.second_index_date_searching()\n",
    "                if flag == True:\n",
    "                    return date\n",
    "                elif flag == False:\n",
    "                    self.third_index_date_searching()\n",
    "                    if flag == True:\n",
    "                        return date\n",
    "                    else:\n",
    "                        print('another error')\n",
    "\n",
    "        except:\n",
    "            print(\"error1\")\n",
    "\n",
    "    def data_format_conversion(self, date_tag):\n",
    "\n",
    "        old_dates_data_pattern = \"[A-Za-z]+\\s\\d\\d\\.\\s[0-9]+\"\n",
    "\n",
    "        try:\n",
    "            if date_tag.endswith(('min', 'g', 's', 'temu')):\n",
    "                prepared_data = date.today().strftime(\"%d-%m-%Y\")\n",
    "                return prepared_data\n",
    "            elif date_tag.startswith(tuple(Months.keys())) and len(date_tag) < 8:      \n",
    "                if len(scraped_publication_date[4:]) == 3:\n",
    "                    day = scraped_publication_date[4:6]\n",
    "                else:\n",
    "                    day = scraped_publication_date[4:5].zfill(2)\n",
    "                month = Months.__members__[date_tag[0:3]].value\n",
    "                year = str(date.today().year)\n",
    "                prepared_data = '-'.join([str(day), month, year])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(old_dates_data_pattern, date_tag)):\n",
    "                day = date_tag[4:6]\n",
    "                month = Months.__members__[date_tag[0:3]].value\n",
    "                year = date_tag[8:13]\n",
    "                prepared_data = '-'.join([day, month, year])\n",
    "                return prepared_data\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Invalid name of the month {e}\")\n",
    "\n",
    "\n",
    "    def first_index_date_searching(self, date_tag):\n",
    "\n",
    "        output_data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "        date_tag = date_tag[0].get_text()\n",
    "        date = data_conversion(date_tag)\n",
    "\n",
    "        flag == True\n",
    "        try:\n",
    "            formatted_data = self.data_format_conversion(date_tag)\n",
    "            if bool(re.search(output_data_pattern, formatted_data)):\n",
    "                return f\"{data_conversion(date_tag)} + ' string 1 success'\", flag\n",
    "        except Exception as e:\n",
    "            return flag == False\n",
    "            print(e)\n",
    "\n",
    "    def second_index_date_searching(self):\n",
    "        \n",
    "        output_data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "        date_tag = date_tag[1].get_text()\n",
    "        date = data_conversion(date_tag)\n",
    "\n",
    "        flag == True\n",
    "        try:\n",
    "            formatted_data = self.data_format_conversion(date_tag)\n",
    "            if bool(re.search(output_data_pattern, formatted_data)):\n",
    "                return f\"{data_conversion(date_tag)} + ' string 2 success'\", flag\n",
    "        except Exception as e:\n",
    "            return flag == False\n",
    "            print(e)\n",
    "\n",
    "    def third_index_date_searching(self):\n",
    "        \n",
    "        output_data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "        date_tag = date_tag[2].get_text()\n",
    "        date = data_conversion(date_tag)\n",
    "\n",
    "        flag == True\n",
    "        try:\n",
    "            formatted_data = self.data_format_conversion(date_tag)\n",
    "            if bool(re.search(output_data_pattern, formatted_data)):\n",
    "                return f\"{data_conversion(date_tag)} + ' string 3 success'\", flag\n",
    "        except Exception as e:\n",
    "            return flag == False\n",
    "            print(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e6a7b6723b894902baae12ce751b40f53dc6201cc32eda169f42fc9fc428a6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
