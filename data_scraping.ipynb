{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import ConnectionError, HTTPError, MissingSchema, ReadTimeout\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta, date\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from enum import Enum, IntEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_all_content_infinite_scroll(scroll_pause_time=1):\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3) #time to load a page\n",
    "    \n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "    i = 1\n",
    "\n",
    "    #infinite scroll loop\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "        if (screen_height) * i > scroll_height:\n",
    "            break\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    return soup.main.prettify()\n",
    "\n",
    "\n",
    "print(get_all_content_infinite_scroll(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_all_article_id():\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3) #time to load a page\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    articles = soup.find_all('article')\n",
    "\n",
    "    list_of_article_id = [x[\"id\"] for x in articles] \n",
    "    \n",
    "    return list_of_article_id\n",
    "\n",
    "print(get_all_article_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_all_article_id_many_subpages(start_page_number=1, end_page_number=3):\n",
    "\n",
    "    page_number = 1\n",
    "    list_of_article_id = []\n",
    "    all_articles = []\n",
    "    \n",
    "\n",
    "    try:\n",
    "        while page_number != end_page_number:\n",
    "\n",
    "            page_url = f\"https://www.pepper.pl/nowe?page={page_number}\"\n",
    "\n",
    "            driver = webdriver.Chrome('./chromedriver') \n",
    "            driver.get(page_url) \n",
    "            time.sleep(1) #time to load a page\n",
    "\n",
    "            page = driver.page_source\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "\n",
    "            articles_from_page = soup.find_all('article')\n",
    "            #all_articles.append(articles_from_page)\n",
    "            print(articles_from_page)\n",
    "\n",
    "            print(len(all_articles))\n",
    "            print(len(soup))\n",
    "\n",
    "\n",
    "            for i in articles:\n",
    "                if i[\"id\"] not in list_of_article_id:\n",
    "                    list_of_article_id.append(i[\"id\"])\n",
    "            \n",
    "            page_number += 1\n",
    "    except:\n",
    "        print(\"Exception occured\")\n",
    "\n",
    "    return list_of_article_id\n",
    "\n",
    "print(get_all_article_id_many_subpages(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_item_name_like(item):\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3) #time to load a page\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    list_all_items_info = soup.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})\n",
    "\n",
    "    list_all_items_names = [x.get_text() for x in list_all_items_info]\n",
    "    \n",
    "    matches = [x for x in list_all_items_names if item.lower() in x.lower()]\n",
    "\n",
    "    print(matches)\n",
    "\n",
    "\n",
    "get_item_name_like(\"Monitor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_content_item_tag():\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3)\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    all_tags_content = soup.find_all(\"article\")\n",
    "    \n",
    "    for i in all_tags_content:\n",
    "        item_name = i.find_all(\"cept-tt thread-link linkPlain thread-title--list js-thread-title\")\n",
    "        discount_price = i.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "        regular_price = i.find_all(attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"})\n",
    "        percentage_discount = i.find_all(attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "        item_link = i.find_all('a', href=True, text=True)\n",
    "        \n",
    "        #(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"}, href=True)\n",
    "        \n",
    "\n",
    "        article_id = i[\"id\"]\n",
    "        print(item_name)\n",
    "        try:    \n",
    "            print(float(discount_price[0].get_text().strip('zł').replace('.','').replace(',','.')))\n",
    "        except: \n",
    "            print(\"N/A\")\n",
    "        try:\n",
    "            print(float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.')))\n",
    "        except:\n",
    "            print(\"N/A\")\n",
    "        try:\n",
    "            print(float(percentage_discount[0].get_text().strip('%')))\n",
    "        except:\n",
    "            print(\"N/A\")\n",
    "        print(item_link[0]['href'])\n",
    "        print(article_id.strip('thread_'))\n",
    "\n",
    "\n",
    "    #print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "get_content_item_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "    \n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "    \n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "def data_conversion(publication_date):\n",
    "\n",
    "    try:\n",
    "        if publication_date.endswith(('min', 'g', 's', 'temu')):\n",
    "            prepared_data = date.today().strftime(\"%d-%m-%Y\")\n",
    "            return prepared_data\n",
    "        elif publication_date.startswith(tuple(Months.keys())) and len(publication_date) < 8:      \n",
    "            if len(publication_date[4:]) == 3:\n",
    "                day = publication_date[4:6]\n",
    "            else:\n",
    "                day = publication_date[4:5].zfill(2)\n",
    "            month = Months.__members__[publication_date[0:3]].value\n",
    "            year = str(date.today().year)\n",
    "            prepared_data = '-'.join([str(day), month, year])\n",
    "            return prepared_data\n",
    "        else:\n",
    "            day = publication_date[4:6]\n",
    "            month = Months.__members__[publication_date[0:3]].value\n",
    "            year = publication_date[7:13]\n",
    "            prepared_data = '-'.join([day, month, year])\n",
    "            return prepared_data\n",
    "    except KeyError as e:\n",
    "        return f'Invalid name of the month: {e}'\n",
    "    except:\n",
    "        print('error')\n",
    "\n",
    "\n",
    "\n",
    "def get_article_data():\n",
    "\n",
    "    data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe?page=25\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3)\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    all_tags_content = soup.find_all(lambda tag:tag.name==\"article\")\n",
    "\n",
    "    n=1\n",
    "\n",
    "    for i in all_tags_content:\n",
    "        publication_date_tag = i.find_all(attrs={'class': \"hide--fromW3\"})\n",
    "        item_name = i.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})\n",
    "        print(f\"item number {n}\")\n",
    "        n+=1\n",
    "        print(item_name[0].get_text()) \n",
    "\n",
    "        print(publication_date_tag)\n",
    "        potentially_data_string_1 = publication_date_tag[0].get_text()\n",
    "        potentially_data_string_2 = publication_date_tag[1].get_text()\n",
    "        try:\n",
    "            formatted_data_1 = data_conversion(potentially_data_string_1)\n",
    "\n",
    "            if bool(re.search(data_pattern, formatted_data_1)):\n",
    "                print(data_conversion(potentially_data_string_1) + \" string 1 success\")\n",
    "            else:\n",
    "                raise Exception(\"didnt work\")\n",
    "        except:\n",
    "            try: \n",
    "                formatted_data_2 = data_conversion(potentially_data_string_2)\n",
    "\n",
    "                if bool(re.search(data_pattern, formatted_data_2)):\n",
    "                    print(data_conversion(potentially_data_string_2) + \" string 2 success\")\n",
    "                else:\n",
    "                    raise Exception(\"didnt work second position\")\n",
    "            except:\n",
    "                potentially_data_string_3 = publication_date_tag[2].get_text()\n",
    "                print(data_conversion(potentially_data_string_3) + \" string 3 success\")\n",
    " \n",
    "       \n",
    "\n",
    "print(get_article_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_278/3414485110.py:23: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('./chromedriver')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deadly Premonition: The Director's Cut za 8,99 zł i Deadly Premonition: The Director's Cut - Deluxe Edition za 10,79 zł @ Steam\n",
      "Elektryczny śrubokręt FERM 4V\n",
      "Ręcznik papierowy Kartika XMASS 3 warstwy 2 rolki za 4,99zł w Biedronce w Opolu\n",
      "Piwo Browar Jana - Peach Gose i (Non)Pastry Sour - Market Point Kraków al. Pokoju 78\n",
      "Ładowarka samochodowa micro USB Titanum\n",
      "Firewatch za 11,90 zł z Węgierskiego Xbox Store @ Xbox One / Xbox Series / PC\n",
      "PS Plus Celebration Avatary + tapety za darmo z kodem @ PS Store\n",
      "Kurtka motocyklowa RST tracktech evo 4 rozmiar S, M 748zł\n",
      "Snow Bros. Nick & Tom Special Nintendo Switch\n",
      "Laptop 3D ACER Predator Helios 300 PH315-55S SpatialLabs 15.6\" IPS i9-12900H 32GB RAM 2 x 1TB SSD GeForce RTX3080 Windows 11 Home\n",
      "Samsung Galaxy Watch 5 Pro (R920) możliwe 999zł (500zł zwrot cashback)\n",
      "INDIANA JONES AND THE LAST CRUSADE PC @ Steam\n",
      "Laptop ACER Nitro 5 AN515-58 15.6\" IPS 165Hz i7-12700H 16GB RAM 512GB SSD GeForce RTX3070Ti\n",
      "Laptop ASUS TUF Gaming A15 FA506QM-HN008W 15.6\" IPS 144Hz R7-5800H 16GB RAM 512GB SSD GeForce RTX3060 Windows 11 Home\n",
      "Męskie buty biegowe Saucony (wiele rozmiarów): Fastwitch 9, Axon 2, Peregrine 12, Guide 15, Ride 15, Kinvara 13, Xodus Ultra\n",
      "Router TP-Link TL-ER605\n",
      "Peeling Himalaya Purifying Neem Scrub\n",
      "[Łódź] Darmowe Kina plenerowe sezon 2023 - lista miejsc\n",
      "Kajak turystyczny pneumatyczny Itiwit 2/3-osobowy\n",
      "Tefal Szybkowar Secure Click 6 l\n",
      "Stacja bi1 Ełk benzyna bezołowiowa PB 98 6.28/l\n",
      "Diablo II: Resurrected | PC | Battle.net\n",
      "SteamWorld Dig @ Steam\n",
      "Crank Brothers Narzędzie wielofunkcyjne 17\n",
      "Body z Tiulu i Satyny Cool Minimal (tylko w ramach MyIntimissimi)\n",
      "Chromebook 314 CB314-2H 14\" IPS 4/64 GB + gratis plecak (możliwe 969,99 z cashbackiem)\n",
      "Piekarnik elektryczny BEKO BBIM18300BS prowadnica teleskopowa (ostatnia cena 1169zł)\n",
      "Rękawiczki rowerowe żelowe Specialized\n",
      "Czarna kurtka zimowa Levi's z kapturem i logo\n",
      "Pompa ciepła Haier AU082FYCRA 7,8kW Monoblok\n",
      "Pepsi Max 2,5l; 1l~2.90\n",
      "Smartfon Realme 11 Pro 5G 8+128GB Amazon.de przedsprzedaż 351.42€\n",
      "Kask HJC C70 Czarny Mat\n",
      "Głośnik przenośny Sony SRS-XB13B Czarny\n",
      "3 noce dla rodziny 2+2 w Hotelu Sympozjum & Spa w Krakowie (wyżywienie HB i strefa wellness) @ Travelist\n",
      "Samsung galaxy watch 5 40mm LTE, możliwe 549zł Orange Flex\n",
      "Telewizor LG OLED65B23LA - 65\" - cena po cashback'u 5500 zł lub model 65C2 za 6000 zł\n",
      "Smartfon Motorola Moto g42 6/128GB 6.4\" OLED FHD+, Snapdragon 680, 5000mAh, 50 MP, Dual SIM)\n",
      "Philips Sonicare 5100 HX6851/34 szczoteczki + etui\n",
      "Zwrot za zakup Samsung Galaxy Watch 5 (350zł) i 5 Pro (500zł)\n",
      "Czytnik Kobo Clara 2E za 599zł @ Morele\n",
      "Cashback w banku Millennium za zakupy w Media Expert/Neonet do -30% (max 50 zł)\n",
      "Energy drink Tiger Rebel Tea 0,5 litra\n",
      "Deska do prasowania Leifheit 72585 AirBoard Compact M\n",
      "Red Dead Redemption 2 TR XBOX One CD Key - wymagany VPN\n",
      "Klucze Yato zestaw narzędziowy 216 elementów 204277\n",
      "World Music Day 2023 Bonus Content za darmo @ PC\n",
      "Olej 900ml rzepakowy Lewiatan.\n",
      "Karmnik dla ptaków typu klatka, 3 sztuki\n",
      "Dysk HDD 3,5\" SATA Seagate Firecuda 4TB - Amazon.pl - Sprzedawca ESP-Tech FR dysk\n",
      "Zniżka 15% na ubezpieczenie turystyczne TU Europa\n",
      "Soundbar LG SN4R 4.1\n",
      "Serum regenerujące Sylveco z olejkiem Blue Tansy za 24,99zł @ Empik\n",
      "Rekuperator iZZi 402 ERV\n",
      "Karta pamięci Patriot 256 GB A1 V30 microSD - zapis/odczyt 80/100 MB/s - gwarancja producenta 5 lat - darmowa dostawa Amazon dla wszystkich\n",
      "Pepsi - Pepsi max,mirinda, 7up, mountain dew 1.5l Delikatesy centrum\n",
      "Pedały Shimano SPD-M540 + bloki - 221,08 zł\n",
      "Laptop MSI RTX 4050 Katana 15 B12VEK-1000PL 15,6 \" Intel Core i5 16 GB / 512 GB czarny\n",
      "Smartfon Motorola Edge 40 Pro 824.76€ + 4,92 €\n",
      "Soundbar LG S95QR 9.1.5 (Wi-Fi, Bluetooth, AirPlay Dolby Atmos, DTS X) @ Euro\n",
      "Soundbar LG S95QR 9.1.5 (Wi-Fi, Bluetooth, AirPlay Dolby Atmos, DTS X) @ Euro\n"
     ]
    }
   ],
   "source": [
    "#important! in the pipeline\n",
    "\n",
    "action_type = \"/nowe?page=\"\n",
    "start_page = 25\n",
    "end_page = 25\n",
    "website_url = \"https://www.pepper.pl\"\n",
    "articles_to_retrieve = 61\n",
    "\n",
    "#informations = [name, discount_price, regular_price, percentage_discount, link]\n",
    "\n",
    "\n",
    "class ScrapWebpage:\n",
    "\n",
    "    def __init__(self, website_url, action_type, articles_to_retrieve, start_page=1):\n",
    "        self.website_url = website_url\n",
    "        self.action_type = action_type\n",
    "        self.articles_to_retrieve = articles_to_retrieve\n",
    "        self.start_page = start_page\n",
    "\n",
    "\n",
    "    def scrap_data(self):\n",
    "\n",
    "        try:\n",
    "            url_to_scrap = self.website_url + self.action_type + str(self.start_page)\n",
    "            driver = webdriver.Chrome('./chromedriver') \n",
    "            driver.get(url_to_scrap) \n",
    "            time.sleep(0.7)\n",
    "            page = driver.page_source\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            print(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            print(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            print(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            print(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "\n",
    "    def infinite_scroll_handling(self):\n",
    "\n",
    "        try:\n",
    "            flag = True\n",
    "            retrived_articles = list()\n",
    "\n",
    "            while flag:\n",
    "                soup = self.scrap_data()\n",
    "                articles = soup.find_all('article')\n",
    "                retrived_articles += articles\n",
    "\n",
    "                if len(retrived_articles) >= self.articles_to_retrieve:\n",
    "                    flag = False\n",
    "\n",
    "                self.start_page += 1\n",
    "\n",
    "            return retrived_articles[:self.articles_to_retrieve]\n",
    "        except IndexError as e:\n",
    "            raise IndexError(\"There aren't that many articles, try retrieve lower quantity of articles\")\n",
    "\n",
    "        \n",
    "    \n",
    "    def choose_informations_to_scrap(self):\n",
    "        \n",
    "        \"\"\"info_call = {artice_id:         [\"id\"],\n",
    "                name:                   attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"},\n",
    "                date:                   ItemAddedDataRetrieve(),\n",
    "                discount_price:         attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"},\n",
    "                regular_price:          attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"},\n",
    "                percentage_discount:    attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"},\n",
    "                link:                   'a', href=True, text=True}\"\"\"\n",
    "\n",
    "        retrived_articles = self.infinite_scroll_handling()\n",
    "\n",
    "        for article in retrived_articles:\n",
    "            print(GetItemName(article).get_data())\n",
    "\n",
    "            \"\"\"for info in self.informations:\n",
    "                #info_call.get(info)\n",
    "                info = article.find_all(attrs={info_call.get(info))\n",
    "                print(info)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \"\"\"item_name = i.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})\n",
    "        discount_price = i.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "        regular_price = i.find_all(attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"})\n",
    "        percentage_discount = i.find_all(attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "        item_link = i.find_all('a', href=True, text=True)\"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    "        #(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"}, href=True)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "output = ScrapWebpage(website_url, action_type, articles_to_retrieve)\n",
    "output.choose_informations_to_scrap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important! in the pipeline\n",
    "\n",
    "\n",
    "class GetItemName:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            name = self.article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})\n",
    "            name = name[0].get_text()\n",
    "            return name\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name: {e}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important! in the pipeline\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "    \n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "    \n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "class GetItemAddedDate(Months):\n",
    "\n",
    "    def data_format_conversion(self, scraped_publication_date):\n",
    "\n",
    "        old_dates_data_pattern = \"[A-Za-z]+\\s\\d\\d\\.\\s[0-9]+\"\n",
    "\n",
    "        try:\n",
    "            if scraped_publication_date.endswith(('min', 'g', 's', 'temu')):\n",
    "                prepared_data = date.today().strftime(\"%d-%m-%Y\")\n",
    "                return prepared_data\n",
    "            elif scraped_publication_date.startswith(tuple(Months.keys())) and len(scraped_publication_date) < 8:      \n",
    "                if len(scraped_publication_date[4:]) == 3:\n",
    "                    day = scraped_publication_date[4:6]\n",
    "                else:\n",
    "                    day = scraped_publication_date[4:5].zfill(2)\n",
    "                month = Months.__members__[scraped_publication_date[0:3]].value\n",
    "                year = str(date.today().year)\n",
    "                prepared_data = '-'.join([str(day), month, year])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(old_dates_data_pattern, scraped_publication_date)):\n",
    "                day = scraped_publication_date[4:6]\n",
    "                month = Months.__members__[scraped_publication_date[0:3]].value\n",
    "                year = scraped_publication_date[8:13]\n",
    "                prepared_data = '-'.join([day, month, year])\n",
    "                return prepared_data\n",
    "        except KeyError as error:\n",
    "            raise KeyError(f\"Invalid name of the month {error}\")\n",
    "\n",
    "\n",
    "    def first_index_position_data_searching(self, html_tag_with_publication_data):\n",
    "\n",
    "        output_data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "        string_to_check = html_tag_with_publication_data[0].get_text()\n",
    "\n",
    "        try:\n",
    "            formatted_data = self.data_format_conversion(string_to_check)\n",
    "            if bool(re.search(output_data_pattern, formatted_data)):\n",
    "                return f\"{data_conversion(string_to_check)} + ' string 1 success'\"\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "    def second_index_position_data_searching(self, html_tag_with_publication_data):\n",
    "        \n",
    "        output_data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "        string_to_check = html_tag_with_publication_data[1].get_text()\n",
    "\n",
    "        try:\n",
    "            formatted_data = self.data_format_conversion(string_to_check)\n",
    "            if bool(re.search(output_data_pattern, formatted_data)):\n",
    "                return f\"{data_conversion(string_to_check)} + ' string 2 success'\"\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "    def third_index_position_data_searching(self, html_tag_with_publication_data):\n",
    "        \n",
    "        output_data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "        string_to_check = html_tag_with_publication_data[2].get_text()\n",
    "\n",
    "        try:\n",
    "            formatted_data = self.data_format_conversion(string_to_check)\n",
    "            if bool(re.search(output_data_pattern, formatted_data)):\n",
    "                return f\"{data_conversion(string_to_check)} + ' string 3 success'\"\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "    def scrap_page_and_receive_data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e6a7b6723b894902baae12ce751b40f53dc6201cc32eda169f42fc9fc428a6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
