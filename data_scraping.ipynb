{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import ConnectionError, HTTPError, MissingSchema, ReadTimeout\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta, date\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from enum import Enum, IntEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_all_content_infinite_scroll(scroll_pause_time=1):\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3) #time to load a page\n",
    "    \n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "    i = 1\n",
    "\n",
    "    #infinite scroll loop\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "        if (screen_height) * i > scroll_height:\n",
    "            break\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    return soup.main.prettify()\n",
    "\n",
    "\n",
    "print(get_all_content_infinite_scroll(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_all_article_id():\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3) #time to load a page\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    articles = soup.find_all('article')\n",
    "\n",
    "    list_of_article_id = [x[\"id\"] for x in articles] \n",
    "    \n",
    "    return list_of_article_id\n",
    "\n",
    "print(get_all_article_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_all_article_id_many_subpages(start_page_number=1, end_page_number=3):\n",
    "\n",
    "    page_number = 1\n",
    "    list_of_article_id = []\n",
    "    all_articles = []\n",
    "    \n",
    "\n",
    "    try:\n",
    "        while page_number != end_page_number:\n",
    "\n",
    "            page_url = f\"https://www.pepper.pl/nowe?page={page_number}\"\n",
    "\n",
    "            driver = webdriver.Chrome('./chromedriver') \n",
    "            driver.get(page_url) \n",
    "            time.sleep(1) #time to load a page\n",
    "\n",
    "            page = driver.page_source\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "\n",
    "            articles_from_page = soup.find_all('article')\n",
    "            #all_articles.append(articles_from_page)\n",
    "            print(articles_from_page)\n",
    "\n",
    "            print(len(all_articles))\n",
    "            print(len(soup))\n",
    "\n",
    "\n",
    "            for i in articles:\n",
    "                if i[\"id\"] not in list_of_article_id:\n",
    "                    list_of_article_id.append(i[\"id\"])\n",
    "            \n",
    "            page_number += 1\n",
    "    except:\n",
    "        print(\"Exception occured\")\n",
    "\n",
    "    return list_of_article_id\n",
    "\n",
    "print(get_all_article_id_many_subpages(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_item_name_like(item):\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3) #time to load a page\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    list_all_items_info = soup.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})\n",
    "\n",
    "    list_all_items_names = [x.get_text() for x in list_all_items_info]\n",
    "    \n",
    "    matches = [x for x in list_all_items_names if item.lower() in x.lower()]\n",
    "\n",
    "    print(matches)\n",
    "\n",
    "\n",
    "get_item_name_like(\"Monitor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "def get_content_item_tag():\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3)\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    all_tags_content = soup.find_all(\"article\")\n",
    "    \n",
    "    for i in all_tags_content:\n",
    "        item_name = i.find_all(\"cept-tt thread-link linkPlain thread-title--list js-thread-title\")\n",
    "        discount_price = i.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "        regular_price = i.find_all(attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"})\n",
    "        percentage_discount = i.find_all(attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "        item_link = i.find_all('a', href=True, text=True)\n",
    "        \n",
    "        #(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"}, href=True)\n",
    "        \n",
    "\n",
    "        article_id = i[\"id\"]\n",
    "        print(item_name)\n",
    "        try:    \n",
    "            print(float(discount_price[0].get_text().strip('zł').replace('.','').replace(',','.')))\n",
    "        except: \n",
    "            print(\"N/A\")\n",
    "        try:\n",
    "            print(float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.')))\n",
    "        except:\n",
    "            print(\"N/A\")\n",
    "        try:\n",
    "            print(float(percentage_discount[0].get_text().strip('%')))\n",
    "        except:\n",
    "            print(\"N/A\")\n",
    "        print(item_link[0]['href'])\n",
    "        print(article_id.strip('thread_'))\n",
    "\n",
    "\n",
    "    #print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "get_content_item_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not important \n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "    \n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "    \n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "def data_conversion(publication_date):\n",
    "\n",
    "    try:\n",
    "        if publication_date.endswith(('min', 'g', 's', 'temu')):\n",
    "            prepared_data = date.today().strftime(\"%d-%m-%Y\")\n",
    "            return prepared_data\n",
    "        elif publication_date.startswith(tuple(Months.keys())) and len(publication_date) < 8:      \n",
    "            if len(publication_date[4:]) == 3:\n",
    "                day = publication_date[4:6]\n",
    "            else:\n",
    "                day = publication_date[4:5].zfill(2)\n",
    "            month = Months.__members__[publication_date[0:3]].value\n",
    "            year = str(date.today().year)\n",
    "            prepared_data = '-'.join([str(day), month, year])\n",
    "            return prepared_data\n",
    "        else:\n",
    "            day = publication_date[4:6]\n",
    "            month = Months.__members__[publication_date[0:3]].value\n",
    "            year = publication_date[7:13]\n",
    "            prepared_data = '-'.join([day, month, year])\n",
    "            return prepared_data\n",
    "    except KeyError as e:\n",
    "        return f'Invalid name of the month: {e}'\n",
    "    except:\n",
    "        print('error')\n",
    "\n",
    "\n",
    "\n",
    "def get_article_data():\n",
    "\n",
    "    data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "    page_url = \"https://www.pepper.pl/nowe?page=25\"\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(page_url) \n",
    "    time.sleep(3)\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    all_tags_content = soup.find_all(lambda tag:tag.name==\"article\")\n",
    "\n",
    "    n=1\n",
    "\n",
    "    for i in all_tags_content:\n",
    "        publication_date_tag = i.find_all(attrs={'class': \"hide--fromW3\"})\n",
    "        item_name = i.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})\n",
    "        print(f\"item number {n}\")\n",
    "        n+=1\n",
    "        print(item_name[0].get_text()) \n",
    "\n",
    "        print(publication_date_tag)\n",
    "        potentially_data_string_1 = publication_date_tag[0].get_text()\n",
    "        potentially_data_string_2 = publication_date_tag[1].get_text()\n",
    "        try:\n",
    "            formatted_data_1 = data_conversion(potentially_data_string_1)\n",
    "\n",
    "            if bool(re.search(data_pattern, formatted_data_1)):\n",
    "                print(data_conversion(potentially_data_string_1) + \" string 1 success\")\n",
    "            else:\n",
    "                raise Exception(\"didnt work\")\n",
    "        except:\n",
    "            try: \n",
    "                formatted_data_2 = data_conversion(potentially_data_string_2)\n",
    "\n",
    "                if bool(re.search(data_pattern, formatted_data_2)):\n",
    "                    print(data_conversion(potentially_data_string_2) + \" string 2 success\")\n",
    "                else:\n",
    "                    raise Exception(\"didnt work second position\")\n",
    "            except:\n",
    "                potentially_data_string_3 = publication_date_tag[2].get_text()\n",
    "                print(data_conversion(potentially_data_string_3) + \" string 3 success\")\n",
    " \n",
    "       \n",
    "\n",
    "print(get_article_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_307/13773120.py:25: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('./chromedriver')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Słuchawki bezprzewodowe dokanałowe TCL SOCL500TWS\n",
      "99.99\n",
      "689837\n",
      "-33.0\n",
      "149.99\n",
      "https://www.pepper.pl/promocje/sluchawki-bezprzewodowe-dokanalowe-tcl-socl500tws-689837\n",
      "Solevita Bio Imbir Seler shot\n",
      "1.99\n",
      "689835\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_307/13773120.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScrapWebpage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebsite_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticles_to_retrieve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_informations_to_scrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_307/13773120.py\u001b[0m in \u001b[0;36mchoose_informations_to_scrap\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetItemDiscountPrice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetItemId\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetItemPercentageDiscount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetItemRegularPrice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetItemUrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_307/3508092008.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mpercentage_discount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"space--ml-1 size--all-l size--fromW3-xl\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mpercentage_discount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentage_discount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpercentage_discount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#important! in the pipeline\n",
    "\n",
    "action_type = \"/nowe?page=\"\n",
    "start_page = 25\n",
    "end_page = 25\n",
    "website_url = \"https://www.pepper.pl\"\n",
    "articles_to_retrieve = 61\n",
    "\n",
    "#informations = [name, discount_price, regular_price, percentage_discount, link]\n",
    "\n",
    "\n",
    "class ScrapWebpage:\n",
    "\n",
    "    def __init__(self, website_url, action_type, articles_to_retrieve, start_page=1):\n",
    "        self.website_url = website_url\n",
    "        self.action_type = action_type\n",
    "        self.articles_to_retrieve = articles_to_retrieve\n",
    "        self.start_page = start_page\n",
    "\n",
    "\n",
    "    def scrap_data(self):\n",
    "\n",
    "        try:\n",
    "            url_to_scrap = self.website_url + self.action_type + str(self.start_page)\n",
    "            driver = webdriver.Chrome('./chromedriver') \n",
    "            driver.get(url_to_scrap) \n",
    "            time.sleep(0.7)\n",
    "            page = driver.page_source\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            return soup\n",
    "        except ConnectionError as e:\n",
    "            print(f\"ConnectionError occured: {e}. \\nTry again later\")\n",
    "        except MissingSchema as e:\n",
    "            print(f\"MissingSchema occured: {e}. \\nMake sure that protocol indicator is icluded in the website url\")\n",
    "        except HTTPError as e:\n",
    "            print(f\"HTTPError occured: {e}. \\nMake sure that website url is valid\")\n",
    "        except ReadTimeout as e:\n",
    "            print(f\"ReadTimeout occured: {e}. \\nTry again later\")\n",
    "\n",
    "\n",
    "    def infinite_scroll_handling(self):\n",
    "\n",
    "        try:\n",
    "            flag = True\n",
    "            retrived_articles = list()\n",
    "\n",
    "            while flag:\n",
    "                soup = self.scrap_data()\n",
    "                articles = soup.find_all('article')\n",
    "                retrived_articles += articles\n",
    "\n",
    "                if len(retrived_articles) >= self.articles_to_retrieve:\n",
    "                    flag = False\n",
    "\n",
    "                self.start_page += 1\n",
    "\n",
    "            return retrived_articles[:self.articles_to_retrieve]\n",
    "        except IndexError as e:\n",
    "            raise IndexError(\"There aren't that many articles, try retrieve lower quantity of articles\")\n",
    "\n",
    "        \n",
    "    \n",
    "    def choose_informations_to_scrap(self):\n",
    "        \n",
    "        \"\"\"info_call = {artice_id:         [\"id\"],\n",
    "                name:                   attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"},\n",
    "                date:                   ItemAddedDataRetrieve(),\n",
    "                discount_price:         attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"},\n",
    "                regular_price:          attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"},\n",
    "                percentage_discount:    attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"},\n",
    "                link:                   'a', href=True, text=True}\"\"\"\n",
    "\n",
    "        retrived_articles = self.infinite_scroll_handling()\n",
    "\n",
    "        for article in retrived_articles:\n",
    "            print(GetItemName(article).get_data())\n",
    "            print(GetItemDiscountPrice(article).get_data())\n",
    "            print(GetItemId(article).get_data())\n",
    "            print(GetItemPercentageDiscount(article).get_data())\n",
    "            print(GetItemRegularPrice(article).get_data())\n",
    "            print(GetItemUrl(article).get_data())\n",
    "\n",
    "\n",
    "            \"\"\"for info in self.informations:\n",
    "                #info_call.get(info)\n",
    "                info = article.find_all(attrs={info_call.get(info))\n",
    "                print(info)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \"\"\"item_name = i.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})\n",
    "        discount_price = i.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "        regular_price = i.find_all(attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"})\n",
    "        percentage_discount = i.find_all(attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "        item_link = i.find_all('a', href=True, text=True)\"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    "        #(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"}, href=True)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "output = ScrapWebpage(website_url, action_type, articles_to_retrieve)\n",
    "output.choose_informations_to_scrap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important! in the pipeline\n",
    "\n",
    "\n",
    "class GetItemName:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            name = self.article.find_all(attrs={'class': \"cept-tt thread-link linkPlain thread-title--list js-thread-title\"})\n",
    "            name = name[0].get_text()\n",
    "            return name\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_name): {e}\")\n",
    "            return \"NA\"\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_name): {e}\")\n",
    "\n",
    "\n",
    "class GetItemId:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "\n",
    "    def get_data(self):\n",
    "        try:\n",
    "            item_id = self.article[\"id\"]\n",
    "            item_id = item_id.strip('thread_')\n",
    "            return item_id\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_id): {e}\")\n",
    "            return \"NA\"\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_id): {e}\")\n",
    "\n",
    "\n",
    "class GetItemDiscountPrice:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            discount_price = self.article.find_all(attrs={'class': \"thread-price text--b cept-tp size--all-l size--fromW3-xl\"})\n",
    "            discount_price = float(discount_price[0].get_text().strip('zł').replace('.','').replace(',','.'))\n",
    "            return discount_price\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_discount_price): {e}\")\n",
    "            return \"NA\"\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_discount_price): {e}\")\n",
    "\n",
    "\n",
    "\n",
    "class GetItemRegularPrice:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            regular_price = self.article.find_all(attrs={'class': \"mute--text text--lineThrough size--all-l size--fromW3-xl\"})\n",
    "            regular_price = float(regular_price[0].get_text().strip('zł').replace('.','').replace(',','.'))\n",
    "            return regular_price\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_regular_price): {e}\")\n",
    "            return \"NA\"\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_regular_price): {e}\")\n",
    "\n",
    "\n",
    "class GetItemPercentageDiscount:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            percentage_discount = self.article.find_all(attrs={'class': \"space--ml-1 size--all-l size--fromW3-xl\"})\n",
    "            percentage_discount = float(percentage_discount[0].get_text().strip('%'))\n",
    "            return percentage_discount\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_percentage_discount): {e}\")\n",
    "            return \"NA\"\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_percentage_discount): {e}\")\n",
    "\n",
    "\n",
    "class GetItemUrl:\n",
    "\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "    \n",
    "    def get_data(self):\n",
    "        try:\n",
    "            item_url = self.article.find_all('a', href=True, text=True)\n",
    "            item_url = item_url[0]['href']\n",
    "            return item_url\n",
    "        except IndexError as e:\n",
    "            raise IndexError(f\"Index out of the range (item_url): {e}\")\n",
    "            return \"NA\"\n",
    "        except TypeError as e:\n",
    "            raise TypeError(f\"Invalid html class name (item_url): {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important! in the pipeline\n",
    "\n",
    "\n",
    "class Months(Enum):\n",
    "\n",
    "    sty = '01'\n",
    "    lut = '02'\n",
    "    mar = '03'\n",
    "    kwi = '04'\n",
    "    maj = '05'\n",
    "    cze = '06'\n",
    "    lip = '07'\n",
    "    sie = '08'\n",
    "    wrz = '09'\n",
    "    paz = '10'\n",
    "    paź = '10'\n",
    "    lis = '11'\n",
    "    gru = '12'\n",
    "\n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Returns a dictionary representation of the enum.\"\"\"\n",
    "        return {e.name: e.value for e in cls}\n",
    "    \n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        \"\"\"Returns a list of all the enum keys.\"\"\"\n",
    "        return cls._member_names_\n",
    "    \n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        \"\"\"Returns a list of all the enum values.\"\"\"\n",
    "        return list(cls._value2member_map_.keys())\n",
    "\n",
    "class GetItemAddedDate(Months):\n",
    "\n",
    "    def data_format_conversion(self, scraped_publication_date):\n",
    "\n",
    "        old_dates_data_pattern = \"[A-Za-z]+\\s\\d\\d\\.\\s[0-9]+\"\n",
    "\n",
    "        try:\n",
    "            if scraped_publication_date.endswith(('min', 'g', 's', 'temu')):\n",
    "                prepared_data = date.today().strftime(\"%d-%m-%Y\")\n",
    "                return prepared_data\n",
    "            elif scraped_publication_date.startswith(tuple(Months.keys())) and len(scraped_publication_date) < 8:      \n",
    "                if len(scraped_publication_date[4:]) == 3:\n",
    "                    day = scraped_publication_date[4:6]\n",
    "                else:\n",
    "                    day = scraped_publication_date[4:5].zfill(2)\n",
    "                month = Months.__members__[scraped_publication_date[0:3]].value\n",
    "                year = str(date.today().year)\n",
    "                prepared_data = '-'.join([str(day), month, year])\n",
    "                return prepared_data\n",
    "            elif bool(re.search(old_dates_data_pattern, scraped_publication_date)):\n",
    "                day = scraped_publication_date[4:6]\n",
    "                month = Months.__members__[scraped_publication_date[0:3]].value\n",
    "                year = scraped_publication_date[8:13]\n",
    "                prepared_data = '-'.join([day, month, year])\n",
    "                return prepared_data\n",
    "        except KeyError as error:\n",
    "            raise KeyError(f\"Invalid name of the month {error}\")\n",
    "\n",
    "\n",
    "    def first_index_position_data_searching(self, html_tag_with_publication_data):\n",
    "\n",
    "        output_data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "        string_to_check = html_tag_with_publication_data[0].get_text()\n",
    "\n",
    "        try:\n",
    "            formatted_data = self.data_format_conversion(string_to_check)\n",
    "            if bool(re.search(output_data_pattern, formatted_data)):\n",
    "                return f\"{data_conversion(string_to_check)} + ' string 1 success'\"\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "    def second_index_position_data_searching(self, html_tag_with_publication_data):\n",
    "        \n",
    "        output_data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "        string_to_check = html_tag_with_publication_data[1].get_text()\n",
    "\n",
    "        try:\n",
    "            formatted_data = self.data_format_conversion(string_to_check)\n",
    "            if bool(re.search(output_data_pattern, formatted_data)):\n",
    "                return f\"{data_conversion(string_to_check)} + ' string 2 success'\"\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "    def third_index_position_data_searching(self, html_tag_with_publication_data):\n",
    "        \n",
    "        output_data_pattern = \"\\d{2}[/.-]\\d{2}[/.-]\\d{4}\"\n",
    "\n",
    "        string_to_check = html_tag_with_publication_data[2].get_text()\n",
    "\n",
    "        try:\n",
    "            formatted_data = self.data_format_conversion(string_to_check)\n",
    "            if bool(re.search(output_data_pattern, formatted_data)):\n",
    "                return f\"{data_conversion(string_to_check)} + ' string 3 success'\"\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "\n",
    "    def scrap_page_and_receive_data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e6a7b6723b894902baae12ce751b40f53dc6201cc32eda169f42fc9fc428a6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
